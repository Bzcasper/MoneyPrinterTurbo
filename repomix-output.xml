This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  ISSUE_TEMPLATE/
    bug_report.yml
    config.yml
    feature_request.yml
app/
  config/
    __init__.py
    config.py
  controllers/
    manager/
      base_manager.py
      memory_manager.py
      redis_manager.py
    v1/
      base.py
      llm.py
      video.py
    base.py
    ping.py
  models/
    const.py
    exception.py
    schema.py
  services/
    utils/
      video_effects.py
    llm.py
    material.py
    state.py
    subtitle.py
    task.py
    video.py
    voice.py
  utils/
    utils.py
  asgi.py
  router.py
docs/
  MoneyPrinterTurbo.ipynb
  voice-list.txt
resource/
  public/
    index.html
test/
  services/
    __init__.py
    test_task.py
    test_video.py
    test_voice.py
  __init__.py
  README.md
webui/
  i18n/
    de.json
    en.json
    pt.json
    vi.json
    zh.json
  Main.py
.dockerignore
.gitignore
config.example.toml
docker-compose.yml
Dockerfile
LICENSE
main.py
README-en.md
README.md
requirements.txt
webui.bat
webui.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/config/__init__.py">
import os
import sys

from loguru import logger

from app.config import config
from app.utils import utils


def __init_logger():
    # _log_file = utils.storage_dir("logs/server.log")
    _lvl = config.log_level
    root_dir = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
    )

    def format_record(record):
        # Ëé∑ÂèñÊó•ÂøóËÆ∞ÂΩï‰∏≠ÁöÑÊñá‰ª∂ÂÖ®Ë∑ØÂæÑ
        file_path = record["file"].path
        # Â∞ÜÁªùÂØπË∑ØÂæÑËΩ¨Êç¢‰∏∫Áõ∏ÂØπ‰∫éÈ°πÁõÆÊ†πÁõÆÂΩïÁöÑË∑ØÂæÑ
        relative_path = os.path.relpath(file_path, root_dir)
        # Êõ¥Êñ∞ËÆ∞ÂΩï‰∏≠ÁöÑÊñá‰ª∂Ë∑ØÂæÑ
        record["file"].path = f"./{relative_path}"
        # ËøîÂõû‰øÆÊîπÂêéÁöÑÊ†ºÂºèÂ≠óÁ¨¶‰∏≤
        # ÊÇ®ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅË∞ÉÊï¥ËøôÈáåÁöÑÊ†ºÂºè
        _format = (
            "<green>{time:%Y-%m-%d %H:%M:%S}</> | "
            + "<level>{level}</> | "
            + '"{file.path}:{line}":<blue> {function}</> '
            + "- <level>{message}</>"
            + "\n"
        )
        return _format

    logger.remove()

    logger.add(
        sys.stdout,
        level=_lvl,
        format=format_record,
        colorize=True,
    )

    # logger.add(
    #     _log_file,
    #     level=_lvl,
    #     format=format_record,
    #     rotation="00:00",
    #     retention="3 days",
    #     backtrace=True,
    #     diagnose=True,
    #     enqueue=True,
    # )


__init_logger()
</file>

<file path="app/controllers/manager/memory_manager.py">
from queue import Queue
from typing import Dict

from app.controllers.manager.base_manager import TaskManager


class InMemoryTaskManager(TaskManager):
    def create_queue(self):
        return Queue()

    def enqueue(self, task: Dict):
        self.queue.put(task)

    def dequeue(self):
        return self.queue.get()

    def is_queue_empty(self):
        return self.queue.empty()
</file>

<file path="app/controllers/manager/redis_manager.py">
import json
from typing import Dict

import redis

from app.controllers.manager.base_manager import TaskManager
from app.models.schema import VideoParams
from app.services import task as tm

FUNC_MAP = {
    "start": tm.start,
    # 'start_test': tm.start_test
}


class RedisTaskManager(TaskManager):
    def __init__(self, max_concurrent_tasks: int, redis_url: str):
        self.redis_client = redis.Redis.from_url(redis_url)
        super().__init__(max_concurrent_tasks)

    def create_queue(self):
        return "task_queue"

    def enqueue(self, task: Dict):
        task_with_serializable_params = task.copy()

        if "params" in task["kwargs"] and isinstance(
            task["kwargs"]["params"], VideoParams
        ):
            task_with_serializable_params["kwargs"]["params"] = task["kwargs"][
                "params"
            ].dict()

        # Â∞ÜÂáΩÊï∞ÂØπË±°ËΩ¨Êç¢‰∏∫ÂÖ∂ÂêçÁß∞
        task_with_serializable_params["func"] = task["func"].__name__
        self.redis_client.rpush(self.queue, json.dumps(task_with_serializable_params))

    def dequeue(self):
        task_json = self.redis_client.lpop(self.queue)
        if task_json:
            task_info = json.loads(task_json)
            # Â∞ÜÂáΩÊï∞ÂêçÁß∞ËΩ¨Êç¢ÂõûÂáΩÊï∞ÂØπË±°
            task_info["func"] = FUNC_MAP[task_info["func"]]

            if "params" in task_info["kwargs"] and isinstance(
                task_info["kwargs"]["params"], dict
            ):
                task_info["kwargs"]["params"] = VideoParams(
                    **task_info["kwargs"]["params"]
                )

            return task_info
        return None

    def is_queue_empty(self):
        return self.redis_client.llen(self.queue) == 0
</file>

<file path="app/controllers/base.py">
from uuid import uuid4

from fastapi import Request

from app.config import config
from app.models.exception import HttpException


def get_task_id(request: Request):
    task_id = request.headers.get("x-task-id")
    if not task_id:
        task_id = uuid4()
    return str(task_id)


def get_api_key(request: Request):
    api_key = request.headers.get("x-api-key")
    return api_key


def verify_token(request: Request):
    token = get_api_key(request)
    if token != config.app.get("api_key", ""):
        request_id = get_task_id(request)
        request_url = request.url
        user_agent = request.headers.get("user-agent")
        raise HttpException(
            task_id=request_id,
            status_code=401,
            message=f"invalid token: {request_url}, {user_agent}",
        )
</file>

<file path="app/models/const.py">
PUNCTUATIONS = [
    "?",
    ",",
    ".",
    "„ÄÅ",
    ";",
    ":",
    "!",
    "‚Ä¶",
    "Ôºü",
    "Ôºå",
    "„ÄÇ",
    "„ÄÅ",
    "Ôºõ",
    "Ôºö",
    "ÔºÅ",
    "...",
]

TASK_STATE_FAILED = -1
TASK_STATE_COMPLETE = 1
TASK_STATE_PROCESSING = 4

FILE_TYPE_VIDEOS = ["mp4", "mov", "mkv", "webm"]
FILE_TYPE_IMAGES = ["jpg", "jpeg", "png", "bmp"]
</file>

<file path="app/router.py">
"""Application configuration - root APIRouter.

Defines all FastAPI application endpoints.

Resources:
    1. https://fastapi.tiangolo.com/tutorial/bigger-applications

"""

from fastapi import APIRouter

from app.controllers.v1 import llm, video

root_api_router = APIRouter()
# v1
root_api_router.include_router(video.router)
root_api_router.include_router(llm.router)
</file>

<file path="docs/voice-list.txt">
Name: af-ZA-AdriNeural
Gender: Female

Name: af-ZA-WillemNeural
Gender: Male

Name: am-ET-AmehaNeural
Gender: Male

Name: am-ET-MekdesNeural
Gender: Female

Name: ar-AE-FatimaNeural
Gender: Female

Name: ar-AE-HamdanNeural
Gender: Male

Name: ar-BH-AliNeural
Gender: Male

Name: ar-BH-LailaNeural
Gender: Female

Name: ar-DZ-AminaNeural
Gender: Female

Name: ar-DZ-IsmaelNeural
Gender: Male

Name: ar-EG-SalmaNeural
Gender: Female

Name: ar-EG-ShakirNeural
Gender: Male

Name: ar-IQ-BasselNeural
Gender: Male

Name: ar-IQ-RanaNeural
Gender: Female

Name: ar-JO-SanaNeural
Gender: Female

Name: ar-JO-TaimNeural
Gender: Male

Name: ar-KW-FahedNeural
Gender: Male

Name: ar-KW-NouraNeural
Gender: Female

Name: ar-LB-LaylaNeural
Gender: Female

Name: ar-LB-RamiNeural
Gender: Male

Name: ar-LY-ImanNeural
Gender: Female

Name: ar-LY-OmarNeural
Gender: Male

Name: ar-MA-JamalNeural
Gender: Male

Name: ar-MA-MounaNeural
Gender: Female

Name: ar-OM-AbdullahNeural
Gender: Male

Name: ar-OM-AyshaNeural
Gender: Female

Name: ar-QA-AmalNeural
Gender: Female

Name: ar-QA-MoazNeural
Gender: Male

Name: ar-SA-HamedNeural
Gender: Male

Name: ar-SA-ZariyahNeural
Gender: Female

Name: ar-SY-AmanyNeural
Gender: Female

Name: ar-SY-LaithNeural
Gender: Male

Name: ar-TN-HediNeural
Gender: Male

Name: ar-TN-ReemNeural
Gender: Female

Name: ar-YE-MaryamNeural
Gender: Female

Name: ar-YE-SalehNeural
Gender: Male

Name: az-AZ-BabekNeural
Gender: Male

Name: az-AZ-BanuNeural
Gender: Female

Name: bg-BG-BorislavNeural
Gender: Male

Name: bg-BG-KalinaNeural
Gender: Female

Name: bn-BD-NabanitaNeural
Gender: Female

Name: bn-BD-PradeepNeural
Gender: Male

Name: bn-IN-BashkarNeural
Gender: Male

Name: bn-IN-TanishaaNeural
Gender: Female

Name: bs-BA-GoranNeural
Gender: Male

Name: bs-BA-VesnaNeural
Gender: Female

Name: ca-ES-EnricNeural
Gender: Male

Name: ca-ES-JoanaNeural
Gender: Female

Name: cs-CZ-AntoninNeural
Gender: Male

Name: cs-CZ-VlastaNeural
Gender: Female

Name: cy-GB-AledNeural
Gender: Male

Name: cy-GB-NiaNeural
Gender: Female

Name: da-DK-ChristelNeural
Gender: Female

Name: da-DK-JeppeNeural
Gender: Male

Name: de-AT-IngridNeural
Gender: Female

Name: de-AT-JonasNeural
Gender: Male

Name: de-CH-JanNeural
Gender: Male

Name: de-CH-LeniNeural
Gender: Female

Name: de-DE-AmalaNeural
Gender: Female

Name: de-DE-ConradNeural
Gender: Male

Name: de-DE-FlorianMultilingualNeural
Gender: Male

Name: de-DE-KatjaNeural
Gender: Female

Name: de-DE-KillianNeural
Gender: Male

Name: de-DE-SeraphinaMultilingualNeural
Gender: Female

Name: el-GR-AthinaNeural
Gender: Female

Name: el-GR-NestorasNeural
Gender: Male

Name: en-AU-NatashaNeural
Gender: Female

Name: en-AU-WilliamNeural
Gender: Male

Name: en-CA-ClaraNeural
Gender: Female

Name: en-CA-LiamNeural
Gender: Male

Name: en-GB-LibbyNeural
Gender: Female

Name: en-GB-MaisieNeural
Gender: Female

Name: en-GB-RyanNeural
Gender: Male

Name: en-GB-SoniaNeural
Gender: Female

Name: en-GB-ThomasNeural
Gender: Male

Name: en-HK-SamNeural
Gender: Male

Name: en-HK-YanNeural
Gender: Female

Name: en-IE-ConnorNeural
Gender: Male

Name: en-IE-EmilyNeural
Gender: Female

Name: en-IN-NeerjaExpressiveNeural
Gender: Female

Name: en-IN-NeerjaNeural
Gender: Female

Name: en-IN-PrabhatNeural
Gender: Male

Name: en-KE-AsiliaNeural
Gender: Female

Name: en-KE-ChilembaNeural
Gender: Male

Name: en-NG-AbeoNeural
Gender: Male

Name: en-NG-EzinneNeural
Gender: Female

Name: en-NZ-MitchellNeural
Gender: Male

Name: en-NZ-MollyNeural
Gender: Female

Name: en-PH-JamesNeural
Gender: Male

Name: en-PH-RosaNeural
Gender: Female

Name: en-SG-LunaNeural
Gender: Female

Name: en-SG-WayneNeural
Gender: Male

Name: en-TZ-ElimuNeural
Gender: Male

Name: en-TZ-ImaniNeural
Gender: Female

Name: en-US-AnaNeural
Gender: Female

Name: en-US-AndrewNeural
Gender: Male

Name: en-US-AriaNeural
Gender: Female

Name: en-US-AvaNeural
Gender: Female

Name: en-US-BrianNeural
Gender: Male

Name: en-US-ChristopherNeural
Gender: Male

Name: en-US-EmmaNeural
Gender: Female

Name: en-US-EricNeural
Gender: Male

Name: en-US-GuyNeural
Gender: Male

Name: en-US-JennyNeural
Gender: Female

Name: en-US-MichelleNeural
Gender: Female

Name: en-US-RogerNeural
Gender: Male

Name: en-US-SteffanNeural
Gender: Male

Name: en-ZA-LeahNeural
Gender: Female

Name: en-ZA-LukeNeural
Gender: Male

Name: es-AR-ElenaNeural
Gender: Female

Name: es-AR-TomasNeural
Gender: Male

Name: es-BO-MarceloNeural
Gender: Male

Name: es-BO-SofiaNeural
Gender: Female

Name: es-CL-CatalinaNeural
Gender: Female

Name: es-CL-LorenzoNeural
Gender: Male

Name: es-CO-GonzaloNeural
Gender: Male

Name: es-CO-SalomeNeural
Gender: Female

Name: es-CR-JuanNeural
Gender: Male

Name: es-CR-MariaNeural
Gender: Female

Name: es-CU-BelkysNeural
Gender: Female

Name: es-CU-ManuelNeural
Gender: Male

Name: es-DO-EmilioNeural
Gender: Male

Name: es-DO-RamonaNeural
Gender: Female

Name: es-EC-AndreaNeural
Gender: Female

Name: es-EC-LuisNeural
Gender: Male

Name: es-ES-AlvaroNeural
Gender: Male

Name: es-ES-ElviraNeural
Gender: Female

Name: es-ES-XimenaNeural
Gender: Female

Name: es-GQ-JavierNeural
Gender: Male

Name: es-GQ-TeresaNeural
Gender: Female

Name: es-GT-AndresNeural
Gender: Male

Name: es-GT-MartaNeural
Gender: Female

Name: es-HN-CarlosNeural
Gender: Male

Name: es-HN-KarlaNeural
Gender: Female

Name: es-MX-DaliaNeural
Gender: Female

Name: es-MX-JorgeNeural
Gender: Male

Name: es-NI-FedericoNeural
Gender: Male

Name: es-NI-YolandaNeural
Gender: Female

Name: es-PA-MargaritaNeural
Gender: Female

Name: es-PA-RobertoNeural
Gender: Male

Name: es-PE-AlexNeural
Gender: Male

Name: es-PE-CamilaNeural
Gender: Female

Name: es-PR-KarinaNeural
Gender: Female

Name: es-PR-VictorNeural
Gender: Male

Name: es-PY-MarioNeural
Gender: Male

Name: es-PY-TaniaNeural
Gender: Female

Name: es-SV-LorenaNeural
Gender: Female

Name: es-SV-RodrigoNeural
Gender: Male

Name: es-US-AlonsoNeural
Gender: Male

Name: es-US-PalomaNeural
Gender: Female

Name: es-UY-MateoNeural
Gender: Male

Name: es-UY-ValentinaNeural
Gender: Female

Name: es-VE-PaolaNeural
Gender: Female

Name: es-VE-SebastianNeural
Gender: Male

Name: et-EE-AnuNeural
Gender: Female

Name: et-EE-KertNeural
Gender: Male

Name: fa-IR-DilaraNeural
Gender: Female

Name: fa-IR-FaridNeural
Gender: Male

Name: fi-FI-HarriNeural
Gender: Male

Name: fi-FI-NooraNeural
Gender: Female

Name: fil-PH-AngeloNeural
Gender: Male

Name: fil-PH-BlessicaNeural
Gender: Female

Name: fr-BE-CharlineNeural
Gender: Female

Name: fr-BE-GerardNeural
Gender: Male

Name: fr-CA-AntoineNeural
Gender: Male

Name: fr-CA-JeanNeural
Gender: Male

Name: fr-CA-SylvieNeural
Gender: Female

Name: fr-CA-ThierryNeural
Gender: Male

Name: fr-CH-ArianeNeural
Gender: Female

Name: fr-CH-FabriceNeural
Gender: Male

Name: fr-FR-DeniseNeural
Gender: Female

Name: fr-FR-EloiseNeural
Gender: Female

Name: fr-FR-HenriNeural
Gender: Male

Name: fr-FR-RemyMultilingualNeural
Gender: Male

Name: fr-FR-VivienneMultilingualNeural
Gender: Female

Name: ga-IE-ColmNeural
Gender: Male

Name: ga-IE-OrlaNeural
Gender: Female

Name: gl-ES-RoiNeural
Gender: Male

Name: gl-ES-SabelaNeural
Gender: Female

Name: gu-IN-DhwaniNeural
Gender: Female

Name: gu-IN-NiranjanNeural
Gender: Male

Name: he-IL-AvriNeural
Gender: Male

Name: he-IL-HilaNeural
Gender: Female

Name: hi-IN-MadhurNeural
Gender: Male

Name: hi-IN-SwaraNeural
Gender: Female

Name: hr-HR-GabrijelaNeural
Gender: Female

Name: hr-HR-SreckoNeural
Gender: Male

Name: hu-HU-NoemiNeural
Gender: Female

Name: hu-HU-TamasNeural
Gender: Male

Name: id-ID-ArdiNeural
Gender: Male

Name: id-ID-GadisNeural
Gender: Female

Name: is-IS-GudrunNeural
Gender: Female

Name: is-IS-GunnarNeural
Gender: Male

Name: it-IT-DiegoNeural
Gender: Male

Name: it-IT-ElsaNeural
Gender: Female

Name: it-IT-GiuseppeNeural
Gender: Male

Name: it-IT-IsabellaNeural
Gender: Female

Name: ja-JP-KeitaNeural
Gender: Male

Name: ja-JP-NanamiNeural
Gender: Female

Name: jv-ID-DimasNeural
Gender: Male

Name: jv-ID-SitiNeural
Gender: Female

Name: ka-GE-EkaNeural
Gender: Female

Name: ka-GE-GiorgiNeural
Gender: Male

Name: kk-KZ-AigulNeural
Gender: Female

Name: kk-KZ-DauletNeural
Gender: Male

Name: km-KH-PisethNeural
Gender: Male

Name: km-KH-SreymomNeural
Gender: Female

Name: kn-IN-GaganNeural
Gender: Male

Name: kn-IN-SapnaNeural
Gender: Female

Name: ko-KR-HyunsuNeural
Gender: Male

Name: ko-KR-InJoonNeural
Gender: Male

Name: ko-KR-SunHiNeural
Gender: Female

Name: lo-LA-ChanthavongNeural
Gender: Male

Name: lo-LA-KeomanyNeural
Gender: Female

Name: lt-LT-LeonasNeural
Gender: Male

Name: lt-LT-OnaNeural
Gender: Female

Name: lv-LV-EveritaNeural
Gender: Female

Name: lv-LV-NilsNeural
Gender: Male

Name: mk-MK-AleksandarNeural
Gender: Male

Name: mk-MK-MarijaNeural
Gender: Female

Name: ml-IN-MidhunNeural
Gender: Male

Name: ml-IN-SobhanaNeural
Gender: Female

Name: mn-MN-BataaNeural
Gender: Male

Name: mn-MN-YesuiNeural
Gender: Female

Name: mr-IN-AarohiNeural
Gender: Female

Name: mr-IN-ManoharNeural
Gender: Male

Name: ms-MY-OsmanNeural
Gender: Male

Name: ms-MY-YasminNeural
Gender: Female

Name: mt-MT-GraceNeural
Gender: Female

Name: mt-MT-JosephNeural
Gender: Male

Name: my-MM-NilarNeural
Gender: Female

Name: my-MM-ThihaNeural
Gender: Male

Name: nb-NO-FinnNeural
Gender: Male

Name: nb-NO-PernilleNeural
Gender: Female

Name: ne-NP-HemkalaNeural
Gender: Female

Name: ne-NP-SagarNeural
Gender: Male

Name: nl-BE-ArnaudNeural
Gender: Male

Name: nl-BE-DenaNeural
Gender: Female

Name: nl-NL-ColetteNeural
Gender: Female

Name: nl-NL-FennaNeural
Gender: Female

Name: nl-NL-MaartenNeural
Gender: Male

Name: pl-PL-MarekNeural
Gender: Male

Name: pl-PL-ZofiaNeural
Gender: Female

Name: ps-AF-GulNawazNeural
Gender: Male

Name: ps-AF-LatifaNeural
Gender: Female

Name: pt-BR-AntonioNeural
Gender: Male

Name: pt-BR-FranciscaNeural
Gender: Female

Name: pt-BR-ThalitaNeural
Gender: Female

Name: pt-PT-DuarteNeural
Gender: Male

Name: pt-PT-RaquelNeural
Gender: Female

Name: ro-RO-AlinaNeural
Gender: Female

Name: ro-RO-EmilNeural
Gender: Male

Name: ru-RU-DmitryNeural
Gender: Male

Name: ru-RU-SvetlanaNeural
Gender: Female

Name: si-LK-SameeraNeural
Gender: Male

Name: si-LK-ThiliniNeural
Gender: Female

Name: sk-SK-LukasNeural
Gender: Male

Name: sk-SK-ViktoriaNeural
Gender: Female

Name: sl-SI-PetraNeural
Gender: Female

Name: sl-SI-RokNeural
Gender: Male

Name: so-SO-MuuseNeural
Gender: Male

Name: so-SO-UbaxNeural
Gender: Female

Name: sq-AL-AnilaNeural
Gender: Female

Name: sq-AL-IlirNeural
Gender: Male

Name: sr-RS-NicholasNeural
Gender: Male

Name: sr-RS-SophieNeural
Gender: Female

Name: su-ID-JajangNeural
Gender: Male

Name: su-ID-TutiNeural
Gender: Female

Name: sv-SE-MattiasNeural
Gender: Male

Name: sv-SE-SofieNeural
Gender: Female

Name: sw-KE-RafikiNeural
Gender: Male

Name: sw-KE-ZuriNeural
Gender: Female

Name: sw-TZ-DaudiNeural
Gender: Male

Name: sw-TZ-RehemaNeural
Gender: Female

Name: ta-IN-PallaviNeural
Gender: Female

Name: ta-IN-ValluvarNeural
Gender: Male

Name: ta-LK-KumarNeural
Gender: Male

Name: ta-LK-SaranyaNeural
Gender: Female

Name: ta-MY-KaniNeural
Gender: Female

Name: ta-MY-SuryaNeural
Gender: Male

Name: ta-SG-AnbuNeural
Gender: Male

Name: ta-SG-VenbaNeural
Gender: Female

Name: te-IN-MohanNeural
Gender: Male

Name: te-IN-ShrutiNeural
Gender: Female

Name: th-TH-NiwatNeural
Gender: Male

Name: th-TH-PremwadeeNeural
Gender: Female

Name: tr-TR-AhmetNeural
Gender: Male

Name: tr-TR-EmelNeural
Gender: Female

Name: uk-UA-OstapNeural
Gender: Male

Name: uk-UA-PolinaNeural
Gender: Female

Name: ur-IN-GulNeural
Gender: Female

Name: ur-IN-SalmanNeural
Gender: Male

Name: ur-PK-AsadNeural
Gender: Male

Name: ur-PK-UzmaNeural
Gender: Female

Name: uz-UZ-MadinaNeural
Gender: Female

Name: uz-UZ-SardorNeural
Gender: Male

Name: vi-VN-HoaiMyNeural
Gender: Female

Name: vi-VN-NamMinhNeural
Gender: Male

Name: zh-CN-XiaoxiaoNeural
Gender: Female

Name: zh-CN-XiaoyiNeural
Gender: Female

Name: zh-CN-YunjianNeural
Gender: Male

Name: zh-CN-YunxiNeural
Gender: Male

Name: zh-CN-YunxiaNeural
Gender: Male

Name: zh-CN-YunyangNeural
Gender: Male

Name: zh-CN-liaoning-XiaobeiNeural
Gender: Female

Name: zh-CN-shaanxi-XiaoniNeural
Gender: Female

Name: zh-HK-HiuGaaiNeural
Gender: Female

Name: zh-HK-HiuMaanNeural
Gender: Female

Name: zh-HK-WanLungNeural
Gender: Male

Name: zh-TW-HsiaoChenNeural
Gender: Female

Name: zh-TW-HsiaoYuNeural
Gender: Female

Name: zh-TW-YunJheNeural
Gender: Male

Name: zu-ZA-ThandoNeural
Gender: Female

Name: zu-ZA-ThembaNeural
Gender: Male
</file>

<file path="resource/public/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>MoneyPrinterTurbo</title>
</head>
<body>
<h1>MoneyPrinterTurbo</h1>
<a href="https://github.com/harry0703/MoneyPrinterTurbo">https://github.com/harry0703/MoneyPrinterTurbo</a>
<p>
    Âè™ÈúÄÊèê‰æõ‰∏Ä‰∏™ËßÜÈ¢ë ‰∏ªÈ¢ò Êàñ ÂÖ≥ÈîÆËØç ÔºåÂ∞±ÂèØ‰ª•ÂÖ®Ëá™Âä®ÁîüÊàêËßÜÈ¢ëÊñáÊ°à„ÄÅËßÜÈ¢ëÁ¥†Êùê„ÄÅËßÜÈ¢ëÂ≠óÂπï„ÄÅËßÜÈ¢ëËÉåÊôØÈü≥‰πêÔºåÁÑ∂ÂêéÂêàÊàê‰∏Ä‰∏™È´òÊ∏ÖÁöÑÁü≠ËßÜÈ¢ë„ÄÇ
</p>

<p>
    Simply provide a topic or keyword for a video, and it will automatically generate the video copy, video materials,
    video subtitles, and video background music before synthesizing a high-definition short video.
</p>
</body>
</html>
</file>

<file path=".dockerignore">
# Exclude common Python files and directories
venv/
__pycache__/
*.pyc
*.pyo
*.pyd
*.pyz
*.pyw
*.pyi
*.egg-info/

# Exclude development and local files
.env
.env.*
*.log
*.db

# Exclude version control system files
.git/
.gitignore
.svn/

storage/
config.toml
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 Harry

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="main.py">
import uvicorn
from loguru import logger

from app.config import config

if __name__ == "__main__":
    logger.info(
        "start server, docs: http://127.0.0.1:" + str(config.listen_port) + "/docs"
    )
    uvicorn.run(
        app="app.asgi:app",
        host=config.listen_host,
        port=config.listen_port,
        reload=config.reload_debug,
        log_level="warning",
    )
</file>

<file path="webui.bat">
@echo off
set CURRENT_DIR=%CD%
echo ***** Current directory: %CURRENT_DIR% *****
set PYTHONPATH=%CURRENT_DIR%

rem set HF_ENDPOINT=https://hf-mirror.com
streamlit run .\webui\Main.py --browser.gatherUsageStats=False --server.enableCORS=True
</file>

<file path="webui.sh">
# If you could not download the model from the official site, you can use the mirror site.
# Just remove the comment of the following line .
# Â¶ÇÊûú‰Ω†Êó†Ê≥ï‰ªéÂÆòÊñπÁΩëÁ´ô‰∏ãËΩΩÊ®°ÂûãÔºå‰Ω†ÂèØ‰ª•‰ΩøÁî®ÈïúÂÉèÁΩëÁ´ô„ÄÇ
# Âè™ÈúÄË¶ÅÁßªÈô§‰∏ãÈù¢‰∏ÄË°åÁöÑÊ≥®ÈáäÂç≥ÂèØ„ÄÇ

# export HF_ENDPOINT=https://hf-mirror.com

streamlit run ./webui/Main.py --browser.serverAddress="0.0.0.0" --server.enableCORS=True --browser.gatherUsageStats=False
</file>

<file path=".github/ISSUE_TEMPLATE/config.yml">
blank_issues_enabled: false
</file>

<file path="app/controllers/manager/base_manager.py">
import threading
from typing import Any, Callable, Dict


class TaskManager:
    def __init__(self, max_concurrent_tasks: int):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.current_tasks = 0
        self.lock = threading.Lock()
        self.queue = self.create_queue()

    def create_queue(self):
        raise NotImplementedError()

    def add_task(self, func: Callable, *args: Any, **kwargs: Any):
        with self.lock:
            if self.current_tasks < self.max_concurrent_tasks:
                print(f"add task: {func.__name__}, current_tasks: {self.current_tasks}")
                self.execute_task(func, *args, **kwargs)
            else:
                print(
                    f"enqueue task: {func.__name__}, current_tasks: {self.current_tasks}"
                )
                self.enqueue({"func": func, "args": args, "kwargs": kwargs})

    def execute_task(self, func: Callable, *args: Any, **kwargs: Any):
        thread = threading.Thread(
            target=self.run_task, args=(func, *args), kwargs=kwargs
        )
        thread.start()

    def run_task(self, func: Callable, *args: Any, **kwargs: Any):
        try:
            with self.lock:
                self.current_tasks += 1
            func(*args, **kwargs)  # call the function here, passing *args and **kwargs.
        finally:
            self.task_done()

    def check_queue(self):
        with self.lock:
            if (
                self.current_tasks < self.max_concurrent_tasks
                and not self.is_queue_empty()
            ):
                task_info = self.dequeue()
                func = task_info["func"]
                args = task_info.get("args", ())
                kwargs = task_info.get("kwargs", {})
                self.execute_task(func, *args, **kwargs)

    def task_done(self):
        with self.lock:
            self.current_tasks -= 1
        self.check_queue()

    def enqueue(self, task: Dict):
        raise NotImplementedError()

    def dequeue(self):
        raise NotImplementedError()

    def is_queue_empty(self):
        raise NotImplementedError()
</file>

<file path="app/controllers/v1/base.py">
from fastapi import APIRouter


def new_router(dependencies=None):
    router = APIRouter()
    router.tags = ["V1"]
    router.prefix = "/api/v1"
    # Â∞ÜËÆ§ËØÅ‰æùËµñÈ°πÂ∫îÁî®‰∫éÊâÄÊúâË∑ØÁî±
    if dependencies:
        router.dependencies = dependencies
    return router
</file>

<file path="app/controllers/v1/llm.py">
from fastapi import Request

from app.controllers.v1.base import new_router
from app.models.schema import (
    VideoScriptRequest,
    VideoScriptResponse,
    VideoTermsRequest,
    VideoTermsResponse,
)
from app.services import llm
from app.utils import utils

# authentication dependency
# router = new_router(dependencies=[Depends(base.verify_token)])
router = new_router()


@router.post(
    "/scripts",
    response_model=VideoScriptResponse,
    summary="Create a script for the video",
)
def generate_video_script(request: Request, body: VideoScriptRequest):
    video_script = llm.generate_script(
        video_subject=body.video_subject,
        language=body.video_language,
        paragraph_number=body.paragraph_number,
    )
    response = {"video_script": video_script}
    return utils.get_response(200, response)


@router.post(
    "/terms",
    response_model=VideoTermsResponse,
    summary="Generate video terms based on the video script",
)
def generate_video_terms(request: Request, body: VideoTermsRequest):
    video_terms = llm.generate_terms(
        video_subject=body.video_subject,
        video_script=body.video_script,
        amount=body.amount,
    )
    response = {"video_terms": video_terms}
    return utils.get_response(200, response)
</file>

<file path="app/controllers/ping.py">
from fastapi import APIRouter, Request

router = APIRouter()


@router.get(
    "/ping",
    tags=["Health Check"],
    description="Ê£ÄÊü•ÊúçÂä°ÂèØÁî®ÊÄß",
    response_description="pong",
)
def ping(request: Request) -> str:
    return "pong"
</file>

<file path="app/models/exception.py">
import traceback
from typing import Any

from loguru import logger


class HttpException(Exception):
    def __init__(
        self, task_id: str, status_code: int, message: str = "", data: Any = None
    ):
        self.message = message
        self.status_code = status_code
        self.data = data
        # Retrieve the exception stack trace information.
        tb_str = traceback.format_exc().strip()
        if not tb_str or tb_str == "NoneType: None":
            msg = f"HttpException: {status_code}, {task_id}, {message}"
        else:
            msg = f"HttpException: {status_code}, {task_id}, {message}\n{tb_str}"

        if status_code == 400:
            logger.warning(msg)
        else:
            logger.error(msg)


class FileNotFoundException(Exception):
    pass
</file>

<file path="app/services/subtitle.py">
import json
import os.path
import re
from timeit import default_timer as timer

from faster_whisper import WhisperModel
from loguru import logger

from app.config import config
from app.utils import utils

model_size = config.whisper.get("model_size", "large-v3")
device = config.whisper.get("device", "cpu")
compute_type = config.whisper.get("compute_type", "int8")
model = None


def create(audio_file, subtitle_file: str = ""):
    global model
    if not model:
        model_path = f"{utils.root_dir()}/models/whisper-{model_size}"
        model_bin_file = f"{model_path}/model.bin"
        if not os.path.isdir(model_path) or not os.path.isfile(model_bin_file):
            model_path = model_size

        logger.info(
            f"loading model: {model_path}, device: {device}, compute_type: {compute_type}"
        )
        try:
            model = WhisperModel(
                model_size_or_path=model_path, device=device, compute_type=compute_type
            )
        except Exception as e:
            logger.error(
                f"failed to load model: {e} \n\n"
                f"********************************************\n"
                f"this may be caused by network issue. \n"
                f"please download the model manually and put it in the 'models' folder. \n"
                f"see [README.md FAQ](https://github.com/harry0703/MoneyPrinterTurbo) for more details.\n"
                f"********************************************\n\n"
            )
            return None

    logger.info(f"start, output file: {subtitle_file}")
    if not subtitle_file:
        subtitle_file = f"{audio_file}.srt"

    segments, info = model.transcribe(
        audio_file,
        beam_size=5,
        word_timestamps=True,
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500),
    )

    logger.info(
        f"detected language: '{info.language}', probability: {info.language_probability:.2f}"
    )

    start = timer()
    subtitles = []

    def recognized(seg_text, seg_start, seg_end):
        seg_text = seg_text.strip()
        if not seg_text:
            return

        msg = "[%.2fs -> %.2fs] %s" % (seg_start, seg_end, seg_text)
        logger.debug(msg)

        subtitles.append(
            {"msg": seg_text, "start_time": seg_start, "end_time": seg_end}
        )

    for segment in segments:
        words_idx = 0
        words_len = len(segment.words)

        seg_start = 0
        seg_end = 0
        seg_text = ""

        if segment.words:
            is_segmented = False
            for word in segment.words:
                if not is_segmented:
                    seg_start = word.start
                    is_segmented = True

                seg_end = word.end
                # If it contains punctuation, then break the sentence.
                seg_text += word.word

                if utils.str_contains_punctuation(word.word):
                    # remove last char
                    seg_text = seg_text[:-1]
                    if not seg_text:
                        continue

                    recognized(seg_text, seg_start, seg_end)

                    is_segmented = False
                    seg_text = ""

                if words_idx == 0 and segment.start < word.start:
                    seg_start = word.start
                if words_idx == (words_len - 1) and segment.end > word.end:
                    seg_end = word.end
                words_idx += 1

        if not seg_text:
            continue

        recognized(seg_text, seg_start, seg_end)

    end = timer()

    diff = end - start
    logger.info(f"complete, elapsed: {diff:.2f} s")

    idx = 1
    lines = []
    for subtitle in subtitles:
        text = subtitle.get("msg")
        if text:
            lines.append(
                utils.text_to_srt(
                    idx, text, subtitle.get("start_time"), subtitle.get("end_time")
                )
            )
            idx += 1

    sub = "\n".join(lines) + "\n"
    with open(subtitle_file, "w", encoding="utf-8") as f:
        f.write(sub)
    logger.info(f"subtitle file created: {subtitle_file}")


def file_to_subtitles(filename):
    if not filename or not os.path.isfile(filename):
        return []

    times_texts = []
    current_times = None
    current_text = ""
    index = 0
    with open(filename, "r", encoding="utf-8") as f:
        for line in f:
            times = re.findall("([0-9]*:[0-9]*:[0-9]*,[0-9]*)", line)
            if times:
                current_times = line
            elif line.strip() == "" and current_times:
                index += 1
                times_texts.append((index, current_times.strip(), current_text.strip()))
                current_times, current_text = None, ""
            elif current_times:
                current_text += line
    return times_texts


def levenshtein_distance(s1, s2):
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)

    if len(s2) == 0:
        return len(s1)

    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row

    return previous_row[-1]


def similarity(a, b):
    distance = levenshtein_distance(a.lower(), b.lower())
    max_length = max(len(a), len(b))
    return 1 - (distance / max_length)


def correct(subtitle_file, video_script):
    subtitle_items = file_to_subtitles(subtitle_file)
    script_lines = utils.split_string_by_punctuations(video_script)

    corrected = False
    new_subtitle_items = []
    script_index = 0
    subtitle_index = 0

    while script_index < len(script_lines) and subtitle_index < len(subtitle_items):
        script_line = script_lines[script_index].strip()
        subtitle_line = subtitle_items[subtitle_index][2].strip()

        if script_line == subtitle_line:
            new_subtitle_items.append(subtitle_items[subtitle_index])
            script_index += 1
            subtitle_index += 1
        else:
            combined_subtitle = subtitle_line
            start_time = subtitle_items[subtitle_index][1].split(" --> ")[0]
            end_time = subtitle_items[subtitle_index][1].split(" --> ")[1]
            next_subtitle_index = subtitle_index + 1

            while next_subtitle_index < len(subtitle_items):
                next_subtitle = subtitle_items[next_subtitle_index][2].strip()
                if similarity(
                    script_line, combined_subtitle + " " + next_subtitle
                ) > similarity(script_line, combined_subtitle):
                    combined_subtitle += " " + next_subtitle
                    end_time = subtitle_items[next_subtitle_index][1].split(" --> ")[1]
                    next_subtitle_index += 1
                else:
                    break

            if similarity(script_line, combined_subtitle) > 0.8:
                logger.warning(
                    f"Merged/Corrected - Script: {script_line}, Subtitle: {combined_subtitle}"
                )
                new_subtitle_items.append(
                    (
                        len(new_subtitle_items) + 1,
                        f"{start_time} --> {end_time}",
                        script_line,
                    )
                )
                corrected = True
            else:
                logger.warning(
                    f"Mismatch - Script: {script_line}, Subtitle: {combined_subtitle}"
                )
                new_subtitle_items.append(
                    (
                        len(new_subtitle_items) + 1,
                        f"{start_time} --> {end_time}",
                        script_line,
                    )
                )
                corrected = True

            script_index += 1
            subtitle_index = next_subtitle_index

    # Process the remaining lines of the script.
    while script_index < len(script_lines):
        logger.warning(f"Extra script line: {script_lines[script_index]}")
        if subtitle_index < len(subtitle_items):
            new_subtitle_items.append(
                (
                    len(new_subtitle_items) + 1,
                    subtitle_items[subtitle_index][1],
                    script_lines[script_index],
                )
            )
            subtitle_index += 1
        else:
            new_subtitle_items.append(
                (
                    len(new_subtitle_items) + 1,
                    "00:00:00,000 --> 00:00:00,000",
                    script_lines[script_index],
                )
            )
        script_index += 1
        corrected = True

    if corrected:
        with open(subtitle_file, "w", encoding="utf-8") as fd:
            for i, item in enumerate(new_subtitle_items):
                fd.write(f"{i + 1}\n{item[1]}\n{item[2]}\n\n")
        logger.info("Subtitle corrected")
    else:
        logger.success("Subtitle is correct")


if __name__ == "__main__":
    task_id = "c12fd1e6-4b0a-4d65-a075-c87abe35a072"
    task_dir = utils.task_dir(task_id)
    subtitle_file = f"{task_dir}/subtitle.srt"
    audio_file = f"{task_dir}/audio.mp3"

    subtitles = file_to_subtitles(subtitle_file)
    print(subtitles)

    script_file = f"{task_dir}/script.json"
    with open(script_file, "r") as f:
        script_content = f.read()
    s = json.loads(script_content)
    script = s.get("script")

    correct(subtitle_file, script)

    subtitle_file = f"{task_dir}/subtitle-test.srt"
    create(audio_file, subtitle_file)
</file>

<file path="app/asgi.py">
"""Application implementation - ASGI."""

import os

from fastapi import FastAPI, Request
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from loguru import logger

from app.config import config
from app.models.exception import HttpException
from app.router import root_api_router
from app.utils import utils


def exception_handler(request: Request, e: HttpException):
    return JSONResponse(
        status_code=e.status_code,
        content=utils.get_response(e.status_code, e.data, e.message),
    )


def validation_exception_handler(request: Request, e: RequestValidationError):
    return JSONResponse(
        status_code=400,
        content=utils.get_response(
            status=400, data=e.errors(), message="field required"
        ),
    )


def get_application() -> FastAPI:
    """Initialize FastAPI application.

    Returns:
       FastAPI: Application object instance.

    """
    instance = FastAPI(
        title=config.project_name,
        description=config.project_description,
        version=config.project_version,
        debug=False,
    )
    instance.include_router(root_api_router)
    instance.add_exception_handler(HttpException, exception_handler)
    instance.add_exception_handler(RequestValidationError, validation_exception_handler)
    return instance


app = get_application()

# Configures the CORS middleware for the FastAPI app
cors_allowed_origins_str = os.getenv("CORS_ALLOWED_ORIGINS", "")
origins = cors_allowed_origins_str.split(",") if cors_allowed_origins_str else ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

task_dir = utils.task_dir()
app.mount(
    "/tasks", StaticFiles(directory=task_dir, html=True, follow_symlink=True), name=""
)

public_dir = utils.public_dir()
app.mount("/", StaticFiles(directory=public_dir, html=True), name="")


@app.on_event("shutdown")
def shutdown_event():
    logger.info("shutdown event")


@app.on_event("startup")
def startup_event():
    logger.info("startup event")
</file>

<file path="docs/MoneyPrinterTurbo.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MoneyPrinterTurbo Setup Guide\n",
        "\n",
        "This notebook will guide you through the process of setting up [MoneyPrinterTurbo](https://github.com/harry0703/MoneyPrinterTurbo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone Repository and Install Dependencies\n",
        "\n",
        "First, we'll clone the repository from GitHub and install all required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Eu-aQarY_B"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/harry0703/MoneyPrinterTurbo.git\n",
        "%cd MoneyPrinterTurbo\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install pyngrok --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure ngrok for Remote Access\n",
        "\n",
        "We'll use ngrok to create a secure tunnel to expose our local Streamlit server to the internet.\n",
        "\n",
        "**Important**: You need to get your authentication token from the [ngrok dashboard](https://dashboard.ngrok.com/get-started/your-authtoken) to use this service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Set your authentication token\n",
        "# Replace \"your_ngrok_auth_token\" with your actual token\n",
        "ngrok.set_auth_token(\"your_ngrok_auth_token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Launch Application and Generate Public URL\n",
        "\n",
        "Now we'll start the Streamlit server and create an ngrok tunnel to make it accessible online:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oahsIOXmwjl9",
        "outputId": "ee23a96c-af21-4207-deb7-9fab69e0c05e"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"üöÄ Starting MoneyPrinterTurbo...\")\n",
        "# Start Streamlit server on port 8501\n",
        "streamlit_proc = subprocess.Popen([\n",
        "    \"streamlit\", \"run\", \"./webui/Main.py\", \"--server.port=8501\"\n",
        "])\n",
        "\n",
        "# Wait for the server to initialize\n",
        "time.sleep(5)\n",
        "\n",
        "print(\"üåê Creating ngrok tunnel to expose the MoneyPrinterTurbo...\")\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "\n",
        "print(\"‚úÖ Deployment complete! Access your MoneyPrinterTurbo at:\")\n",
        "print(public_url)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
</file>

<file path="test/__init__.py">
# Unit test package for test
</file>

<file path=".gitignore">
.DS_Store
/config.toml
/storage/
/.idea/
/app/services/__pycache__
/app/__pycache__/
/app/config/__pycache__/
/app/models/__pycache__/
/app/utils/__pycache__/
/*/__pycache__/*
.vscode
/**/.streamlit
__pycache__
logs/

node_modules
# VuePress ÈªòËÆ§‰∏¥Êó∂Êñá‰ª∂ÁõÆÂΩï
/sites/docs/.vuepress/.temp
# VuePress ÈªòËÆ§ÁºìÂ≠òÁõÆÂΩï
/sites/docs/.vuepress/.cache
# VuePress ÈªòËÆ§ÊûÑÂª∫ÁîüÊàêÁöÑÈùôÊÄÅÊñá‰ª∂ÁõÆÂΩï
/sites/docs/.vuepress/dist
# Ê®°ÂûãÁõÆÂΩï
/models/
./models/*

venv/
.venv
</file>

<file path="docker-compose.yml">
x-common-volumes: &common-volumes
  - ./:/MoneyPrinterTurbo

services:
  webui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: "moneyprinterturbo-webui"
    ports:
      - "8501:8501"
    command: [ "streamlit", "run", "./webui/Main.py","--browser.serverAddress=127.0.0.1","--server.enableCORS=True","--browser.gatherUsageStats=False" ]
    volumes: *common-volumes
    restart: always
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: "moneyprinterturbo-api"
    ports:
      - "8080:8080"
    command: [ "python3", "main.py" ]
    volumes: *common-volumes
    restart: always
</file>

<file path="app/controllers/v1/video.py">
import glob
import os
import pathlib
import shutil
from typing import Union

from fastapi import BackgroundTasks, Depends, Path, Request, UploadFile
from fastapi.params import File
from fastapi.responses import FileResponse, StreamingResponse
from loguru import logger

from app.config import config
from app.controllers import base
from app.controllers.manager.memory_manager import InMemoryTaskManager
from app.controllers.manager.redis_manager import RedisTaskManager
from app.controllers.v1.base import new_router
from app.models.exception import HttpException
from app.models.schema import (
    AudioRequest,
    BgmRetrieveResponse,
    BgmUploadResponse,
    SubtitleRequest,
    TaskDeletionResponse,
    TaskQueryRequest,
    TaskQueryResponse,
    TaskResponse,
    TaskVideoRequest,
)
from app.services import state as sm
from app.services import task as tm
from app.utils import utils

# ËÆ§ËØÅ‰æùËµñÈ°π
# router = new_router(dependencies=[Depends(base.verify_token)])
router = new_router()

_enable_redis = config.app.get("enable_redis", False)
_redis_host = config.app.get("redis_host", "localhost")
_redis_port = config.app.get("redis_port", 6379)
_redis_db = config.app.get("redis_db", 0)
_redis_password = config.app.get("redis_password", None)
_max_concurrent_tasks = config.app.get("max_concurrent_tasks", 5)

redis_url = f"redis://:{_redis_password}@{_redis_host}:{_redis_port}/{_redis_db}"
# Ê†πÊçÆÈÖçÁΩÆÈÄâÊã©ÂêàÈÄÇÁöÑ‰ªªÂä°ÁÆ°ÁêÜÂô®
if _enable_redis:
    task_manager = RedisTaskManager(
        max_concurrent_tasks=_max_concurrent_tasks, redis_url=redis_url
    )
else:
    task_manager = InMemoryTaskManager(max_concurrent_tasks=_max_concurrent_tasks)


@router.post("/videos", response_model=TaskResponse, summary="Generate a short video")
def create_video(
    background_tasks: BackgroundTasks, request: Request, body: TaskVideoRequest
):
    return create_task(request, body, stop_at="video")


@router.post("/subtitle", response_model=TaskResponse, summary="Generate subtitle only")
def create_subtitle(
    background_tasks: BackgroundTasks, request: Request, body: SubtitleRequest
):
    return create_task(request, body, stop_at="subtitle")


@router.post("/audio", response_model=TaskResponse, summary="Generate audio only")
def create_audio(
    background_tasks: BackgroundTasks, request: Request, body: AudioRequest
):
    return create_task(request, body, stop_at="audio")


def create_task(
    request: Request,
    body: Union[TaskVideoRequest, SubtitleRequest, AudioRequest],
    stop_at: str,
):
    task_id = utils.get_uuid()
    request_id = base.get_task_id(request)
    try:
        task = {
            "task_id": task_id,
            "request_id": request_id,
            "params": body.model_dump(),
        }
        sm.state.update_task(task_id)
        task_manager.add_task(tm.start, task_id=task_id, params=body, stop_at=stop_at)
        logger.success(f"Task created: {utils.to_json(task)}")
        return utils.get_response(200, task)
    except ValueError as e:
        raise HttpException(
            task_id=task_id, status_code=400, message=f"{request_id}: {str(e)}"
        )


from fastapi import Query


@router.get("/tasks", response_model=TaskQueryResponse, summary="Get all tasks")
def get_all_tasks(
    request: Request, page: int = Query(1, ge=1), page_size: int = Query(10, ge=1)
):
    request_id = base.get_task_id(request)
    tasks, total = sm.state.get_all_tasks(page, page_size)

    response = {
        "tasks": tasks,
        "total": total,
        "page": page,
        "page_size": page_size,
    }
    return utils.get_response(200, response)


@router.get(
    "/tasks/{task_id}", response_model=TaskQueryResponse, summary="Query task status"
)
def get_task(
    request: Request,
    task_id: str = Path(..., description="Task ID"),
    query: TaskQueryRequest = Depends(),
):
    endpoint = config.app.get("endpoint", "")
    if not endpoint:
        endpoint = str(request.base_url)
    endpoint = endpoint.rstrip("/")

    request_id = base.get_task_id(request)
    task = sm.state.get_task(task_id)
    if task:
        task_dir = utils.task_dir()

        def file_to_uri(file):
            if not file.startswith(endpoint):
                _uri_path = v.replace(task_dir, "tasks").replace("\\", "/")
                _uri_path = f"{endpoint}/{_uri_path}"
            else:
                _uri_path = file
            return _uri_path

        if "videos" in task:
            videos = task["videos"]
            urls = []
            for v in videos:
                urls.append(file_to_uri(v))
            task["videos"] = urls
        if "combined_videos" in task:
            combined_videos = task["combined_videos"]
            urls = []
            for v in combined_videos:
                urls.append(file_to_uri(v))
            task["combined_videos"] = urls
        return utils.get_response(200, task)

    raise HttpException(
        task_id=task_id, status_code=404, message=f"{request_id}: task not found"
    )


@router.delete(
    "/tasks/{task_id}",
    response_model=TaskDeletionResponse,
    summary="Delete a generated short video task",
)
def delete_video(request: Request, task_id: str = Path(..., description="Task ID")):
    request_id = base.get_task_id(request)
    task = sm.state.get_task(task_id)
    if task:
        tasks_dir = utils.task_dir()
        current_task_dir = os.path.join(tasks_dir, task_id)
        if os.path.exists(current_task_dir):
            shutil.rmtree(current_task_dir)

        sm.state.delete_task(task_id)
        logger.success(f"video deleted: {utils.to_json(task)}")
        return utils.get_response(200)

    raise HttpException(
        task_id=task_id, status_code=404, message=f"{request_id}: task not found"
    )


@router.get(
    "/musics", response_model=BgmRetrieveResponse, summary="Retrieve local BGM files"
)
def get_bgm_list(request: Request):
    suffix = "*.mp3"
    song_dir = utils.song_dir()
    files = glob.glob(os.path.join(song_dir, suffix))
    bgm_list = []
    for file in files:
        bgm_list.append(
            {
                "name": os.path.basename(file),
                "size": os.path.getsize(file),
                "file": file,
            }
        )
    response = {"files": bgm_list}
    return utils.get_response(200, response)


@router.post(
    "/musics",
    response_model=BgmUploadResponse,
    summary="Upload the BGM file to the songs directory",
)
def upload_bgm_file(request: Request, file: UploadFile = File(...)):
    request_id = base.get_task_id(request)
    # check file ext
    if file.filename.endswith("mp3"):
        song_dir = utils.song_dir()
        save_path = os.path.join(song_dir, file.filename)
        # save file
        with open(save_path, "wb+") as buffer:
            # If the file already exists, it will be overwritten
            file.file.seek(0)
            buffer.write(file.file.read())
        response = {"file": save_path}
        return utils.get_response(200, response)

    raise HttpException(
        "", status_code=400, message=f"{request_id}: Only *.mp3 files can be uploaded"
    )


@router.get("/stream/{file_path:path}")
async def stream_video(request: Request, file_path: str):
    tasks_dir = utils.task_dir()
    video_path = os.path.join(tasks_dir, file_path)
    range_header = request.headers.get("Range")
    video_size = os.path.getsize(video_path)
    start, end = 0, video_size - 1

    length = video_size
    if range_header:
        range_ = range_header.split("bytes=")[1]
        start, end = [int(part) if part else None for part in range_.split("-")]
        if start is None:
            start = video_size - end
            end = video_size - 1
        if end is None:
            end = video_size - 1
        length = end - start + 1

    def file_iterator(file_path, offset=0, bytes_to_read=None):
        with open(file_path, "rb") as f:
            f.seek(offset, os.SEEK_SET)
            remaining = bytes_to_read or video_size
            while remaining > 0:
                bytes_to_read = min(4096, remaining)
                data = f.read(bytes_to_read)
                if not data:
                    break
                remaining -= len(data)
                yield data

    response = StreamingResponse(
        file_iterator(video_path, start, length), media_type="video/mp4"
    )
    response.headers["Content-Range"] = f"bytes {start}-{end}/{video_size}"
    response.headers["Accept-Ranges"] = "bytes"
    response.headers["Content-Length"] = str(length)
    response.status_code = 206  # Partial Content

    return response


@router.get("/download/{file_path:path}")
async def download_video(_: Request, file_path: str):
    """
    download video
    :param _: Request request
    :param file_path: video file path, eg: /cd1727ed-3473-42a2-a7da-4faafafec72b/final-1.mp4
    :return: video file
    """
    tasks_dir = utils.task_dir()
    video_path = os.path.join(tasks_dir, file_path)
    file_path = pathlib.Path(video_path)
    filename = file_path.stem
    extension = file_path.suffix
    headers = {"Content-Disposition": f"attachment; filename={filename}{extension}"}
    return FileResponse(
        path=video_path,
        headers=headers,
        filename=f"{filename}{extension}",
        media_type=f"video/{extension[1:]}",
    )
</file>

<file path="app/services/utils/video_effects.py">
from moviepy import Clip, vfx


# FadeIn
def fadein_transition(clip: Clip, t: float) -> Clip:
    return clip.with_effects([vfx.FadeIn(t)])


# FadeOut
def fadeout_transition(clip: Clip, t: float) -> Clip:
    return clip.with_effects([vfx.FadeOut(t)])


# SlideIn
def slidein_transition(clip: Clip, t: float, side: str) -> Clip:
    return clip.with_effects([vfx.SlideIn(t, side)])


# SlideOut
def slideout_transition(clip: Clip, t: float, side: str) -> Clip:
    return clip.with_effects([vfx.SlideOut(t, side)])
</file>

<file path="test/services/__init__.py">
# Unit test package for services
</file>

<file path="test/services/test_task.py">
import unittest
import os
import sys
from pathlib import Path

# add project root to python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from app.services import task as tm
from app.models.schema import MaterialInfo, VideoParams

resources_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "resources")


class TestTaskService(unittest.TestCase):
    def setUp(self):
        pass

    def tearDown(self):
        pass

    def test_task_local_materials(self):
        task_id = "00000000-0000-0000-0000-000000000000"
        video_materials = []
        for i in range(1, 4):
            video_materials.append(
                MaterialInfo(
                    provider="local",
                    url=os.path.join(resources_dir, f"{i}.png"),
                    duration=0,
                )
            )

        params = VideoParams(
            video_subject="ÈáëÈí±ÁöÑ‰ΩúÁî®",
            video_script="ÈáëÈí±‰∏ç‰ªÖÊòØ‰∫§Êç¢Â™í‰ªãÔºåÊõ¥ÊòØÁ§æ‰ºöËµÑÊ∫êÁöÑÂàÜÈÖçÂ∑•ÂÖ∑„ÄÇÂÆÉËÉΩÊª°Ë∂≥Âü∫Êú¨ÁîüÂ≠òÈúÄÊ±ÇÔºåÂ¶ÇÈ£üÁâ©Âíå‰ΩèÊàøÔºå‰πüËÉΩÊèê‰æõÊïôËÇ≤„ÄÅÂåªÁñóÁ≠âÊèêÂçáÁîüÊ¥ªÂìÅË¥®ÁöÑÊú∫‰ºö„ÄÇÊã•ÊúâË∂≥Â§üÁöÑÈáëÈí±ÊÑèÂë≥ÁùÄÊõ¥Â§öÈÄâÊã©ÊùÉÔºåÊØîÂ¶ÇËÅå‰∏öËá™Áî±ÊàñÂàõ‰∏öÂèØËÉΩ„ÄÇ‰ΩÜÈáëÈí±ÁöÑ‰ΩúÁî®‰πüÊúâËæπÁïåÔºåÂÆÉÊó†Ê≥ïÁõ¥Êé•Ë¥≠‰π∞Âπ∏Á¶è„ÄÅÂÅ•Â∫∑ÊàñÁúüËØöÁöÑ‰∫∫ÈôÖÂÖ≥Á≥ª„ÄÇËøáÂ∫¶ËøΩÈÄêË¥¢ÂØåÂèØËÉΩÂØºËá¥‰ª∑ÂÄºËßÇÊâ≠Êõ≤ÔºåÂøΩËßÜÁ≤æÁ•ûÂ±ÇÈù¢ÁöÑÈúÄÊ±Ç„ÄÇÁêÜÊÉ≥ÁöÑÁä∂ÊÄÅÊòØÁêÜÊÄßÁúãÂæÖÈáëÈí±ÔºåÂ∞ÜÂÖ∂‰Ωú‰∏∫ÂÆûÁé∞ÁõÆÊ†áÁöÑÂ∑•ÂÖ∑ËÄåÈùûÁªàÊûÅÁõÆÁöÑ„ÄÇ",
            video_terms="money importance, wealth and society, financial freedom, money and happiness, role of money",
            video_aspect="9:16",
            video_concat_mode="random",
            video_transition_mode="None",
            video_clip_duration=3,
            video_count=1,
            video_source="local",
            video_materials=video_materials,
            video_language="",
            voice_name="zh-CN-XiaoxiaoNeural-Female",
            voice_volume=1.0,
            voice_rate=1.0,
            bgm_type="random",
            bgm_file="",
            bgm_volume=0.2,
            subtitle_enabled=True,
            subtitle_position="bottom",
            custom_position=70.0,
            font_name="MicrosoftYaHeiBold.ttc",
            text_fore_color="#FFFFFF",
            text_background_color=True,
            font_size=60,
            stroke_color="#000000",
            stroke_width=1.5,
            n_threads=2,
            paragraph_number=1,
        )
        result = tm.start(task_id=task_id, params=params)
        print(result)


if __name__ == "__main__":
    unittest.main()
</file>

<file path="test/services/test_video.py">
import unittest
import os
import sys
from pathlib import Path
from moviepy import (
    VideoFileClip,
)

# add project root to python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from app.models.schema import MaterialInfo
from app.services import video as vd
from app.utils import utils

resources_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "resources")


class TestVideoService(unittest.TestCase):
    def setUp(self):
        self.test_img_path = os.path.join(resources_dir, "1.png")

    def tearDown(self):
        pass

    def test_preprocess_video(self):
        if not os.path.exists(self.test_img_path):
            self.fail(f"test image not found: {self.test_img_path}")

        # test preprocess_video function
        m = MaterialInfo()
        m.url = self.test_img_path
        m.provider = "local"
        print(m)

        materials = vd.preprocess_video([m], clip_duration=4)
        print(materials)

        # verify result
        self.assertIsNotNone(materials)
        self.assertEqual(len(materials), 1)
        self.assertTrue(materials[0].url.endswith(".mp4"))

        # moviepy get video info
        clip = VideoFileClip(materials[0].url)
        print(clip)

        # clean generated test video file
        if os.path.exists(materials[0].url):
            os.remove(materials[0].url)

    def test_wrap_text(self):
        """test text wrapping function"""
        try:
            font_path = os.path.join(utils.font_dir(), "STHeitiMedium.ttc")
            if not os.path.exists(font_path):
                self.fail(f"font file not found: {font_path}")

            # test english text wrapping
            test_text_en = (
                "This is a test text for wrapping long sentences in english language"
            )

            wrapped_text_en, text_height_en = vd.wrap_text(
                text=test_text_en, max_width=300, font=font_path, fontsize=30
            )
            print(wrapped_text_en, text_height_en)
            # verify text is wrapped
            self.assertIn("\n", wrapped_text_en)

            # test chinese text wrapping
            test_text_zh = (
                "ËøôÊòØ‰∏ÄÊÆµÁî®Êù•ÊµãËØï‰∏≠ÊñáÈïøÂè•Êç¢Ë°åÁöÑÊñáÊú¨ÂÜÖÂÆπÔºåÂ∫îËØ•‰ºöÊ†πÊçÆÂÆΩÂ∫¶ÈôêÂà∂ËøõË°åÊç¢Ë°åÂ§ÑÁêÜ"
            )
            wrapped_text_zh, text_height_zh = vd.wrap_text(
                text=test_text_zh, max_width=300, font=font_path, fontsize=30
            )
            print(wrapped_text_zh, text_height_zh)
            # verify chinese text is wrapped
            self.assertIn("\n", wrapped_text_zh)
        except Exception as e:
            self.fail(f"test wrap_text failed: {str(e)}")


if __name__ == "__main__":
    unittest.main()
</file>

<file path="test/services/test_voice.py">
import asyncio
import unittest
import os
import sys
from pathlib import Path

# add project root to python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from app.utils import utils
from app.services import voice as vs

temp_dir = utils.storage_dir("temp")

text_en = """
What is the meaning of life? 
This question has puzzled philosophers, scientists, and thinkers of all kinds for centuries. 
Throughout history, various cultures and individuals have come up with their interpretations and beliefs around the purpose of life. 
Some say it's to seek happiness and self-fulfillment, while others believe it's about contributing to the welfare of others and making a positive impact in the world. 
Despite the myriad of perspectives, one thing remains clear: the meaning of life is a deeply personal concept that varies from one person to another. 
It's an existential inquiry that encourages us to reflect on our values, desires, and the essence of our existence.
"""

text_zh = """
È¢ÑËÆ°Êú™Êù•3Â§©Ê∑±Âú≥ÂÜ∑Á©∫Ê∞îÊ¥ªÂä®È¢ëÁπÅÔºåÊú™Êù•‰∏§Â§©ÊåÅÁª≠Èò¥Â§©ÊúâÂ∞èÈõ®ÔºåÂá∫Èó®Â∏¶Â•ΩÈõ®ÂÖ∑Ôºõ
10-11Êó•ÊåÅÁª≠Èò¥Â§©ÊúâÂ∞èÈõ®ÔºåÊó•Ê∏©Â∑ÆÂ∞èÔºåÊ∞îÊ∏©Âú®13-17‚ÑÉ‰πãÈó¥Ôºå‰ΩìÊÑüÈò¥ÂáâÔºõ
12Êó•Â§©Ê∞îÁü≠ÊöÇÂ•ΩËΩ¨ÔºåÊó©ÊôöÊ∏ÖÂáâÔºõ
"""

voice_rate = 1.0
voice_volume = 1.0


class TestVoiceService(unittest.TestCase):
    def setUp(self):
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)

    def tearDown(self):
        self.loop.close()

    def test_siliconflow(self):
        voice_name = "siliconflow:FunAudioLLM/CosyVoice2-0.5B:alex-Male"
        voice_name = vs.parse_voice_name(voice_name)

        async def _do():
            parts = voice_name.split(":")
            if len(parts) >= 3:
                model = parts[1]
                # ÁßªÈô§ÊÄßÂà´ÂêéÁºÄÔºå‰æãÂ¶Ç "alex-Male" -> "alex"
                voice_with_gender = parts[2]
                voice = voice_with_gender.split("-")[0]
                # ÊûÑÂª∫ÂÆåÊï¥ÁöÑvoiceÂèÇÊï∞ÔºåÊ†ºÂºè‰∏∫ "model:voice"
                full_voice = f"{model}:{voice}"
                voice_file = f"{temp_dir}/tts-siliconflow-{voice}.mp3"
                subtitle_file = f"{temp_dir}/tts-siliconflow-{voice}.srt"
                sub_maker = vs.siliconflow_tts(
                    text=text_zh,
                    model=model,
                    voice=full_voice,
                    voice_file=voice_file,
                    voice_rate=voice_rate,
                    voice_volume=voice_volume,
                )
                if not sub_maker:
                    self.fail("siliconflow tts failed")
                vs.create_subtitle(
                    sub_maker=sub_maker, text=text_zh, subtitle_file=subtitle_file
                )
                audio_duration = vs.get_audio_duration(sub_maker)
                print(f"voice: {voice_name}, audio duration: {audio_duration}s")
            else:
                self.fail("siliconflow invalid voice name")

        self.loop.run_until_complete(_do())

    def test_azure_tts_v1(self):
        voice_name = "zh-CN-XiaoyiNeural-Female"
        voice_name = vs.parse_voice_name(voice_name)
        print(voice_name)

        voice_file = f"{temp_dir}/tts-azure-v1-{voice_name}.mp3"
        subtitle_file = f"{temp_dir}/tts-azure-v1-{voice_name}.srt"
        sub_maker = vs.azure_tts_v1(
            text=text_zh,
            voice_name=voice_name,
            voice_file=voice_file,
            voice_rate=voice_rate,
        )
        if not sub_maker:
            self.fail("azure tts v1 failed")
        vs.create_subtitle(
            sub_maker=sub_maker, text=text_zh, subtitle_file=subtitle_file
        )
        audio_duration = vs.get_audio_duration(sub_maker)
        print(f"voice: {voice_name}, audio duration: {audio_duration}s")

    def test_azure_tts_v2(self):
        voice_name = "zh-CN-XiaoxiaoMultilingualNeural-V2-Female"
        voice_name = vs.parse_voice_name(voice_name)
        print(voice_name)

        async def _do():
            voice_file = f"{temp_dir}/tts-azure-v2-{voice_name}.mp3"
            subtitle_file = f"{temp_dir}/tts-azure-v2-{voice_name}.srt"
            sub_maker = vs.azure_tts_v2(
                text=text_zh, voice_name=voice_name, voice_file=voice_file
            )
            if not sub_maker:
                self.fail("azure tts v2 failed")
            vs.create_subtitle(
                sub_maker=sub_maker, text=text_zh, subtitle_file=subtitle_file
            )
            audio_duration = vs.get_audio_duration(sub_maker)
            print(f"voice: {voice_name}, audio duration: {audio_duration}s")

        self.loop.run_until_complete(_do())


if __name__ == "__main__":
    # python -m unittest test.services.test_voice.TestVoiceService.test_azure_tts_v1
    # python -m unittest test.services.test_voice.TestVoiceService.test_azure_tts_v2
    unittest.main()
</file>

<file path="test/README.md">
# MoneyPrinterTurbo Test Directory

This directory contains unit tests for the **MoneyPrinterTurbo** project.

## Directory Structure

- `services/`: Tests for components in the `app/services` directory  
  - `test_video.py`: Tests for the video service  
  - `test_task.py`: Tests for the task service  
  - `test_voice.py`: Tests for the voice service  

## Running Tests

You can run the tests using Python‚Äôs built-in `unittest` framework:

```bash
# Run all tests
python -m unittest discover -s test

# Run a specific test file
python -m unittest test/services/test_video.py

# Run a specific test class
python -m unittest test.services.test_video.TestVideoService

# Run a specific test method
python -m unittest test.services.test_video.TestVideoService.test_preprocess_video
````

## Adding New Tests

To add tests for other components, follow these guidelines:

1. Create test files prefixed with `test_` in the appropriate subdirectory
2. Use `unittest.TestCase` as the base class for your test classes
3. Name test methods with the `test_` prefix

## Test Resources

Place any resource files required for testing in the `test/resources` directory.
</file>

<file path="Dockerfile">
# Use an official Python runtime as a parent image
FROM python:3.11-slim-bullseye

# Set the working directory in the container
WORKDIR /MoneyPrinterTurbo

# ËÆæÁΩÆ/MoneyPrinterTurboÁõÆÂΩïÊùÉÈôê‰∏∫777
RUN chmod 777 /MoneyPrinterTurbo

ENV PYTHONPATH="/MoneyPrinterTurbo"

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    imagemagick \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Fix security policy for ImageMagick
RUN sed -i '/<policy domain="path" rights="none" pattern="@\*"/d' /etc/ImageMagick-6/policy.xml

# Copy only the requirements.txt first to leverage Docker cache
COPY requirements.txt ./

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Now copy the rest of the codebase into the image
COPY . .

# Expose the port the app runs on
EXPOSE 8501

# Command to run the application
CMD ["streamlit", "run", "./webui/Main.py","--browser.serverAddress=127.0.0.1","--server.enableCORS=True","--browser.gatherUsageStats=False"]

# 1. Build the Docker image using the following command
# docker build -t moneyprinterturbo .

# 2. Run the Docker container using the following command
## For Linux or MacOS:
# docker run -v $(pwd)/config.toml:/MoneyPrinterTurbo/config.toml -v $(pwd)/storage:/MoneyPrinterTurbo/storage -p 8501:8501 moneyprinterturbo
## For Windows:
# docker run -v ${PWD}/config.toml:/MoneyPrinterTurbo/config.toml -v ${PWD}/storage:/MoneyPrinterTurbo/storage -p 8501:8501 moneyprinterturbo
</file>

<file path=".github/ISSUE_TEMPLATE/feature_request.yml">
name: ‚ú® Â¢ûÂä†ÂäüËÉΩ | Feature Request
description: ‰∏∫Ê≠§È°πÁõÆÊèêÂá∫‰∏Ä‰∏™Êñ∞ÊÉ≥Ê≥ïÊàñÂª∫ËÆÆ | Suggest a new idea for this project
title: "[Feature]: "
labels:
  - enhancement

body:
  - type: textarea
    attributes:
      label: ÈúÄÊ±ÇÊèèËø∞ | Problem Statement
      description: |
        ËØ∑ÊèèËø∞ÊÇ®Â∏åÊúõËß£ÂÜ≥ÁöÑÈóÆÈ¢òÊàñÈúÄÊ±Ç 
        Please describe the problem you want to solve
      placeholder: |
        ÊàëÂú®‰ΩøÁî®ËøáÁ®ã‰∏≠ÈÅáÂà∞‰∫Ü...
        I encountered... when using this project
    validations:
      required: true
  - type: textarea
    attributes:
      label: Âª∫ËÆÆÁöÑËß£ÂÜ≥ÊñπÊ°à | Proposed Solution
      description: |
        ËØ∑ÊèèËø∞ÊÇ®ËÆ§‰∏∫ÂèØË°åÁöÑËß£ÂÜ≥ÊñπÊ°àÊàñÂÆûÁé∞ÊñπÂºè
        Please describe your suggested solution or implementation
      placeholder: |
        ÂèØ‰ª•ËÄÉËôëÊ∑ªÂä†...ÂäüËÉΩÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò
        Consider adding... feature to address this issue
    validations:
      required: true
</file>

<file path="app/models/schema.py">
import warnings
from enum import Enum
from typing import Any, List, Optional, Union

import pydantic
from pydantic import BaseModel

# ÂøΩÁï• Pydantic ÁöÑÁâπÂÆöË≠¶Âëä
warnings.filterwarnings(
    "ignore",
    category=UserWarning,
    message="Field name.*shadows an attribute in parent.*",
)


class VideoConcatMode(str, Enum):
    random = "random"
    sequential = "sequential"


class VideoTransitionMode(str, Enum):
    none = None
    shuffle = "Shuffle"
    fade_in = "FadeIn"
    fade_out = "FadeOut"
    slide_in = "SlideIn"
    slide_out = "SlideOut"


class VideoAspect(str, Enum):
    landscape = "16:9"
    portrait = "9:16"
    square = "1:1"

    def to_resolution(self):
        if self == VideoAspect.landscape.value:
            return 1920, 1080
        elif self == VideoAspect.portrait.value:
            return 1080, 1920
        elif self == VideoAspect.square.value:
            return 1080, 1080
        return 1080, 1920


class _Config:
    arbitrary_types_allowed = True


@pydantic.dataclasses.dataclass(config=_Config)
class MaterialInfo:
    provider: str = "pexels"
    url: str = ""
    duration: int = 0


class VideoParams(BaseModel):
    """
    {
      "video_subject": "",
      "video_aspect": "Ê®™Â±è 16:9ÔºàË•øÁìúËßÜÈ¢ëÔºâ",
      "voice_name": "Â•≥Áîü-ÊôìÊôì",
      "bgm_name": "random",
      "font_name": "STHeitiMedium Èªë‰Ωì-‰∏≠",
      "text_color": "#FFFFFF",
      "font_size": 60,
      "stroke_color": "#000000",
      "stroke_width": 1.5
    }
    """

    video_subject: str
    video_script: str = ""  # Script used to generate the video
    video_terms: Optional[str | list] = None  # Keywords used to generate the video
    video_aspect: Optional[VideoAspect] = VideoAspect.portrait.value
    video_concat_mode: Optional[VideoConcatMode] = VideoConcatMode.random.value
    video_transition_mode: Optional[VideoTransitionMode] = None
    video_clip_duration: Optional[int] = 5
    video_count: Optional[int] = 1

    video_source: Optional[str] = "pexels"
    video_materials: Optional[List[MaterialInfo]] = (
        None  # Materials used to generate the video
    )

    video_language: Optional[str] = ""  # auto detect

    voice_name: Optional[str] = ""
    voice_volume: Optional[float] = 1.0
    voice_rate: Optional[float] = 1.0
    bgm_type: Optional[str] = "random"
    bgm_file: Optional[str] = ""
    bgm_volume: Optional[float] = 0.2

    subtitle_enabled: Optional[bool] = True
    subtitle_position: Optional[str] = "bottom"  # top, bottom, center
    custom_position: float = 70.0
    font_name: Optional[str] = "STHeitiMedium.ttc"
    text_fore_color: Optional[str] = "#FFFFFF"
    text_background_color: Union[bool, str] = True

    font_size: int = 60
    stroke_color: Optional[str] = "#000000"
    stroke_width: float = 1.5
    n_threads: Optional[int] = 2
    paragraph_number: Optional[int] = 1


class SubtitleRequest(BaseModel):
    video_script: str
    video_language: Optional[str] = ""
    voice_name: Optional[str] = "zh-CN-XiaoxiaoNeural-Female"
    voice_volume: Optional[float] = 1.0
    voice_rate: Optional[float] = 1.2
    bgm_type: Optional[str] = "random"
    bgm_file: Optional[str] = ""
    bgm_volume: Optional[float] = 0.2
    subtitle_position: Optional[str] = "bottom"
    font_name: Optional[str] = "STHeitiMedium.ttc"
    text_fore_color: Optional[str] = "#FFFFFF"
    text_background_color: Union[bool, str] = True
    font_size: int = 60
    stroke_color: Optional[str] = "#000000"
    stroke_width: float = 1.5
    video_source: Optional[str] = "local"
    subtitle_enabled: Optional[str] = "true"


class AudioRequest(BaseModel):
    video_script: str
    video_language: Optional[str] = ""
    voice_name: Optional[str] = "zh-CN-XiaoxiaoNeural-Female"
    voice_volume: Optional[float] = 1.0
    voice_rate: Optional[float] = 1.2
    bgm_type: Optional[str] = "random"
    bgm_file: Optional[str] = ""
    bgm_volume: Optional[float] = 0.2
    video_source: Optional[str] = "local"


class VideoScriptParams:
    """
    {
      "video_subject": "Êò•Â§©ÁöÑËä±Êµ∑",
      "video_language": "",
      "paragraph_number": 1
    }
    """

    video_subject: Optional[str] = "Êò•Â§©ÁöÑËä±Êµ∑"
    video_language: Optional[str] = ""
    paragraph_number: Optional[int] = 1


class VideoTermsParams:
    """
    {
      "video_subject": "",
      "video_script": "",
      "amount": 5
    }
    """

    video_subject: Optional[str] = "Êò•Â§©ÁöÑËä±Êµ∑"
    video_script: Optional[str] = (
        "Êò•Â§©ÁöÑËä±Êµ∑ÔºåÂ¶ÇËØóÂ¶ÇÁîªËà¨Â±ïÁé∞Âú®ÁúºÂâç„ÄÇ‰∏áÁâ©Â§çËãèÁöÑÂ≠£ËäÇÈáåÔºåÂ§ßÂú∞Êä´‰∏ä‰∫Ü‰∏ÄË¢≠Áªö‰∏ΩÂ§öÂΩ©ÁöÑÁõõË£Ö„ÄÇÈáëÈªÑÁöÑËøéÊò•„ÄÅÁ≤âÂ´©ÁöÑÊ®±Ëä±„ÄÅÊ¥ÅÁôΩÁöÑÊ¢®Ëä±„ÄÅËâ≥‰∏ΩÁöÑÈÉÅÈáëÈ¶ô‚Ä¶‚Ä¶"
    )
    amount: Optional[int] = 5


class BaseResponse(BaseModel):
    status: int = 200
    message: Optional[str] = "success"
    data: Any = None


class TaskVideoRequest(VideoParams, BaseModel):
    pass


class TaskQueryRequest(BaseModel):
    pass


class VideoScriptRequest(VideoScriptParams, BaseModel):
    pass


class VideoTermsRequest(VideoTermsParams, BaseModel):
    pass


######################################################################################################
######################################################################################################
######################################################################################################
######################################################################################################
class TaskResponse(BaseResponse):
    class TaskResponseData(BaseModel):
        task_id: str

    data: TaskResponseData

    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {"task_id": "6c85c8cc-a77a-42b9-bc30-947815aa0558"},
            },
        }


class TaskQueryResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {
                    "state": 1,
                    "progress": 100,
                    "videos": [
                        "http://127.0.0.1:8080/tasks/6c85c8cc-a77a-42b9-bc30-947815aa0558/final-1.mp4"
                    ],
                    "combined_videos": [
                        "http://127.0.0.1:8080/tasks/6c85c8cc-a77a-42b9-bc30-947815aa0558/combined-1.mp4"
                    ],
                },
            },
        }


class TaskDeletionResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {
                    "state": 1,
                    "progress": 100,
                    "videos": [
                        "http://127.0.0.1:8080/tasks/6c85c8cc-a77a-42b9-bc30-947815aa0558/final-1.mp4"
                    ],
                    "combined_videos": [
                        "http://127.0.0.1:8080/tasks/6c85c8cc-a77a-42b9-bc30-947815aa0558/combined-1.mp4"
                    ],
                },
            },
        }


class VideoScriptResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {
                    "video_script": "Êò•Â§©ÁöÑËä±Êµ∑ÔºåÊòØÂ§ßËá™ÁÑ∂ÁöÑ‰∏ÄÂπÖÁæé‰∏ΩÁîªÂç∑„ÄÇÂú®Ëøô‰∏™Â≠£ËäÇÈáåÔºåÂ§ßÂú∞Â§çËãèÔºå‰∏áÁâ©ÁîüÈïøÔºåËä±Êúµ‰∫âÁõ∏ÁªΩÊîæÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁâá‰∫îÂΩ©ÊñëÊñìÁöÑËä±Êµ∑..."
                },
            },
        }


class VideoTermsResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {"video_terms": ["sky", "tree"]},
            },
        }


class BgmRetrieveResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {
                    "files": [
                        {
                            "name": "output013.mp3",
                            "size": 1891269,
                            "file": "/MoneyPrinterTurbo/resource/songs/output013.mp3",
                        }
                    ]
                },
            },
        }


class BgmUploadResponse(BaseResponse):
    class Config:
        json_schema_extra = {
            "example": {
                "status": 200,
                "message": "success",
                "data": {"file": "/MoneyPrinterTurbo/resource/songs/example.mp3"},
            },
        }
</file>

<file path="app/services/material.py">
import os
import random
from typing import List
from urllib.parse import urlencode

import requests
from loguru import logger
from moviepy.video.io.VideoFileClip import VideoFileClip

from app.config import config
from app.models.schema import MaterialInfo, VideoAspect, VideoConcatMode
from app.utils import utils

requested_count = 0


def get_api_key(cfg_key: str):
    api_keys = config.app.get(cfg_key)
    if not api_keys:
        raise ValueError(
            f"\n\n##### {cfg_key} is not set #####\n\nPlease set it in the config.toml file: {config.config_file}\n\n"
            f"{utils.to_json(config.app)}"
        )

    # if only one key is provided, return it
    if isinstance(api_keys, str):
        return api_keys

    global requested_count
    requested_count += 1
    return api_keys[requested_count % len(api_keys)]


def search_videos_pexels(
    search_term: str,
    minimum_duration: int,
    video_aspect: VideoAspect = VideoAspect.portrait,
) -> List[MaterialInfo]:
    aspect = VideoAspect(video_aspect)
    video_orientation = aspect.name
    video_width, video_height = aspect.to_resolution()
    api_key = get_api_key("pexels_api_keys")
    headers = {
        "Authorization": api_key,
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    }
    # Build URL
    params = {"query": search_term, "per_page": 20, "orientation": video_orientation}
    query_url = f"https://api.pexels.com/videos/search?{urlencode(params)}"
    logger.info(f"searching videos: {query_url}, with proxies: {config.proxy}")

    try:
        r = requests.get(
            query_url,
            headers=headers,
            proxies=config.proxy,
            verify=False,
            timeout=(30, 60),
        )
        response = r.json()
        video_items = []
        if "videos" not in response:
            logger.error(f"search videos failed: {response}")
            return video_items
        videos = response["videos"]
        # loop through each video in the result
        for v in videos:
            duration = v["duration"]
            # check if video has desired minimum duration
            if duration < minimum_duration:
                continue
            video_files = v["video_files"]
            # loop through each url to determine the best quality
            for video in video_files:
                w = int(video["width"])
                h = int(video["height"])
                if w == video_width and h == video_height:
                    item = MaterialInfo()
                    item.provider = "pexels"
                    item.url = video["link"]
                    item.duration = duration
                    video_items.append(item)
                    break
        return video_items
    except Exception as e:
        logger.error(f"search videos failed: {str(e)}")

    return []


def search_videos_pixabay(
    search_term: str,
    minimum_duration: int,
    video_aspect: VideoAspect = VideoAspect.portrait,
) -> List[MaterialInfo]:
    aspect = VideoAspect(video_aspect)

    video_width, video_height = aspect.to_resolution()

    api_key = get_api_key("pixabay_api_keys")
    # Build URL
    params = {
        "q": search_term,
        "video_type": "all",  # Accepted values: "all", "film", "animation"
        "per_page": 50,
        "key": api_key,
    }
    query_url = f"https://pixabay.com/api/videos/?{urlencode(params)}"
    logger.info(f"searching videos: {query_url}, with proxies: {config.proxy}")

    try:
        r = requests.get(
            query_url, proxies=config.proxy, verify=False, timeout=(30, 60)
        )
        response = r.json()
        video_items = []
        if "hits" not in response:
            logger.error(f"search videos failed: {response}")
            return video_items
        videos = response["hits"]
        # loop through each video in the result
        for v in videos:
            duration = v["duration"]
            # check if video has desired minimum duration
            if duration < minimum_duration:
                continue
            video_files = v["videos"]
            # loop through each url to determine the best quality
            for video_type in video_files:
                video = video_files[video_type]
                w = int(video["width"])
                # h = int(video["height"])
                if w >= video_width:
                    item = MaterialInfo()
                    item.provider = "pixabay"
                    item.url = video["url"]
                    item.duration = duration
                    video_items.append(item)
                    break
        return video_items
    except Exception as e:
        logger.error(f"search videos failed: {str(e)}")

    return []


def save_video(video_url: str, save_dir: str = "") -> str:
    if not save_dir:
        save_dir = utils.storage_dir("cache_videos")

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    url_without_query = video_url.split("?")[0]
    url_hash = utils.md5(url_without_query)
    video_id = f"vid-{url_hash}"
    video_path = f"{save_dir}/{video_id}.mp4"

    # if video already exists, return the path
    if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
        logger.info(f"video already exists: {video_path}")
        return video_path

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
    }

    # if video does not exist, download it
    with open(video_path, "wb") as f:
        f.write(
            requests.get(
                video_url,
                headers=headers,
                proxies=config.proxy,
                verify=False,
                timeout=(60, 240),
            ).content
        )

    if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
        try:
            clip = VideoFileClip(video_path)
            duration = clip.duration
            fps = clip.fps
            clip.close()
            if duration > 0 and fps > 0:
                return video_path
        except Exception as e:
            try:
                os.remove(video_path)
            except Exception:
                pass
            logger.warning(f"invalid video file: {video_path} => {str(e)}")
    return ""


def download_videos(
    task_id: str,
    search_terms: List[str],
    source: str = "pexels",
    video_aspect: VideoAspect = VideoAspect.portrait,
    video_contact_mode: VideoConcatMode = VideoConcatMode.random,
    audio_duration: float = 0.0,
    max_clip_duration: int = 5,
) -> List[str]:
    valid_video_items = []
    valid_video_urls = []
    found_duration = 0.0
    search_videos = search_videos_pexels
    if source == "pixabay":
        search_videos = search_videos_pixabay

    for search_term in search_terms:
        video_items = search_videos(
            search_term=search_term,
            minimum_duration=max_clip_duration,
            video_aspect=video_aspect,
        )
        logger.info(f"found {len(video_items)} videos for '{search_term}'")

        for item in video_items:
            if item.url not in valid_video_urls:
                valid_video_items.append(item)
                valid_video_urls.append(item.url)
                found_duration += item.duration

    logger.info(
        f"found total videos: {len(valid_video_items)}, required duration: {audio_duration} seconds, found duration: {found_duration} seconds"
    )
    video_paths = []

    material_directory = config.app.get("material_directory", "").strip()
    if material_directory == "task":
        material_directory = utils.task_dir(task_id)
    elif material_directory and not os.path.isdir(material_directory):
        material_directory = ""

    if video_contact_mode.value == VideoConcatMode.random.value:
        random.shuffle(valid_video_items)

    total_duration = 0.0
    for item in valid_video_items:
        try:
            logger.info(f"downloading video: {item.url}")
            saved_video_path = save_video(
                video_url=item.url, save_dir=material_directory
            )
            if saved_video_path:
                logger.info(f"video saved: {saved_video_path}")
                video_paths.append(saved_video_path)
                seconds = min(max_clip_duration, item.duration)
                total_duration += seconds
                if total_duration > audio_duration:
                    logger.info(
                        f"total duration of downloaded videos: {total_duration} seconds, skip downloading more"
                    )
                    break
        except Exception as e:
            logger.error(f"failed to download video: {utils.to_json(item)} => {str(e)}")
    logger.success(f"downloaded {len(video_paths)} videos")
    return video_paths


if __name__ == "__main__":
    download_videos(
        "test123", ["Money Exchange Medium"], audio_duration=100, source="pixabay"
    )
</file>

<file path="app/services/state.py">
import ast
from abc import ABC, abstractmethod

from app.config import config
from app.models import const


# Base class for state management
class BaseState(ABC):
    @abstractmethod
    def update_task(self, task_id: str, state: int, progress: int = 0, **kwargs):
        pass

    @abstractmethod
    def get_task(self, task_id: str):
        pass

    @abstractmethod
    def get_all_tasks(self, page: int, page_size: int):
        pass


# Memory state management
class MemoryState(BaseState):
    def __init__(self):
        self._tasks = {}

    def get_all_tasks(self, page: int, page_size: int):
        start = (page - 1) * page_size
        end = start + page_size
        tasks = list(self._tasks.values())
        total = len(tasks)
        return tasks[start:end], total

    def update_task(
        self,
        task_id: str,
        state: int = const.TASK_STATE_PROCESSING,
        progress: int = 0,
        **kwargs,
    ):
        progress = int(progress)
        if progress > 100:
            progress = 100

        self._tasks[task_id] = {
            "task_id": task_id,
            "state": state,
            "progress": progress,
            **kwargs,
        }

    def get_task(self, task_id: str):
        return self._tasks.get(task_id, None)

    def delete_task(self, task_id: str):
        if task_id in self._tasks:
            del self._tasks[task_id]


# Redis state management
class RedisState(BaseState):
    def __init__(self, host="localhost", port=6379, db=0, password=None):
        import redis

        self._redis = redis.StrictRedis(host=host, port=port, db=db, password=password)

    def get_all_tasks(self, page: int, page_size: int):
        start = (page - 1) * page_size
        end = start + page_size
        tasks = []
        cursor = 0
        total = 0
        while True:
            cursor, keys = self._redis.scan(cursor, count=page_size)
            total += len(keys)
            if total > start:
                for key in keys[max(0, start - total) : end - total]:
                    task_data = self._redis.hgetall(key)
                    task = {
                        k.decode("utf-8"): self._convert_to_original_type(v)
                        for k, v in task_data.items()
                    }
                    tasks.append(task)
                    if len(tasks) >= page_size:
                        break
            if cursor == 0 or len(tasks) >= page_size:
                break
        return tasks, total

    def update_task(
        self,
        task_id: str,
        state: int = const.TASK_STATE_PROCESSING,
        progress: int = 0,
        **kwargs,
    ):
        progress = int(progress)
        if progress > 100:
            progress = 100

        fields = {
            "task_id": task_id,
            "state": state,
            "progress": progress,
            **kwargs,
        }

        for field, value in fields.items():
            self._redis.hset(task_id, field, str(value))

    def get_task(self, task_id: str):
        task_data = self._redis.hgetall(task_id)
        if not task_data:
            return None

        task = {
            key.decode("utf-8"): self._convert_to_original_type(value)
            for key, value in task_data.items()
        }
        return task

    def delete_task(self, task_id: str):
        self._redis.delete(task_id)

    @staticmethod
    def _convert_to_original_type(value):
        """
        Convert the value from byte string to its original data type.
        You can extend this method to handle other data types as needed.
        """
        value_str = value.decode("utf-8")

        try:
            # try to convert byte string array to list
            return ast.literal_eval(value_str)
        except (ValueError, SyntaxError):
            pass

        if value_str.isdigit():
            return int(value_str)
        # Add more conversions here if needed
        return value_str


# Global state
_enable_redis = config.app.get("enable_redis", False)
_redis_host = config.app.get("redis_host", "localhost")
_redis_port = config.app.get("redis_port", 6379)
_redis_db = config.app.get("redis_db", 0)
_redis_password = config.app.get("redis_password", None)

state = (
    RedisState(
        host=_redis_host, port=_redis_port, db=_redis_db, password=_redis_password
    )
    if _enable_redis
    else MemoryState()
)
</file>

<file path="app/utils/utils.py">
import json
import locale
import os
from pathlib import Path
import threading
from typing import Any
from uuid import uuid4

import urllib3
from loguru import logger

from app.models import const

urllib3.disable_warnings()


def get_response(status: int, data: Any = None, message: str = ""):
    obj = {
        "status": status,
    }
    if data:
        obj["data"] = data
    if message:
        obj["message"] = message
    return obj


def to_json(obj):
    try:
        # Define a helper function to handle different types of objects
        def serialize(o):
            # If the object is a serializable type, return it directly
            if isinstance(o, (int, float, bool, str)) or o is None:
                return o
            # If the object is binary data, convert it to a base64-encoded string
            elif isinstance(o, bytes):
                return "*** binary data ***"
            # If the object is a dictionary, recursively process each key-value pair
            elif isinstance(o, dict):
                return {k: serialize(v) for k, v in o.items()}
            # If the object is a list or tuple, recursively process each element
            elif isinstance(o, (list, tuple)):
                return [serialize(item) for item in o]
            # If the object is a custom type, attempt to return its __dict__ attribute
            elif hasattr(o, "__dict__"):
                return serialize(o.__dict__)
            # Return None for other cases (or choose to raise an exception)
            else:
                return None

        # Use the serialize function to process the input object
        serialized_obj = serialize(obj)

        # Serialize the processed object into a JSON string
        return json.dumps(serialized_obj, ensure_ascii=False, indent=4)
    except Exception:
        return None


def get_uuid(remove_hyphen: bool = False):
    u = str(uuid4())
    if remove_hyphen:
        u = u.replace("-", "")
    return u


def root_dir():
    return os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))


def storage_dir(sub_dir: str = "", create: bool = False):
    d = os.path.join(root_dir(), "storage")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if create and not os.path.exists(d):
        os.makedirs(d)

    return d


def resource_dir(sub_dir: str = ""):
    d = os.path.join(root_dir(), "resource")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    return d


def task_dir(sub_dir: str = ""):
    d = os.path.join(storage_dir(), "tasks")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def font_dir(sub_dir: str = ""):
    d = resource_dir("fonts")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def song_dir(sub_dir: str = ""):
    d = resource_dir("songs")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def public_dir(sub_dir: str = ""):
    d = resource_dir("public")
    if sub_dir:
        d = os.path.join(d, sub_dir)
    if not os.path.exists(d):
        os.makedirs(d)
    return d


def run_in_background(func, *args, **kwargs):
    def run():
        try:
            func(*args, **kwargs)
        except Exception as e:
            logger.error(f"run_in_background error: {e}")

    thread = threading.Thread(target=run)
    thread.start()
    return thread


def time_convert_seconds_to_hmsm(seconds) -> str:
    hours = int(seconds // 3600)
    seconds = seconds % 3600
    minutes = int(seconds // 60)
    milliseconds = int(seconds * 1000) % 1000
    seconds = int(seconds % 60)
    return "{:02d}:{:02d}:{:02d},{:03d}".format(hours, minutes, seconds, milliseconds)


def text_to_srt(idx: int, msg: str, start_time: float, end_time: float) -> str:
    start_time = time_convert_seconds_to_hmsm(start_time)
    end_time = time_convert_seconds_to_hmsm(end_time)
    srt = """%d
%s --> %s
%s
        """ % (
        idx,
        start_time,
        end_time,
        msg,
    )
    return srt


def str_contains_punctuation(word):
    for p in const.PUNCTUATIONS:
        if p in word:
            return True
    return False


def split_string_by_punctuations(s):
    result = []
    txt = ""

    previous_char = ""
    next_char = ""
    for i in range(len(s)):
        char = s[i]
        if char == "\n":
            result.append(txt.strip())
            txt = ""
            continue

        if i > 0:
            previous_char = s[i - 1]
        if i < len(s) - 1:
            next_char = s[i + 1]

        if char == "." and previous_char.isdigit() and next_char.isdigit():
            # # In the case of "withdraw 10,000, charged at 2.5% fee", the dot in "2.5" should not be treated as a line break marker
            txt += char
            continue

        if char not in const.PUNCTUATIONS:
            txt += char
        else:
            result.append(txt.strip())
            txt = ""
    result.append(txt.strip())
    # filter empty string
    result = list(filter(None, result))
    return result


def md5(text):
    import hashlib

    return hashlib.md5(text.encode("utf-8")).hexdigest()


def get_system_locale():
    try:
        loc = locale.getdefaultlocale()
        # zh_CN, zh_TW return zh
        # en_US, en_GB return en
        language_code = loc[0].split("_")[0]
        return language_code
    except Exception:
        return "en"


def load_locales(i18n_dir):
    _locales = {}
    for root, dirs, files in os.walk(i18n_dir):
        for file in files:
            if file.endswith(".json"):
                lang = file.split(".")[0]
                with open(os.path.join(root, file), "r", encoding="utf-8") as f:
                    _locales[lang] = json.loads(f.read())
    return _locales


def parse_extension(filename):
    return Path(filename).suffix.lower().lstrip(".")
</file>

<file path=".github/ISSUE_TEMPLATE/bug_report.yml">
name: üêõ Bug | Bug Report
description: Êä•ÂëäÈîôËØØÊàñÂºÇÂ∏∏ÈóÆÈ¢ò | Report an error or unexpected behavior
title: "[Bug]: "
labels:
  - bug

body:
  - type: markdown
    attributes:
      value: |
        **Êèê‰∫§ÈóÆÈ¢òÂâçÔºåËØ∑Á°Æ‰øùÊÇ®Â∑≤ÈòÖËØª‰ª•‰∏ãÊñáÊ°£Ôºö[Getting Started (English)](https://github.com/harry0703/MoneyPrinterTurbo/blob/main/README-en.md#system-requirements-) Êàñ [Âø´ÈÄüÂºÄÂßã (‰∏≠Êñá)](https://github.com/harry0703/MoneyPrinterTurbo/blob/main/README.md#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B-)„ÄÇ**
        
        **Before submitting an issue, please make sure you've read the following documentation: [Getting Started (English)](https://github.com/harry0703/MoneyPrinterTurbo/blob/main/README-en.md#system-requirements-) or [Âø´ÈÄüÂºÄÂßã (Chinese)](https://github.com/harry0703/MoneyPrinterTurbo/blob/main/README.md#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B-).**
        
  - type: textarea
    attributes:
      label: ÈóÆÈ¢òÊèèËø∞ | Current Behavior
      description: |
        ÊèèËø∞ÊÇ®ÈÅáÂà∞ÁöÑÈóÆÈ¢ò
        Describe the issue you're experiencing
      placeholder: |
        ÂΩìÊàëÊâßË°å...Êìç‰ΩúÊó∂ÔºåÁ®ãÂ∫èÂá∫Áé∞‰∫Ü...ÈóÆÈ¢ò
        When I perform..., the program shows...
    validations:
      required: true
  - type: textarea
    attributes:
      label: ÈáçÁé∞Ê≠•È™§ | Steps to Reproduce
      description: |
        ËØ¶ÁªÜÊèèËø∞Â¶Ç‰ΩïÈáçÁé∞Ê≠§ÈóÆÈ¢ò
        Describe in detail how to reproduce this issue
      placeholder: |
        1. ÊâìÂºÄ...
        2. ÁÇπÂáª...
        3. Âá∫Áé∞ÈîôËØØ...
        
        1. Open...
        2. Click on...
        3. Error occurs...
    validations:
      required: true
  - type: textarea
    attributes:
      label: ÈîôËØØÊó•Âøó | Error Logs
      description: |
        ËØ∑Êèê‰æõÁõ∏ÂÖ≥ÈîôËØØ‰ø°ÊÅØÊàñÊó•ÂøóÔºàÊ≥®ÊÑè‰∏çË¶ÅÂåÖÂê´ÊïèÊÑü‰ø°ÊÅØÔºâ
        Please provide any error messages or logs (be careful not to include sensitive information)
      placeholder: |
        ÈîôËØØ‰ø°ÊÅØ„ÄÅÊó•ÂøóÊàñÊà™Âõæ...
        Error messages, logs, or screenshots...
    validations:
      required: true
  - type: input
    attributes:
      label: Python ÁâàÊú¨ | Python Version
      description: |
        ÊÇ®‰ΩøÁî®ÁöÑ Python ÁâàÊú¨
        The Python version you're using
      placeholder: v3.13.0, v3.10.0, etc.
    validations:
      required: true
  - type: input
    attributes:
      label: Êìç‰ΩúÁ≥ªÁªü | Operating System
      description: |
        ÊÇ®ÁöÑÊìç‰ΩúÁ≥ªÁªü‰ø°ÊÅØ
        Your operating system information
      placeholder: macOS 14.1, Windows 11, Ubuntu 22.04, etc.
    validations:
      required: true
  - type: input
    attributes:
      label: MoneyPrinterTurbo ÁâàÊú¨ | Version
      description: |
        ÊÇ®‰ΩøÁî®ÁöÑ MoneyPrinterTurbo ÁâàÊú¨
        The version of MoneyPrinterTurbo you're using
      placeholder: v1.2.2, etc.
    validations:
      required: true
  - type: textarea
    attributes:
      label: Ë°•ÂÖÖ‰ø°ÊÅØ | Additional Information
      description: |
        ÂÖ∂‰ªñÂØπËß£ÂÜ≥ÈóÆÈ¢òÊúâÂ∏ÆÂä©ÁöÑ‰ø°ÊÅØÔºàÂ¶ÇÊà™Âõæ„ÄÅËßÜÈ¢ëÁ≠âÔºâ
        Any other information that might help solve the issue (screenshots, videos, etc.)
    validations:
      required: false
</file>

<file path="app/services/task.py">
import math
import os.path
import re
from os import path

from loguru import logger

from app.config import config
from app.models import const
from app.models.schema import VideoConcatMode, VideoParams
from app.services import llm, material, subtitle, video, voice
from app.services import state as sm
from app.utils import utils


def generate_script(task_id, params):
    logger.info("\n\n## generating video script")
    video_script = params.video_script.strip()
    if not video_script:
        video_script = llm.generate_script(
            video_subject=params.video_subject,
            language=params.video_language,
            paragraph_number=params.paragraph_number,
        )
    else:
        logger.debug(f"video script: \n{video_script}")

    if not video_script:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        logger.error("failed to generate video script.")
        return None

    return video_script


def generate_terms(task_id, params, video_script):
    logger.info("\n\n## generating video terms")
    video_terms = params.video_terms
    if not video_terms:
        video_terms = llm.generate_terms(
            video_subject=params.video_subject, video_script=video_script, amount=5
        )
    else:
        if isinstance(video_terms, str):
            video_terms = [term.strip() for term in re.split(r"[,Ôºå]", video_terms)]
        elif isinstance(video_terms, list):
            video_terms = [term.strip() for term in video_terms]
        else:
            raise ValueError("video_terms must be a string or a list of strings.")

        logger.debug(f"video terms: {utils.to_json(video_terms)}")

    if not video_terms:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        logger.error("failed to generate video terms.")
        return None

    return video_terms


def save_script_data(task_id, video_script, video_terms, params):
    script_file = path.join(utils.task_dir(task_id), "script.json")
    script_data = {
        "script": video_script,
        "search_terms": video_terms,
        "params": params,
    }

    with open(script_file, "w", encoding="utf-8") as f:
        f.write(utils.to_json(script_data))


def generate_audio(task_id, params, video_script):
    logger.info("\n\n## generating audio")
    audio_file = path.join(utils.task_dir(task_id), "audio.mp3")
    sub_maker = voice.tts(
        text=video_script,
        voice_name=voice.parse_voice_name(params.voice_name),
        voice_rate=params.voice_rate,
        voice_file=audio_file,
    )
    if sub_maker is None:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        logger.error(
            """failed to generate audio:
1. check if the language of the voice matches the language of the video script.
2. check if the network is available. If you are in China, it is recommended to use a VPN and enable the global traffic mode.
        """.strip()
        )
        return None, None, None

    audio_duration = math.ceil(voice.get_audio_duration(sub_maker))
    return audio_file, audio_duration, sub_maker


def generate_subtitle(task_id, params, video_script, sub_maker, audio_file):
    if not params.subtitle_enabled:
        return ""

    subtitle_path = path.join(utils.task_dir(task_id), "subtitle.srt")
    subtitle_provider = config.app.get("subtitle_provider", "edge").strip().lower()
    logger.info(f"\n\n## generating subtitle, provider: {subtitle_provider}")

    subtitle_fallback = False
    if subtitle_provider == "edge":
        voice.create_subtitle(
            text=video_script, sub_maker=sub_maker, subtitle_file=subtitle_path
        )
        if not os.path.exists(subtitle_path):
            subtitle_fallback = True
            logger.warning("subtitle file not found, fallback to whisper")

    if subtitle_provider == "whisper" or subtitle_fallback:
        subtitle.create(audio_file=audio_file, subtitle_file=subtitle_path)
        logger.info("\n\n## correcting subtitle")
        subtitle.correct(subtitle_file=subtitle_path, video_script=video_script)

    subtitle_lines = subtitle.file_to_subtitles(subtitle_path)
    if not subtitle_lines:
        logger.warning(f"subtitle file is invalid: {subtitle_path}")
        return ""

    return subtitle_path


def get_video_materials(task_id, params, video_terms, audio_duration):
    if params.video_source == "local":
        logger.info("\n\n## preprocess local materials")
        materials = video.preprocess_video(
            materials=params.video_materials, clip_duration=params.video_clip_duration
        )
        if not materials:
            sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
            logger.error(
                "no valid materials found, please check the materials and try again."
            )
            return None
        return [material_info.url for material_info in materials]
    else:
        logger.info(f"\n\n## downloading videos from {params.video_source}")
        downloaded_videos = material.download_videos(
            task_id=task_id,
            search_terms=video_terms,
            source=params.video_source,
            video_aspect=params.video_aspect,
            video_contact_mode=params.video_concat_mode,
            audio_duration=audio_duration * params.video_count,
            max_clip_duration=params.video_clip_duration,
        )
        if not downloaded_videos:
            sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
            logger.error(
                "failed to download videos, maybe the network is not available. if you are in China, please use a VPN."
            )
            return None
        return downloaded_videos


def generate_final_videos(
    task_id, params, downloaded_videos, audio_file, subtitle_path
):
    final_video_paths = []
    combined_video_paths = []
    video_concat_mode = (
        params.video_concat_mode if params.video_count == 1 else VideoConcatMode.random
    )
    video_transition_mode = params.video_transition_mode

    _progress = 50
    for i in range(params.video_count):
        index = i + 1
        combined_video_path = path.join(
            utils.task_dir(task_id), f"combined-{index}.mp4"
        )
        logger.info(f"\n\n## combining video: {index} => {combined_video_path}")
        video.combine_videos(
            combined_video_path=combined_video_path,
            video_paths=downloaded_videos,
            audio_file=audio_file,
            video_aspect=params.video_aspect,
            video_concat_mode=video_concat_mode,
            video_transition_mode=video_transition_mode,
            max_clip_duration=params.video_clip_duration,
            threads=params.n_threads,
        )

        _progress += 50 / params.video_count / 2
        sm.state.update_task(task_id, progress=_progress)

        final_video_path = path.join(utils.task_dir(task_id), f"final-{index}.mp4")

        logger.info(f"\n\n## generating video: {index} => {final_video_path}")
        video.generate_video(
            video_path=combined_video_path,
            audio_path=audio_file,
            subtitle_path=subtitle_path,
            output_file=final_video_path,
            params=params,
        )

        _progress += 50 / params.video_count / 2
        sm.state.update_task(task_id, progress=_progress)

        final_video_paths.append(final_video_path)
        combined_video_paths.append(combined_video_path)

    return final_video_paths, combined_video_paths


def start(task_id, params: VideoParams, stop_at: str = "video"):
    logger.info(f"start task: {task_id}, stop_at: {stop_at}")
    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=5)

    if type(params.video_concat_mode) is str:
        params.video_concat_mode = VideoConcatMode(params.video_concat_mode)

    # 1. Generate script
    video_script = generate_script(task_id, params)
    if not video_script or "Error: " in video_script:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        return

    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=10)

    if stop_at == "script":
        sm.state.update_task(
            task_id, state=const.TASK_STATE_COMPLETE, progress=100, script=video_script
        )
        return {"script": video_script}

    # 2. Generate terms
    video_terms = ""
    if params.video_source != "local":
        video_terms = generate_terms(task_id, params, video_script)
        if not video_terms:
            sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
            return

    save_script_data(task_id, video_script, video_terms, params)

    if stop_at == "terms":
        sm.state.update_task(
            task_id, state=const.TASK_STATE_COMPLETE, progress=100, terms=video_terms
        )
        return {"script": video_script, "terms": video_terms}

    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=20)

    # 3. Generate audio
    audio_file, audio_duration, sub_maker = generate_audio(
        task_id, params, video_script
    )
    if not audio_file:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        return

    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=30)

    if stop_at == "audio":
        sm.state.update_task(
            task_id,
            state=const.TASK_STATE_COMPLETE,
            progress=100,
            audio_file=audio_file,
        )
        return {"audio_file": audio_file, "audio_duration": audio_duration}

    # 4. Generate subtitle
    subtitle_path = generate_subtitle(
        task_id, params, video_script, sub_maker, audio_file
    )

    if stop_at == "subtitle":
        sm.state.update_task(
            task_id,
            state=const.TASK_STATE_COMPLETE,
            progress=100,
            subtitle_path=subtitle_path,
        )
        return {"subtitle_path": subtitle_path}

    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=40)

    # 5. Get video materials
    downloaded_videos = get_video_materials(
        task_id, params, video_terms, audio_duration
    )
    if not downloaded_videos:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        return

    if stop_at == "materials":
        sm.state.update_task(
            task_id,
            state=const.TASK_STATE_COMPLETE,
            progress=100,
            materials=downloaded_videos,
        )
        return {"materials": downloaded_videos}

    sm.state.update_task(task_id, state=const.TASK_STATE_PROCESSING, progress=50)

    # 6. Generate final videos
    final_video_paths, combined_video_paths = generate_final_videos(
        task_id, params, downloaded_videos, audio_file, subtitle_path
    )

    if not final_video_paths:
        sm.state.update_task(task_id, state=const.TASK_STATE_FAILED)
        return

    logger.success(
        f"task {task_id} finished, generated {len(final_video_paths)} videos."
    )

    kwargs = {
        "videos": final_video_paths,
        "combined_videos": combined_video_paths,
        "script": video_script,
        "terms": video_terms,
        "audio_file": audio_file,
        "audio_duration": audio_duration,
        "subtitle_path": subtitle_path,
        "materials": downloaded_videos,
    }
    sm.state.update_task(
        task_id, state=const.TASK_STATE_COMPLETE, progress=100, **kwargs
    )
    return kwargs


if __name__ == "__main__":
    task_id = "task_id"
    params = VideoParams(
        video_subject="ÈáëÈí±ÁöÑ‰ΩúÁî®",
        voice_name="zh-CN-XiaoyiNeural-Female",
        voice_rate=1.0,
    )
    start(task_id, params, stop_at="video")
</file>

<file path="webui/i18n/de.json">
{
  "Language": "Deutsch",
  "Translation": {
    "Login Required": "Anmeldung erforderlich",
    "Please login to access settings": "Bitte melden Sie sich an, um auf die Einstellungen zuzugreifen",
    "Username": "Benutzername",
    "Password": "Passwort",
    "Login": "Anmelden",
    "Login Error": "Anmeldefehler",
    "Incorrect username or password": "Falscher Benutzername oder Passwort",
    "Please enter your username and password": "Bitte geben Sie Ihren Benutzernamen und Ihr Passwort ein",
    "Video Script Settings": "**Drehbuch / Topic des Videos**",
    "Video Subject": "Worum soll es in dem Video gehen? (Geben Sie ein Keyword an, :red[Dank KI wird automatisch ein Drehbuch generieren])",
    "Script Language": "Welche Sprache soll zum Generieren von Drehb√ºchern  verwendet werden? :red[KI generiert anhand dieses Begriffs das Drehbuch]",
    "Generate Video Script and Keywords": "Klicken Sie hier, um mithilfe von KI ein [Video Drehbuch] und [Video Keywords] basierend auf dem **Keyword** zu generieren.",
    "Auto Detect": "Automatisch erkennen",
    "Video Script": "Drehbuch (Storybook) (:blue[‚ë† Optional, KI generiert  ‚ë° Die richtige Zeichensetzung hilft bei der Erstellung von Untertiteln])",
    "Generate Video Keywords": "Klicken Sie, um KI zum Generieren zu verwenden [Video Keywords] basierend auf dem **Drehbuch**",
    "Please Enter the Video Subject": "Bitte geben Sie zuerst das Drehbuch an",
    "Generating Video Script and Keywords": "KI generiert ein Drehbuch und Schl√ºsselw√∂rter...",
    "Generating Video Keywords": "KI generiert Video-Schl√ºsselw√∂rter...",
    "Video Keywords": "Video Schl√ºsselw√∂rter (:blue[‚ë† Optional, KI generiert ‚ë° Verwende **, (Kommas)** zur Trennung der W√∂rter, in englischer Sprache])",
    "Video Settings": "**Video Einstellungen**",
    "Video Concat Mode": "Videoverkettungsmodus",
    "Random": "Zuf√§llige Verkettung (empfohlen)",
    "Sequential": "Sequentielle Verkettung",
    "Video Transition Mode": "Video √úbergangsmodus",
    "None": "Kein √úbergang",
    "Shuffle": "Zuf√§llige √úberg√§nge",
    "FadeIn": "FadeIn",
    "FadeOut": "FadeOut",
    "SlideIn": "SlideIn",
    "SlideOut": "SlideOut",
    "Video Ratio": "Video-Seitenverh√§ltnis",
    "Portrait": "Portrait 9:16",
    "Landscape": "Landschaft 16:9",
    "Clip Duration": "Maximale Dauer einzelner Videoclips in sekunden",
    "Number of Videos Generated Simultaneously": "Anzahl der parallel generierten Videos",
    "Audio Settings": "**Audio Einstellungen**",
    "Speech Synthesis": "Sprachausgabe",
    "Speech Region": "Region(:red[ErforderlichÔºå[Region abrufen](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Key": "API-Schl√ºssel(:red[ErforderlichÔºå[API-Schl√ºssel abrufen](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Volume": "Lautst√§rke der Sprachausgabe",
    "Speech Rate": "Lesegeschwindigkeit (1,0 bedeutet 1x)",
    "Male": "M√§nnlich",
    "Female": "Weiblich",
    "Background Music": "Hintergrundmusik",
    "No Background Music": "Ohne Hintergrundmusik",
    "Random Background Music": "Zuf√§llig erzeugte Hintergrundmusik",
    "Custom Background Music": "Benutzerdefinierte Hintergrundmusik",
    "Custom Background Music File": "Bitte gib den Pfad zur Musikdatei an:",
    "Background Music Volume": "Lautst√§rke: (0.2 entspricht 20%, sollte nicht zu laut sein)",
    "Subtitle Settings": "**Untertitel-Einstellungen**",
    "Enable Subtitles": "Untertitel aktivieren (Wenn diese Option deaktiviert ist, werden die Einstellungen nicht genutzt)",
    "Font": "Schriftart des Untertitels",
    "Position": "Ausrichtung des Untertitels",
    "Top": "Oben",
    "Center": "Mittig",
    "Bottom": "Unten (empfohlen)",
    "Custom": "Benutzerdefinierte Position (70, was 70% von oben bedeutet)",
    "Font Size": "Schriftgr√∂√üe f√ºr Untertitel",
    "Font Color": "Schriftfarbe",
    "Stroke Color": "Kontur",
    "Stroke Width": "Breite der Untertitelkontur",
    "Generate Video": "Generiere Videos durch KI",
    "Video Script and Subject Cannot Both Be Empty": "Das Video-Thema und Drehbuch d√ºrfen nicht beide leer sein",
    "Generating Video": "Video wird erstellt, bitte warten...",
    "Start Generating Video": "Beginne mit der Generierung",
    "Video Generation Completed": "Video erfolgreich generiert",
    "Video Generation Failed": "Video Generierung fehlgeschlagen",
    "You can download the generated video from the following links": "Sie k√∂nnen das generierte Video √ºber die folgenden Links herunterladen",
    "Basic Settings": "**Grundeinstellungen** (:blue[Klicken zum Erweitern])",
    "Language": "Sprache",
    "Pexels API Key": "Pexels API-Schl√ºssel ([API-Schl√ºssel abrufen](https://www.pexels.com/api/))",
    "Pixabay API Key": "Pixabay API-Schl√ºssel ([API-Schl√ºssel abrufen](https://pixabay.com/api/docs/#api_search_videos))",
    "LLM Provider": "KI-Modellanbieter",
    "API Key": "API-Schl√ºssel (:red[Erforderlich])",
    "Base Url": "Basis-URL",
    "Account ID": "Konto-ID (Aus dem Cloudflare-Dashboard)",
    "Model Name": "Modellname",
    "Please Enter the LLM API Key": "Bitte geben Sie den **KI-Modell API-Schl√ºssel** ein",
    "Please Enter the Pexels API Key": "Bitte geben Sie den **Pexels API-Schl√ºssel** ein",
    "Please Enter the Pixabay API Key": "Bitte geben Sie den **Pixabay API-Schl√ºssel** ein",
    "Get Help": "Wenn Sie Hilfe ben√∂tigen oder Fragen haben, k√∂nnen Sie dem Discord beitreten: https://harryai.cc",
    "Video Source": "Videoquelle",
    "TikTok": "TikTok (TikTok-Unterst√ºtzung kommt bald)",
    "Bilibili": "Bilibili (Bilibili-Unterst√ºtzung kommt bald)",
    "Xiaohongshu": "Xiaohongshu (Xiaohongshu-Unterst√ºtzung kommt bald)",
    "Local file": "Lokale Datei",
    "Play Voice": "Sprachausgabe abspielen",
    "Voice Example": "Dies ist ein Beispieltext zum Testen der Sprachsynthese",
    "Synthesizing Voice": "Sprachsynthese l√§uft, bitte warten...",
    "TTS Provider": "Sprachsynthese-Anbieter ausw√§hlen",
    "TTS Servers": "TTS-Server",
    "No voices available for the selected TTS server. Please select another server.": "Keine Stimmen f√ºr den ausgew√§hlten TTS-Server verf√ºgbar. Bitte w√§hlen Sie einen anderen Server.",
    "SiliconFlow API Key": "SiliconFlow API-Schl√ºssel",
    "SiliconFlow TTS Settings": "SiliconFlow TTS-Einstellungen",
    "Speed: Range [0.25, 4.0], default is 1.0": "Geschwindigkeit: Bereich [0.25, 4.0], Standardwert ist 1.0",
    "Volume: Uses Speech Volume setting, default 1.0 maps to gain 0": "Lautst√§rke: Verwendet die Sprachlautst√§rke-Einstellung, Standardwert 1.0 entspricht Verst√§rkung 0",
    "Hide Log": "Protokoll ausblenden",
    "Hide Basic Settings": "Basis-Einstellungen ausblenden\n\nWenn diese Option deaktiviert ist, wird die Basis-Einstellungen-Leiste nicht auf der Seite angezeigt.\n\nWenn Sie sie erneut anzeigen m√∂chten, setzen Sie `hide_config = false` in `config.toml`",
    "LLM Settings": "**LLM-Einstellungen**",
    "Video Source Settings": "**Videoquellen-Einstellungen**"
  }
}
</file>

<file path="webui/i18n/vi.json">
{
  "Language": "Ti·∫øng Vi·ªát",
  "Translation": {
    "Login Required": "Y√™u c·∫ßu ƒëƒÉng nh·∫≠p",
    "Please login to access settings": "Vui l√≤ng ƒëƒÉng nh·∫≠p ƒë·ªÉ truy c·∫≠p c√†i ƒë·∫∑t",
    "Username": "T√™n ƒëƒÉng nh·∫≠p",
    "Password": "M·∫≠t kh·∫©u",
    "Login": "ƒêƒÉng nh·∫≠p",
    "Login Error": "L·ªói ƒëƒÉng nh·∫≠p",
    "Incorrect username or password": "T√™n ƒëƒÉng nh·∫≠p ho·∫∑c m·∫≠t kh·∫©u kh√¥ng ch√≠nh x√°c",
    "Please enter your username and password": "Vui l√≤ng nh·∫≠p t√™n ƒëƒÉng nh·∫≠p v√† m·∫≠t kh·∫©u c·ªßa b·∫°n",
    "Video Script Settings": "**C√†i ƒê·∫∑t K·ªãch B·∫£n Video**",
    "Video Subject": "Ch·ªß ƒê·ªÅ Video (Cung c·∫•p m·ªôt t·ª´ kh√≥a, :red[AI s·∫Ω t·ª± ƒë·ªông t·∫°o ra] k·ªãch b·∫£n video)",
    "Script Language": "Ng√¥n Ng·ªØ cho Vi·ªác T·∫°o K·ªãch B·∫£n Video (AI s·∫Ω t·ª± ƒë·ªông xu·∫•t ra d·ª±a tr√™n ng√¥n ng·ªØ c·ªßa ch·ªß ƒë·ªÅ c·ªßa b·∫°n)",
    "Generate Video Script and Keywords": "Nh·∫•n ƒë·ªÉ s·ª≠ d·ª•ng AI ƒë·ªÉ t·∫°o [K·ªãch B·∫£n Video] v√† [T·ª´ Kh√≥a Video] d·ª±a tr√™n **ch·ªß ƒë·ªÅ**",
    "Auto Detect": "T·ª± ƒê·ªông Ph√°t Hi·ªán",
    "Video Script": "K·ªãch B·∫£n Video (:blue[‚ë† T√πy ch·ªçn, AI t·∫°o ra  ‚ë° D·∫•u c√¢u ch√≠nh x√°c gi√∫p vi·ªác t·∫°o ph·ª• ƒë·ªÅ)",
    "Generate Video Keywords": "Nh·∫•n ƒë·ªÉ s·ª≠ d·ª•ng AI ƒë·ªÉ t·∫°o [T·ª´ Kh√≥a Video] d·ª±a tr√™n **k·ªãch b·∫£n**",
    "Please Enter the Video Subject": "Vui l√≤ng Nh·∫≠p K·ªãch B·∫£n Video Tr∆∞·ªõc",
    "Generating Video Script and Keywords": "AI ƒëang t·∫°o k·ªãch b·∫£n video v√† t·ª´ kh√≥a...",
    "Generating Video Keywords": "AI ƒëang t·∫°o t·ª´ kh√≥a video...",
    "Video Keywords": "T·ª´ Kh√≥a Video (:blue[‚ë† T√πy ch·ªçn, AI t·∫°o ra ‚ë° S·ª≠ d·ª•ng d·∫•u ph·∫©y **Ti·∫øng Anh** ƒë·ªÉ ph√¢n t√°ch, ch·ªâ s·ª≠ d·ª•ng Ti·∫øng Anh])",
    "Video Settings": "**C√†i ƒê·∫∑t Video**",
    "Video Concat Mode": "Ch·∫ø ƒê·ªô N·ªëi Video",
    "Random": "N·ªëi Ng·∫´u Nhi√™n (ƒê∆∞·ª£c Khuy·∫øn Ngh·ªã)",
    "Sequential": "N·ªëi Theo Th·ª© T·ª±",
    "Video Transition Mode": "Ch·∫ø ƒê·ªô Chuy·ªÉn ƒê·ªïi Video",
    "None": "Kh√¥ng C√≥ Chuy·ªÉn ƒê·ªïi",
    "Shuffle": "Chuy·ªÉn ƒê·ªïi Ng·∫´u Nhi√™n",
    "FadeIn": "FadeIn",
    "FadeOut": "FadeOut",
    "SlideIn": "SlideIn",
    "SlideOut": "SlideOut",
    "Video Ratio": "T·ª∑ L·ªá Khung H√¨nh Video",
    "Portrait": "D·ªçc 9:16",
    "Landscape": "Ngang 16:9",
    "Clip Duration": "Th·ªùi L∆∞·ª£ng T·ªëi ƒêa C·ªßa ƒêo·∫°n Video (gi√¢y)",
    "Number of Videos Generated Simultaneously": "S·ªë Video ƒê∆∞·ª£c T·∫°o Ra ƒê·ªìng Th·ªùi",
    "Audio Settings": "**C√†i ƒê·∫∑t √Çm Thanh**",
    "Speech Synthesis": "Gi·ªçng ƒê·ªçc VƒÉn B·∫£n",
    "Speech Region": "V√πng(:red[B·∫Øt Bu·ªôcÔºå[L·∫•y V√πng](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Key": "Kh√≥a API(:red[B·∫Øt Bu·ªôcÔºå[L·∫•y Kh√≥a API](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Volume": "√Çm L∆∞·ª£ng Gi·ªçng ƒê·ªçc (1.0 ƒë·∫°i di·ªán cho 100%)",
    "Speech Rate": "T·ªëc ƒë·ªô ƒë·ªçc (1.0 bi·ªÉu th·ªã t·ªëc ƒë·ªô g·ªëc)",
    "Male": "Nam",
    "Female": "N·ªØ",
    "Background Music": "√Çm Nh·∫°c N·ªÅn",
    "No Background Music": "Kh√¥ng C√≥ √Çm Nh·∫°c N·ªÅn",
    "Random Background Music": "√Çm Nh·∫°c N·ªÅn Ng·∫´u Nhi√™n",
    "Custom Background Music": "√Çm Nh·∫°c N·ªÅn T√πy Ch·ªânh",
    "Custom Background Music File": "Vui l√≤ng nh·∫≠p ƒë∆∞·ªùng d·∫´n t·ªáp cho √¢m nh·∫°c n·ªÅn t√πy ch·ªânh:",
    "Background Music Volume": "√Çm L∆∞·ª£ng √Çm Nh·∫°c N·ªÅn (0.2 ƒë·∫°i di·ªán cho 20%, √¢m nh·∫°c n·ªÅn kh√¥ng n√™n qu√° to)",
    "Subtitle Settings": "**C√†i ƒê·∫∑t Ph·ª• ƒê·ªÅ**",
    "Enable Subtitles": "B·∫≠t Ph·ª• ƒê·ªÅ (N·∫øu kh√¥ng ch·ªçn, c√°c c√†i ƒë·∫∑t d∆∞·ªõi ƒë√¢y s·∫Ω kh√¥ng c√≥ hi·ªáu l·ª±c)",
    "Font": "Ph√¥ng Ch·ªØ Ph·ª• ƒê·ªÅ",
    "Position": "V·ªã Tr√≠ Ph·ª• ƒê·ªÅ",
    "Top": "Tr√™n",
    "Center": "Gi·ªØa",
    "Bottom": "D∆∞·ªõi (ƒê∆∞·ª£c Khuy·∫øn Ngh·ªã)",
    "Custom": "V·ªã tr√≠ t√πy ch·ªânh (70, ch·ªâ ra l√† c√°ch ƒë·∫ßu trang 70%)",
    "Font Size": "C·ª° Ch·ªØ Ph·ª• ƒê·ªÅ",
    "Font Color": "M√†u Ch·ªØ Ph·ª• ƒê·ªÅ",
    "Stroke Color": "M√†u Vi·ªÅn Ph·ª• ƒê·ªÅ",
    "Stroke Width": "ƒê·ªô R·ªông Vi·ªÅn Ph·ª• ƒê·ªÅ",
    "Generate Video": "T·∫°o Video",
    "Video Script and Subject Cannot Both Be Empty": "Ch·ªß ƒê·ªÅ Video v√† K·ªãch B·∫£n Video kh√¥ng th·ªÉ c√πng tr·ªëng",
    "Generating Video": "ƒêang t·∫°o video, vui l√≤ng ƒë·ª£i...",
    "Start Generating Video": "B·∫Øt ƒê·∫ßu T·∫°o Video",
    "Video Generation Completed": "Ho√†n T·∫•t T·∫°o Video",
    "Video Generation Failed": "T·∫°o Video Th·∫•t B·∫°i",
    "You can download the generated video from the following links": "B·∫°n c√≥ th·ªÉ t·∫£i video ƒë∆∞·ª£c t·∫°o ra t·ª´ c√°c li√™n k·∫øt sau",
    "Basic Settings": "**C√†i ƒê·∫∑t C∆° B·∫£n** (:blue[Nh·∫•p ƒë·ªÉ m·ªü r·ªông])",
    "Language": "Ng√¥n Ng·ªØ",
    "Pexels API Key": "Kh√≥a API Pexels ([L·∫•y Kh√≥a API](https://www.pexels.com/api/))",
    "Pixabay API Key": "Kh√≥a API Pixabay ([L·∫•y Kh√≥a API](https://pixabay.com/api/docs/#api_search_videos))",
    "LLM Provider": "Nh√† Cung C·∫•p LLM",
    "API Key": "Kh√≥a API (:red[B·∫Øt Bu·ªôc])",
    "Base Url": "Url C∆° B·∫£n",
    "Account ID": "ID T√†i Kho·∫£n (L·∫•y t·ª´ b·∫£ng ƒëi·ªÅu khi·ªÉn Cloudflare)",
    "Model Name": "T√™n M√¥ H√¨nh",
    "Please Enter the LLM API Key": "Vui l√≤ng Nh·∫≠p **Kh√≥a API LLM**",
    "Please Enter the Pexels API Key": "Vui l√≤ng Nh·∫≠p **Kh√≥a API Pexels**",
    "Please Enter the Pixabay API Key": "Vui l√≤ng Nh·∫≠p **Kh√≥a API Pixabay**",
    "Get Help": "N·∫øu b·∫°n c·∫ßn gi√∫p ƒë·ª° ho·∫∑c c√≥ b·∫•t k·ª≥ c√¢u h·ªèi n√†o, b·∫°n c√≥ th·ªÉ tham gia discord ƒë·ªÉ ƒë∆∞·ª£c gi√∫p ƒë·ª°: https://harryai.cc",
    "Video Source": "Ngu·ªìn Video",
    "TikTok": "TikTok (H·ªó tr·ª£ TikTok s·∫Øp ra m·∫Øt)",
    "Bilibili": "Bilibili (H·ªó tr·ª£ Bilibili s·∫Øp ra m·∫Øt)",
    "Xiaohongshu": "Xiaohongshu (H·ªó tr·ª£ Xiaohongshu s·∫Øp ra m·∫Øt)",
    "Local file": "T·ªáp c·ª•c b·ªô",
    "Play Voice": "Ph√°t Gi·ªçng N√≥i",
    "Voice Example": "ƒê√¢y l√† vƒÉn b·∫£n m·∫´u ƒë·ªÉ ki·ªÉm tra t·ªïng h·ª£p gi·ªçng n√≥i",
    "Synthesizing Voice": "ƒêang t·ªïng h·ª£p gi·ªçng n√≥i, vui l√≤ng ƒë·ª£i...",
    "TTS Provider": "Ch·ªçn nh√† cung c·∫•p t·ªïng h·ª£p gi·ªçng n√≥i",
    "TTS Servers": "M√°y ch·ªß TTS",
    "No voices available for the selected TTS server. Please select another server.": "Kh√¥ng c√≥ gi·ªçng n√≥i n√†o cho m√°y ch·ªß TTS ƒë√£ ch·ªçn. Vui l√≤ng ch·ªçn m√°y ch·ªß kh√°c.",
    "SiliconFlow API Key": "Kh√≥a API SiliconFlow",
    "SiliconFlow TTS Settings": "C√†i ƒë·∫∑t SiliconFlow TTS",
    "Speed: Range [0.25, 4.0], default is 1.0": "T·ªëc ƒë·ªô: Ph·∫°m vi [0.25, 4.0], m·∫∑c ƒë·ªãnh l√† 1.0",
    "Volume: Uses Speech Volume setting, default 1.0 maps to gain 0": "√Çm l∆∞·ª£ng: S·ª≠ d·ª•ng c√†i ƒë·∫∑t √Çm l∆∞·ª£ng Gi·ªçng n√≥i, m·∫∑c ƒë·ªãnh 1.0 t∆∞∆°ng ·ª©ng v·ªõi tƒÉng √≠ch 0",
    "Hide Log": "·∫®n Nh·∫≠t K√Ω",
    "Hide Basic Settings": "·∫®n C√†i ƒê·∫∑t C∆° B·∫£n\n\n·∫®n, thanh c√†i ƒë·∫∑t c∆° b·∫£n s·∫Ω kh√¥ng hi·ªÉn th·ªã tr√™n trang web.\n\nN·∫øu b·∫°n mu·ªën hi·ªÉn th·ªã l·∫°i, vui l√≤ng ƒë·∫∑t `hide_config = false` trong `config.toml`",
    "LLM Settings": "**C√†i ƒê·∫∑t LLM**",
    "Video Source Settings": "**C√†i ƒê·∫∑t Ngu·ªìn Video**"
  }
}
</file>

<file path="config.example.toml">
[app]
video_source = "pexels" # "pexels" or "pixabay"

# ÊòØÂê¶ÈöêËóèÈÖçÁΩÆÈù¢Êùø
hide_config = false

# Pexels API Key
# Register at https://www.pexels.com/api/ to get your API key.
# You can use multiple keys to avoid rate limits.
# For example: pexels_api_keys = ["123adsf4567adf89","abd1321cd13efgfdfhi"]
# ÁâπÂà´Ê≥®ÊÑèÊ†ºÂºèÔºåKey Áî®Ëã±ÊñáÂèåÂºïÂè∑Êã¨Ëµ∑Êù•ÔºåÂ§ö‰∏™KeyÁî®ÈÄóÂè∑ÈöîÂºÄ
pexels_api_keys = []

# Pixabay API Key
# Register at https://pixabay.com/api/docs/ to get your API key.
# You can use multiple keys to avoid rate limits.
# For example: pixabay_api_keys = ["123adsf4567adf89","abd1321cd13efgfdfhi"]
# ÁâπÂà´Ê≥®ÊÑèÊ†ºÂºèÔºåKey Áî®Ëã±ÊñáÂèåÂºïÂè∑Êã¨Ëµ∑Êù•ÔºåÂ§ö‰∏™KeyÁî®ÈÄóÂè∑ÈöîÂºÄ
pixabay_api_keys = []

# ÊîØÊåÅÁöÑÊèê‰æõÂïÜ (Supported providers):
#   openai
#   moonshot    (Êúà‰πãÊöóÈù¢)
#   azure
#   qwen        (ÈÄö‰πâÂçÉÈóÆ)
#   deepseek
#   gemini
#   ollama
#   g4f
#   oneapi
#   cloudflare
#   ernie       (ÊñáÂøÉ‰∏ÄË®Ä)
llm_provider = "openai"

########## Pollinations AI Settings
# Visit https://pollinations.ai/ to learn more
# API Key is optional - leave empty for public access
pollinations_api_key = ""
# Default base URL for Pollinations API
pollinations_base_url = "https://pollinations.ai/api/v1"
# Default model for text generation
pollinations_model_name = "openai-fast"

########## Ollama Settings
# No need to set it unless you want to use your own proxy
ollama_base_url = ""
# Check your available models at https://ollama.com/library
ollama_model_name = ""

########## OpenAI API Key
# Get your API key at https://platform.openai.com/api-keys
openai_api_key = ""
# No need to set it unless you want to use your own proxy
openai_base_url = ""
# Check your available models at https://platform.openai.com/account/limits
openai_model_name = "gpt-4o-mini"

########## Moonshot API Key
# Visit https://platform.moonshot.cn/console/api-keys to get your API key.
moonshot_api_key = ""
moonshot_base_url = "https://api.moonshot.cn/v1"
moonshot_model_name = "moonshot-v1-8k"

########## OneAPI API Key
# Visit https://github.com/songquanpeng/one-api to get your API key
oneapi_api_key = ""
oneapi_base_url = ""
oneapi_model_name = ""

########## G4F
# Visit https://github.com/xtekky/gpt4free to get more details
# Supported model list: https://github.com/xtekky/gpt4free/blob/main/g4f/models.py
g4f_model_name = "gpt-3.5-turbo"

########## Azure API Key
# Visit https://learn.microsoft.com/zh-cn/azure/ai-services/openai/ to get more details
# API documentation: https://learn.microsoft.com/zh-cn/azure/ai-services/openai/reference
azure_api_key = ""
azure_base_url = ""
azure_model_name = "gpt-35-turbo"        # replace with your model deployment name
azure_api_version = "2024-02-15-preview"

########## Gemini API Key
gemini_api_key = ""
gemini_model_name = "gemini-1.0-pro"

########## Qwen API Key
# Visit https://dashscope.console.aliyun.com/apiKey to get your API key
# Visit below links to get more details
# https://tongyi.aliyun.com/qianwen/
# https://help.aliyun.com/zh/dashscope/developer-reference/model-introduction
qwen_api_key = ""
qwen_model_name = "qwen-max"


########## DeepSeek API Key
# Visit https://platform.deepseek.com/api_keys to get your API key
deepseek_api_key = ""
deepseek_base_url = "https://api.deepseek.com"
deepseek_model_name = "deepseek-chat"

# Subtitle Provider, "edge" or "whisper"
# If empty, the subtitle will not be generated
subtitle_provider = "edge"

#
# ImageMagick
#
# Once you have installed it, ImageMagick will be automatically detected, except on Windows!
# On Windows, for example "C:\Program Files (x86)\ImageMagick-7.1.1-Q16-HDRI\magick.exe"
# Download from https://imagemagick.org/archive/binaries/ImageMagick-7.1.1-29-Q16-x64-static.exe

# imagemagick_path = "C:\\Program Files (x86)\\ImageMagick-7.1.1-Q16\\magick.exe"


#
# FFMPEG
#
# ÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåffmpeg ‰ºöË¢´Ëá™Âä®‰∏ãËΩΩÔºåÂπ∂‰∏î‰ºöË¢´Ëá™Âä®Ê£ÄÊµãÂà∞„ÄÇ
# ‰ΩÜÊòØÂ¶ÇÊûú‰Ω†ÁöÑÁéØÂ¢ÉÊúâÈóÆÈ¢òÔºåÊó†Ê≥ïËá™Âä®‰∏ãËΩΩÔºåÂèØËÉΩ‰ºöÈÅáÂà∞Â¶Ç‰∏ãÈîôËØØÔºö
#   RuntimeError: No ffmpeg exe could be found.
#   Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
# Ê≠§Êó∂‰Ω†ÂèØ‰ª•ÊâãÂä®‰∏ãËΩΩ ffmpeg Âπ∂ËÆæÁΩÆ ffmpeg_pathÔºå‰∏ãËΩΩÂú∞ÂùÄÔºöhttps://www.gyan.dev/ffmpeg/builds/

# Under normal circumstances, ffmpeg is downloaded automatically and detected automatically.
# However, if there is an issue with your environment that prevents automatic downloading, you might encounter the following error:
#   RuntimeError: No ffmpeg exe could be found.
#   Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
# In such cases, you can manually download ffmpeg and set the ffmpeg_path, download link: https://www.gyan.dev/ffmpeg/builds/

# ffmpeg_path = "C:\\Users\\harry\\Downloads\\ffmpeg.exe"
#########################################################################################

# ÂΩìËßÜÈ¢ëÁîüÊàêÊàêÂäüÂêéÔºåAPIÊúçÂä°Êèê‰æõÁöÑËßÜÈ¢ë‰∏ãËΩΩÊé•ÂÖ•ÁÇπÔºåÈªòËÆ§‰∏∫ÂΩìÂâçÊúçÂä°ÁöÑÂú∞ÂùÄÂíåÁõëÂê¨Á´ØÂè£
# ÊØîÂ¶Ç http://127.0.0.1:8080/tasks/6357f542-a4e1-46a1-b4c9-bf3bd0df5285/final-1.mp4
# Â¶ÇÊûú‰Ω†ÈúÄË¶Å‰ΩøÁî®ÂüüÂêçÂØπÂ§ñÊèê‰æõÊúçÂä°Ôºà‰∏ÄËà¨‰ºöÁî®nginxÂÅö‰ª£ÁêÜÔºâÔºåÂàôÂèØ‰ª•ËÆæÁΩÆ‰∏∫‰Ω†ÁöÑÂüüÂêç
# ÊØîÂ¶Ç https://xxxx.com/tasks/6357f542-a4e1-46a1-b4c9-bf3bd0df5285/final-1.mp4
# endpoint="https://xxxx.com"

# When the video is successfully generated, the API service provides a download endpoint for the video, defaulting to the service's current address and listening port.
# For example, http://127.0.0.1:8080/tasks/6357f542-a4e1-46a1-b4c9-bf3bd0df5285/final-1.mp4
# If you need to provide the service externally using a domain name (usually done with nginx as a proxy), you can set it to your domain name.
# For example, https://xxxx.com/tasks/6357f542-a4e1-46a1-b4c9-bf3bd0df5285/final-1.mp4
# endpoint="https://xxxx.com"
endpoint = ""


# Video material storage location
# material_directory = ""                    # Indicates that video materials will be downloaded to the default folder, the default folder is ./storage/cache_videos under the current project
# material_directory = "/user/harry/videos"  # Indicates that video materials will be downloaded to a specified folder
# material_directory = "task"                # Indicates that video materials will be downloaded to the current task's folder, this method does not allow sharing of already downloaded video materials

# ËßÜÈ¢ëÁ¥†ÊùêÂ≠òÊîæ‰ΩçÁΩÆ
# material_directory = ""                    #Ë°®Á§∫Â∞ÜËßÜÈ¢ëÁ¥†Êùê‰∏ãËΩΩÂà∞ÈªòËÆ§ÁöÑÊñá‰ª∂Â§πÔºåÈªòËÆ§Êñá‰ª∂Â§π‰∏∫ÂΩìÂâçÈ°πÁõÆ‰∏ãÁöÑ ./storage/cache_videos
# material_directory = "/user/harry/videos"  #Ë°®Á§∫Â∞ÜËßÜÈ¢ëÁ¥†Êùê‰∏ãËΩΩÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂Â§π‰∏≠
# material_directory = "task"                #Ë°®Á§∫Â∞ÜËßÜÈ¢ëÁ¥†Êùê‰∏ãËΩΩÂà∞ÂΩìÂâç‰ªªÂä°ÁöÑÊñá‰ª∂Â§π‰∏≠ÔºåËøôÁßçÊñπÂºèÊó†Ê≥ïÂÖ±‰∫´Â∑≤Áªè‰∏ãËΩΩÁöÑËßÜÈ¢ëÁ¥†Êùê

material_directory = ""

# Used for state management of the task
enable_redis = false
redis_host = "localhost"
redis_port = 6379
redis_db = 0
redis_password = ""

# ÊñáÁîüËßÜÈ¢ëÊó∂ÁöÑÊúÄÂ§ßÂπ∂Âèë‰ªªÂä°Êï∞
max_concurrent_tasks = 5


[whisper]
# Only effective when subtitle_provider is "whisper"

# Run on GPU with FP16
# model = WhisperModel(model_size, device="cuda", compute_type="float16")

# Run on GPU with INT8
# model = WhisperModel(model_size, device="cuda", compute_type="int8_float16")

# Run on CPU with INT8
# model = WhisperModel(model_size, device="cpu", compute_type="int8")

# recommended model_size: "large-v3"
model_size = "large-v3"
# if you want to use GPU, set device="cuda"
device = "CPU"
compute_type = "int8"


[proxy]
### Use a proxy to access the Pexels API
### Format: "http://<username>:<password>@<proxy>:<port>"
### Example: "http://user:pass@proxy:1234"
### Doc: https://requests.readthedocs.io/en/latest/user/advanced/#proxies

# http = "http://10.10.1.10:3128"
# https = "http://10.10.1.10:1080"

[azure]
# Azure Speech API Key
# Get your API key at https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices
speech_key = ""
speech_region = ""

[siliconflow]
# SiliconFlow API Key
# Get your API key at https://siliconflow.cn
api_key = ""

[ui]
# UI related settings
# ÊòØÂê¶ÈöêËóèÊó•Âøó‰ø°ÊÅØ
# Whether to hide logs in the UI
hide_log = false
</file>

<file path="app/services/llm.py">
import json
import logging
import re
import requests
from typing import List

import g4f
from loguru import logger
from openai import AzureOpenAI, OpenAI
from openai.types.chat import ChatCompletion

from app.config import config

_max_retries = 5


def _generate_response(prompt: str) -> str:
    try:
        content = ""
        llm_provider = config.app.get("llm_provider", "openai")
        logger.info(f"llm provider: {llm_provider}")
        if llm_provider == "g4f":
            model_name = config.app.get("g4f_model_name", "")
            if not model_name:
                model_name = "gpt-3.5-turbo-16k-0613"
            content = g4f.ChatCompletion.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
            )
        else:
            api_version = ""  # for azure
            if llm_provider == "moonshot":
                api_key = config.app.get("moonshot_api_key")
                model_name = config.app.get("moonshot_model_name")
                base_url = "https://api.moonshot.cn/v1"
            elif llm_provider == "ollama":
                # api_key = config.app.get("openai_api_key")
                api_key = "ollama"  # any string works but you are required to have one
                model_name = config.app.get("ollama_model_name")
                base_url = config.app.get("ollama_base_url", "")
                if not base_url:
                    base_url = "http://localhost:11434/v1"
            elif llm_provider == "openai":
                api_key = config.app.get("openai_api_key")
                model_name = config.app.get("openai_model_name")
                base_url = config.app.get("openai_base_url", "")
                if not base_url:
                    base_url = "https://api.openai.com/v1"
            elif llm_provider == "oneapi":
                api_key = config.app.get("oneapi_api_key")
                model_name = config.app.get("oneapi_model_name")
                base_url = config.app.get("oneapi_base_url", "")
            elif llm_provider == "azure":
                api_key = config.app.get("azure_api_key")
                model_name = config.app.get("azure_model_name")
                base_url = config.app.get("azure_base_url", "")
                api_version = config.app.get("azure_api_version", "2024-02-15-preview")
            elif llm_provider == "gemini":
                api_key = config.app.get("gemini_api_key")
                model_name = config.app.get("gemini_model_name")
                base_url = "***"
            elif llm_provider == "qwen":
                api_key = config.app.get("qwen_api_key")
                model_name = config.app.get("qwen_model_name")
                base_url = "***"
            elif llm_provider == "cloudflare":
                api_key = config.app.get("cloudflare_api_key")
                model_name = config.app.get("cloudflare_model_name")
                account_id = config.app.get("cloudflare_account_id")
                base_url = "***"
            elif llm_provider == "deepseek":
                api_key = config.app.get("deepseek_api_key")
                model_name = config.app.get("deepseek_model_name")
                base_url = config.app.get("deepseek_base_url")
                if not base_url:
                    base_url = "https://api.deepseek.com"
            elif llm_provider == "ernie":
                api_key = config.app.get("ernie_api_key")
                secret_key = config.app.get("ernie_secret_key")
                base_url = config.app.get("ernie_base_url")
                model_name = "***"
                if not secret_key:
                    raise ValueError(
                        f"{llm_provider}: secret_key is not set, please set it in the config.toml file."
                    )
            elif llm_provider == "pollinations":
                try:
                    base_url = config.app.get("pollinations_base_url", "")
                    if not base_url:
                        base_url = "https://text.pollinations.ai/openai"
                    model_name = config.app.get(
                        "pollinations_model_name", "openai-fast"
                    )

                    # Prepare the payload
                    payload = {
                        "model": model_name,
                        "messages": [{"role": "user", "content": prompt}],
                        "seed": 101,  # Optional but helps with reproducibility
                    }

                    # Optional parameters if configured
                    if config.app.get("pollinations_private"):
                        payload["private"] = True
                    if config.app.get("pollinations_referrer"):
                        payload["referrer"] = config.app.get("pollinations_referrer")

                    headers = {"Content-Type": "application/json"}

                    # Make the API request
                    response = requests.post(base_url, headers=headers, json=payload)
                    response.raise_for_status()
                    result = response.json()

                    if result and "choices" in result and len(result["choices"]) > 0:
                        content = result["choices"][0]["message"]["content"]
                        return content.replace("\n", "")
                    else:
                        raise Exception(
                            f"[{llm_provider}] returned an invalid response format"
                        )

                except requests.exceptions.RequestException as e:
                    raise Exception(f"[{llm_provider}] request failed: {str(e)}")
                except Exception as e:
                    raise Exception(f"[{llm_provider}] error: {str(e)}")

            if llm_provider not in [
                "pollinations",
                "ollama",
            ]:  # Skip validation for providers that don't require API key
                if not api_key:
                    raise ValueError(
                        f"{llm_provider}: api_key is not set, please set it in the config.toml file."
                    )
                if not model_name:
                    raise ValueError(
                        f"{llm_provider}: model_name is not set, please set it in the config.toml file."
                    )
                if not base_url:
                    raise ValueError(
                        f"{llm_provider}: base_url is not set, please set it in the config.toml file."
                    )

            if llm_provider == "qwen":
                import dashscope
                from dashscope.api_entities.dashscope_response import GenerationResponse

                dashscope.api_key = api_key
                response = dashscope.Generation.call(
                    model=model_name, messages=[{"role": "user", "content": prompt}]
                )
                if response:
                    if isinstance(response, GenerationResponse):
                        status_code = response.status_code
                        if status_code != 200:
                            raise Exception(
                                f'[{llm_provider}] returned an error response: "{response}"'
                            )

                        content = response["output"]["text"]
                        return content.replace("\n", "")
                    else:
                        raise Exception(
                            f'[{llm_provider}] returned an invalid response: "{response}"'
                        )
                else:
                    raise Exception(f"[{llm_provider}] returned an empty response")

            if llm_provider == "gemini":
                import google.generativeai as genai

                genai.configure(api_key=api_key, transport="rest")

                generation_config = {
                    "temperature": 0.5,
                    "top_p": 1,
                    "top_k": 1,
                    "max_output_tokens": 2048,
                }

                safety_settings = [
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "threshold": "BLOCK_ONLY_HIGH",
                    },
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH",
                        "threshold": "BLOCK_ONLY_HIGH",
                    },
                    {
                        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        "threshold": "BLOCK_ONLY_HIGH",
                    },
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "threshold": "BLOCK_ONLY_HIGH",
                    },
                ]

                model = genai.GenerativeModel(
                    model_name=model_name,
                    generation_config=generation_config,
                    safety_settings=safety_settings,
                )

                try:
                    response = model.generate_content(prompt)
                    candidates = response.candidates
                    generated_text = candidates[0].content.parts[0].text
                except (AttributeError, IndexError) as e:
                    print("Gemini Error:", e)

                return generated_text

            if llm_provider == "cloudflare":
                response = requests.post(
                    f"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{model_name}",
                    headers={"Authorization": f"Bearer {api_key}"},
                    json={
                        "messages": [
                            {
                                "role": "system",
                                "content": "You are a friendly assistant",
                            },
                            {"role": "user", "content": prompt},
                        ]
                    },
                )
                result = response.json()
                logger.info(result)
                return result["result"]["response"]

            if llm_provider == "ernie":
                response = requests.post(
                    "https://aip.baidubce.com/oauth/2.0/token",
                    params={
                        "grant_type": "client_credentials",
                        "client_id": api_key,
                        "client_secret": secret_key,
                    },
                )
                access_token = response.json().get("access_token")
                url = f"{base_url}?access_token={access_token}"

                payload = json.dumps(
                    {
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.5,
                        "top_p": 0.8,
                        "penalty_score": 1,
                        "disable_search": False,
                        "enable_citation": False,
                        "response_format": "text",
                    }
                )
                headers = {"Content-Type": "application/json"}

                response = requests.request(
                    "POST", url, headers=headers, data=payload
                ).json()
                return response.get("result")

            if llm_provider == "azure":
                client = AzureOpenAI(
                    api_key=api_key,
                    api_version=api_version,
                    azure_endpoint=base_url,
                )
            else:
                client = OpenAI(
                    api_key=api_key,
                    base_url=base_url,
                )

            response = client.chat.completions.create(
                model=model_name, messages=[{"role": "user", "content": prompt}]
            )
            if response:
                if isinstance(response, ChatCompletion):
                    content = response.choices[0].message.content
                else:
                    raise Exception(
                        f'[{llm_provider}] returned an invalid response: "{response}", please check your network '
                        f"connection and try again."
                    )
            else:
                raise Exception(
                    f"[{llm_provider}] returned an empty response, please check your network connection and try again."
                )

        return content.replace("\n", "")
    except Exception as e:
        return f"Error: {str(e)}"


def generate_script(
    video_subject: str, language: str = "", paragraph_number: int = 1
) -> str:
    prompt = f"""
# Role: Video Script Generator

## Goals:
Generate a script for a video, depending on the subject of the video.

## Constrains:
1. the script is to be returned as a string with the specified number of paragraphs.
2. do not under any circumstance reference this prompt in your response.
3. get straight to the point, don't start with unnecessary things like, "welcome to this video".
4. you must not include any type of markdown or formatting in the script, never use a title.
5. only return the raw content of the script.
6. do not include "voiceover", "narrator" or similar indicators of what should be spoken at the beginning of each paragraph or line.
7. you must not mention the prompt, or anything about the script itself. also, never talk about the amount of paragraphs or lines. just write the script.
8. respond in the same language as the video subject.

# Initialization:
- video subject: {video_subject}
- number of paragraphs: {paragraph_number}
""".strip()
    if language:
        prompt += f"\n- language: {language}"

    final_script = ""
    logger.info(f"subject: {video_subject}")

    def format_response(response):
        # Clean the script
        # Remove asterisks, hashes
        response = response.replace("*", "")
        response = response.replace("#", "")

        # Remove markdown syntax
        response = re.sub(r"\[.*\]", "", response)
        response = re.sub(r"\(.*\)", "", response)

        # Split the script into paragraphs
        paragraphs = response.split("\n\n")

        # Select the specified number of paragraphs
        # selected_paragraphs = paragraphs[:paragraph_number]

        # Join the selected paragraphs into a single string
        return "\n\n".join(paragraphs)

    for i in range(_max_retries):
        try:
            response = _generate_response(prompt=prompt)
            if response:
                final_script = format_response(response)
            else:
                logging.error("gpt returned an empty response")

            # g4f may return an error message
            if final_script and "ÂΩìÊó•È¢ùÂ∫¶Â∑≤Ê∂àËÄóÂÆå" in final_script:
                raise ValueError(final_script)

            if final_script:
                break
        except Exception as e:
            logger.error(f"failed to generate script: {e}")

        if i < _max_retries:
            logger.warning(f"failed to generate video script, trying again... {i + 1}")
    if "Error: " in final_script:
        logger.error(f"failed to generate video script: {final_script}")
    else:
        logger.success(f"completed: \n{final_script}")
    return final_script.strip()


def generate_terms(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    prompt = f"""
# Role: Video Search Terms Generator

## Goals:
Generate {amount} search terms for stock videos, depending on the subject of a video.

## Constrains:
1. the search terms are to be returned as a json-array of strings.
2. each search term should consist of 1-3 words, always add the main subject of the video.
3. you must only return the json-array of strings. you must not return anything else. you must not return the script.
4. the search terms must be related to the subject of the video.
5. reply with english search terms only.

## Output Example:
["search term 1", "search term 2", "search term 3","search term 4","search term 5"]

## Context:
### Video Subject
{video_subject}

### Video Script
{video_script}

Please note that you must use English for generating video search terms; Chinese is not accepted.
""".strip()

    logger.info(f"subject: {video_subject}")

    search_terms = []
    response = ""
    for i in range(_max_retries):
        try:
            response = _generate_response(prompt)
            if "Error: " in response:
                logger.error(f"failed to generate video script: {response}")
                return response
            search_terms = json.loads(response)
            if not isinstance(search_terms, list) or not all(
                isinstance(term, str) for term in search_terms
            ):
                logger.error("response is not a list of strings.")
                continue

        except Exception as e:
            logger.warning(f"failed to generate video terms: {str(e)}")
            if response:
                match = re.search(r"\[.*]", response)
                if match:
                    try:
                        search_terms = json.loads(match.group())
                    except Exception as e:
                        logger.warning(f"failed to generate video terms: {str(e)}")
                        pass

        if search_terms and len(search_terms) > 0:
            break
        if i < _max_retries:
            logger.warning(f"failed to generate video terms, trying again... {i + 1}")

    logger.success(f"completed: \n{search_terms}")
    return search_terms


if __name__ == "__main__":
    video_subject = "ÁîüÂëΩÁöÑÊÑè‰πâÊòØ‰ªÄ‰πà"
    script = generate_script(
        video_subject=video_subject, language="zh-CN", paragraph_number=1
    )
    print("######################")
    print(script)
    search_terms = generate_terms(
        video_subject=video_subject, video_script=script, amount=5
    )
    print("######################")
    print(search_terms)
</file>

<file path="webui/i18n/en.json">
{
  "Language": "English",
  "Translation": {
    "Login Required": "Login Required",
    "Please login to access settings": "Please login to access settings",
    "Username": "Username",
    "Password": "Password",
    "Login": "Login",
    "Login Error": "Login Error",
    "Incorrect username or password": "Incorrect username or password",
    "Please enter your username and password": "Please enter your username and password",
    "Video Script Settings": "**Video Script Settings**",
    "Video Subject": "Video Subject (Provide a keyword, :red[AI will automatically generate] video script)",
    "Script Language": "Language for Generating Video Script (AI will automatically output based on the language of your subject)",
    "Generate Video Script and Keywords": "Click to use AI to generate [Video Script] and [Video Keywords] based on **subject**",
    "Auto Detect": "Auto Detect",
    "Video Script": "Video Script (:blue[‚ë† Optional, AI generated  ‚ë° Proper punctuation helps with subtitle generation])",
    "Generate Video Keywords": "Click to use AI to generate [Video Keywords] based on **script**",
    "Please Enter the Video Subject": "Please Enter the Video Script First",
    "Generating Video Script and Keywords": "AI is generating video script and keywords...",
    "Generating Video Keywords": "AI is generating video keywords...",
    "Video Keywords": "Video Keywords (:blue[‚ë† Optional, AI generated ‚ë° Use **English commas** for separation, English only])",
    "Video Settings": "**Video Settings**",
    "Video Concat Mode": "Video Concatenation Mode",
    "Random": "Random Concatenation (Recommended)",
    "Sequential": "Sequential Concatenation",
    "Video Transition Mode": "Video Transition Mode",
    "None": "None",
    "Shuffle": "Shuffle",
    "FadeIn": "FadeIn",
    "FadeOut": "FadeOut",
    "SlideIn": "SlideIn",
    "SlideOut": "SlideOut",
    "Video Ratio": "Video Aspect Ratio",
    "Portrait": "Portrait 9:16",
    "Landscape": "Landscape 16:9",
    "Clip Duration": "Maximum Duration of Video Clips (seconds)",
    "Number of Videos Generated Simultaneously": "Number of Videos Generated Simultaneously",
    "Audio Settings": "**Audio Settings**",
    "Speech Synthesis": "Speech Synthesis Voice",
    "Speech Region": "Region(:red[RequiredÔºå[Get Region](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Key": "API Key(:red[RequiredÔºå[Get API Key](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Volume": "Speech Volume (1.0 represents 100%)",
    "Speech Rate": "Speech Rate (1.0 means 1x speed)",
    "Male": "Male",
    "Female": "Female",
    "Background Music": "Background Music",
    "No Background Music": "No Background Music",
    "Random Background Music": "Random Background Music",
    "Custom Background Music": "Custom Background Music",
    "Custom Background Music File": "Please enter the file path for custom background music:",
    "Background Music Volume": "Background Music Volume (0.2 represents 20%, background music should not be too loud)",
    "Subtitle Settings": "**Subtitle Settings**",
    "Enable Subtitles": "Enable Subtitles (If unchecked, the settings below will not take effect)",
    "Font": "Subtitle Font",
    "Position": "Subtitle Position",
    "Top": "Top",
    "Center": "Center",
    "Bottom": "Bottom (Recommended)",
    "Custom": "Custom position (70, indicating 70% down from the top)",
    "Font Size": "Subtitle Font Size",
    "Font Color": "Subtitle Font Color",
    "Stroke Color": "Subtitle Outline Color",
    "Stroke Width": "Subtitle Outline Width",
    "Generate Video": "Generate Video",
    "Video Script and Subject Cannot Both Be Empty": "Video Subject and Video Script cannot both be empty",
    "Generating Video": "Generating video, please wait...",
    "Start Generating Video": "Start Generating Video",
    "Video Generation Completed": "Video Generation Completed",
    "Video Generation Failed": "Video Generation Failed",
    "You can download the generated video from the following links": "You can download the generated video from the following links",
    "Pexels API Key": "Pexels API Key ([Get API Key](https://www.pexels.com/api/))",
    "Pixabay API Key": "Pixabay API Key ([Get API Key](https://pixabay.com/api/docs/#api_search_videos))",
    "Basic Settings": "**Basic Settings** (:blue[Click to expand])",
    "Language": "Language",
    "LLM Provider": "LLM Provider",
    "API Key": "API Key (:red[Required])",
    "Base Url": "Base Url",
    "Account ID": "Account ID (Get from Cloudflare dashboard)",
    "Model Name": "Model Name",
    "Please Enter the LLM API Key": "Please Enter the **LLM API Key**",
    "Please Enter the Pexels API Key": "Please Enter the **Pexels API Key**",
    "Please Enter the Pixabay API Key": "Please Enter the **Pixabay API Key**",
    "Get Help": "If you need help, or have any questions, you can join discord for help: https://harryai.cc",
    "Video Source": "Video Source",
    "TikTok": "TikTok (TikTok support is coming soon)",
    "Bilibili": "Bilibili (Bilibili support is coming soon)",
    "Xiaohongshu": "Xiaohongshu (Xiaohongshu support is coming soon)",
    "Local file": "Local file",
    "Play Voice": "Play Voice",
    "Voice Example": "This is an example text for testing speech synthesis",
    "Synthesizing Voice": "Synthesizing voice, please wait...",
    "TTS Provider": "Select the voice synthesis provider",
    "TTS Servers": "TTS Servers",
    "No voices available for the selected TTS server. Please select another server.": "No voices available for the selected TTS server. Please select another server.",
    "SiliconFlow API Key": "SiliconFlow API Key [Click to get](https://cloud.siliconflow.cn/account/ak)",
    "SiliconFlow TTS Settings": "SiliconFlow TTS Settings",
    "Speed: Range [0.25, 4.0], default is 1.0": "Speed: Range [0.25, 4.0], default is 1.0",
    "Volume: Uses Speech Volume setting, default 1.0 maps to gain 0": "Volume: Uses Speech Volume setting, default 1.0 maps to gain 0",
    "Hide Log": "Hide Log",
    "Hide Basic Settings": "Hide Basic Settings\n\nHidden, the basic settings panel will not be displayed on the page.\n\nIf you need to display it again, please set `hide_config = false` in `config.toml`",
    "LLM Settings": "**LLM Settings**",
    "Video Source Settings": "**Video Source Settings**"
  }
}
</file>

<file path="webui/i18n/pt.json">
{
  "Language": "Portugu√™s Brasileiro",
  "Translation": {
    "Login Required": "Login Necess√°rio",
    "Please login to access settings": "Por favor, fa√ßa login para acessar as configura√ß√µes",
    "Username": "Nome de usu√°rio",
    "Password": "Senha",
    "Login": "Entrar",
    "Login Error": "Erro de Login",
    "Incorrect username or password": "Nome de usu√°rio ou senha incorretos",
    "Please enter your username and password": "Por favor, digite seu nome de usu√°rio e senha",
    "Video Script Settings": "**Configura√ß√µes do Roteiro do V√≠deo**",
    "Video Subject": "Tema do V√≠deo (Forne√ßa uma palavra-chave, :red[a IA ir√° gerar automaticamente] o roteiro do v√≠deo)",
    "Script Language": "Idioma para Gerar o Roteiro do V√≠deo (a IA ir√° gerar automaticamente com base no idioma do seu tema)",
    "Generate Video Script and Keywords": "Clique para usar a IA para gerar o [Roteiro do V√≠deo] e as [Palavras-chave do V√≠deo] com base no **tema**",
    "Auto Detect": "Detectar Automaticamente",
    "Video Script": "Roteiro do V√≠deo (:blue[‚ë† Opcional, gerado pela IA  ‚ë° Pontua√ß√£o adequada ajuda na gera√ß√£o de legendas])",
    "Generate Video Keywords": "Clique para usar a IA para gerar [Palavras-chave do V√≠deo] com base no **roteiro**",
    "Please Enter the Video Subject": "Por favor, insira o Roteiro do V√≠deo primeiro",
    "Generating Video Script and Keywords": "A IA est√° gerando o roteiro do v√≠deo e as palavras-chave...",
    "Generating Video Keywords": "A IA est√° gerando as palavras-chave do v√≠deo...",
    "Video Keywords": "Palavras-chave do V√≠deo (:blue[‚ë† Opcional, gerado pela IA ‚ë° Use **v√≠rgulas em ingl√™s** para separar, somente em ingl√™s])",
    "Video Settings": "**Configura√ß√µes do V√≠deo**",
    "Video Concat Mode": "Modo de Concatena√ß√£o de V√≠deo",
    "Random": "Concatena√ß√£o Aleat√≥ria (Recomendado)",
    "Sequential": "Concatena√ß√£o Sequencial",
    "Video Transition Mode": "Modo de Transi√ß√£o de V√≠deo",
    "None": "Nenhuma Transi√ß√£o",
    "Shuffle": "Transi√ß√£o Aleat√≥ria",
    "FadeIn": "FadeIn",
    "FadeOut": "FadeOut",
    "SlideIn": "SlideIn",
    "SlideOut": "SlideOut",
    "Video Ratio": "Propor√ß√£o do V√≠deo",
    "Portrait": "Retrato 9:16",
    "Landscape": "Paisagem 16:9",
    "Clip Duration": "Dura√ß√£o M√°xima dos Clipes de V√≠deo (segundos)",
    "Number of Videos Generated Simultaneously": "N√∫mero de V√≠deos Gerados Simultaneamente",
    "Audio Settings": "**Configura√ß√µes de √Åudio**",
    "Speech Synthesis": "Voz de S√≠ntese de Fala",
    "Speech Region": "Regi√£o(:red[Obrigat√≥rioÔºå[Obter Regi√£o](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Key": "Chave da API(:red[Obrigat√≥rioÔºå[Obter Chave da API](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Volume": "Volume da Fala (1.0 representa 100%)",
    "Speech Rate": "Velocidade da Fala (1.0 significa velocidade 1x)",
    "Male": "Masculino",
    "Female": "Feminino",
    "Background Music": "M√∫sica de Fundo",
    "No Background Music": "Sem M√∫sica de Fundo",
    "Random Background Music": "M√∫sica de Fundo Aleat√≥ria",
    "Custom Background Music": "M√∫sica de Fundo Personalizada",
    "Custom Background Music File": "Por favor, insira o caminho do arquivo para a m√∫sica de fundo personalizada:",
    "Background Music Volume": "Volume da M√∫sica de Fundo (0.2 representa 20%, a m√∫sica de fundo n√£o deve ser muito alta)",
    "Subtitle Settings": "**Configura√ß√µes de Legendas**",
    "Enable Subtitles": "Ativar Legendas (Se desmarcado, as configura√ß√µes abaixo n√£o ter√£o efeito)",
    "Font": "Fonte da Legenda",
    "Position": "Posi√ß√£o da Legenda",
    "Top": "Superior",
    "Center": "Centralizar",
    "Bottom": "Inferior (Recomendado)",
    "Custom": "Posi√ß√£o personalizada (70, indicando 70% abaixo do topo)",
    "Font Size": "Tamanho da Fonte da Legenda",
    "Font Color": "Cor da Fonte da Legenda",
    "Stroke Color": "Cor do Contorno da Legenda",
    "Stroke Width": "Largura do Contorno da Legenda",
    "Generate Video": "Gerar V√≠deo",
    "Video Script and Subject Cannot Both Be Empty": "O Tema do V√≠deo e o Roteiro do V√≠deo n√£o podem estar ambos vazios",
    "Generating Video": "Gerando v√≠deo, por favor aguarde...",
    "Start Generating Video": "Come√ßar a Gerar V√≠deo",
    "Video Generation Completed": "Gera√ß√£o do V√≠deo Conclu√≠da",
    "Video Generation Failed": "Falha na Gera√ß√£o do V√≠deo",
    "You can download the generated video from the following links": "Voc√™ pode baixar o v√≠deo gerado a partir dos seguintes links",
    "Basic Settings": "**Configura√ß√µes B√°sicas** (:blue[Clique para expandir])",
    "Language": "Idioma",
    "Pexels API Key": "Chave da API do Pexels ([Obter Chave da API](https://www.pexels.com/api/))",
    "Pixabay API Key": "Chave da API do Pixabay ([Obter Chave da API](https://pixabay.com/api/docs/#api_search_videos))",
    "LLM Provider": "Provedor LLM",
    "API Key": "Chave da API (:red[Obrigat√≥rio])",
    "Base Url": "URL Base",
    "Account ID": "ID da Conta (Obter no painel do Cloudflare)",
    "Model Name": "Nome do Modelo",
    "Please Enter the LLM API Key": "Por favor, insira a **Chave da API LLM**",
    "Please Enter the Pexels API Key": "Por favor, insira a **Chave da API do Pexels**",
    "Please Enter the Pixabay API Key": "Por favor, insira a **Chave da API do Pixabay**",
    "Get Help": "Se precisar de ajuda ou tiver alguma d√∫vida, voc√™ pode entrar no discord para obter ajuda: https://harryai.cc",
    "Video Source": "Fonte do V√≠deo",
    "TikTok": "TikTok (Suporte para TikTok em breve)",
    "Bilibili": "Bilibili (Suporte para Bilibili em breve)",
    "Xiaohongshu": "Xiaohongshu (Suporte para Xiaohongshu em breve)",
    "Local file": "Arquivo local",
    "Play Voice": "Reproduzir Voz",
    "Voice Example": "Este √© um exemplo de texto para testar a s√≠ntese de fala",
    "Synthesizing Voice": "Sintetizando voz, por favor aguarde...",
    "TTS Provider": "Selecione o provedor de s√≠ntese de voz",
    "TTS Servers": "Servidores TTS",
    "No voices available for the selected TTS server. Please select another server.": "N√£o h√° vozes dispon√≠veis para o servidor TTS selecionado. Por favor, selecione outro servidor.",
    "SiliconFlow API Key": "Chave API do SiliconFlow",
    "SiliconFlow TTS Settings": "Configura√ß√µes do SiliconFlow TTS",
    "Speed: Range [0.25, 4.0], default is 1.0": "Velocidade: Intervalo [0.25, 4.0], o padr√£o √© 1.0",
    "Volume: Uses Speech Volume setting, default 1.0 maps to gain 0": "Volume: Usa a configura√ß√£o de Volume de Fala, o padr√£o 1.0 corresponde ao ganho 0",
    "Hide Log": "Ocultar Log",
    "Hide Basic Settings": "Ocultar Configura√ß√µes B√°sicas\n\nOculto, o painel de configura√ß√µes b√°sicas n√£o ser√° exibido na p√°gina.\n\nSe precisar exibi-lo novamente, defina `hide_config = false` em `config.toml`",
    "LLM Settings": "**Configura√ß√µes do LLM**",
    "Video Source Settings": "**Configura√ß√µes da Fonte do V√≠deo**"
  }
}
</file>

<file path="webui/i18n/zh.json">
{
  "Language": "ÁÆÄ‰Ωì‰∏≠Êñá",
  "Translation": {
    "Login Required": "ÈúÄË¶ÅÁôªÂΩï",
    "Please login to access settings": "ËØ∑ÁôªÂΩïÂêéËÆøÈóÆÈÖçÁΩÆËÆæÁΩÆ (:gray[ÈªòËÆ§Áî®Êà∑Âêç: admin, ÂØÜÁ†Å: admin, ÊÇ®ÂèØ‰ª•Âú® config.toml ‰∏≠‰øÆÊîπ])",
    "Username": "Áî®Êà∑Âêç",
    "Password": "ÂØÜÁ†Å",
    "Login": "ÁôªÂΩï",
    "Login Error": "ÁôªÂΩïÈîôËØØ",
    "Incorrect username or password": "Áî®Êà∑ÂêçÊàñÂØÜÁ†Å‰∏çÊ≠£Á°Æ",
    "Please enter your username and password": "ËØ∑ËæìÂÖ•Áî®Êà∑ÂêçÂíåÂØÜÁ†Å",
    "Video Script Settings": "**ÊñáÊ°àËÆæÁΩÆ**",
    "Video Subject": "ËßÜÈ¢ë‰∏ªÈ¢òÔºàÁªôÂÆö‰∏Ä‰∏™ÂÖ≥ÈîÆËØçÔºå:red[AIËá™Âä®ÁîüÊàê]ËßÜÈ¢ëÊñáÊ°àÔºâ",
    "Script Language": "ÁîüÊàêËßÜÈ¢ëËÑöÊú¨ÁöÑËØ≠Ë®ÄÔºà‰∏ÄËà¨ÊÉÖÂÜµAI‰ºöËá™Âä®Ê†πÊçÆ‰Ω†ËæìÂÖ•ÁöÑ‰∏ªÈ¢òËØ≠Ë®ÄËæìÂá∫Ôºâ",
    "Generate Video Script and Keywords": "ÁÇπÂáª‰ΩøÁî®AIÊ†πÊçÆ**‰∏ªÈ¢ò**ÁîüÊàê „ÄêËßÜÈ¢ëÊñáÊ°à„Äë Âíå „ÄêËßÜÈ¢ëÂÖ≥ÈîÆËØç„Äë",
    "Auto Detect": "Ëá™Âä®Ê£ÄÊµã",
    "Video Script": "ËßÜÈ¢ëÊñáÊ°àÔºà:blue[‚ë†ÂèØ‰∏çÂ°´Ôºå‰ΩøÁî®AIÁîüÊàê  ‚ë°ÂêàÁêÜ‰ΩøÁî®Ê†áÁÇπÊñ≠Âè•ÔºåÊúâÂä©‰∫éÁîüÊàêÂ≠óÂπï]Ôºâ",
    "Generate Video Keywords": "ÁÇπÂáª‰ΩøÁî®AIÊ†πÊçÆ**ÊñáÊ°à**ÁîüÊàê„ÄêËßÜÈ¢ëÂÖ≥ÈîÆËØç„Äë",
    "Please Enter the Video Subject": "ËØ∑ÂÖàÂ°´ÂÜôËßÜÈ¢ëÊñáÊ°à",
    "Generating Video Script and Keywords": "AIÊ≠£Âú®ÁîüÊàêËßÜÈ¢ëÊñáÊ°àÂíåÂÖ≥ÈîÆËØç...",
    "Generating Video Keywords": "AIÊ≠£Âú®ÁîüÊàêËßÜÈ¢ëÂÖ≥ÈîÆËØç...",
    "Video Keywords": "ËßÜÈ¢ëÂÖ≥ÈîÆËØçÔºà:blue[‚ë†ÂèØ‰∏çÂ°´Ôºå‰ΩøÁî®AIÁîüÊàê ‚ë°Áî®**Ëã±ÊñáÈÄóÂè∑**ÂàÜÈöîÔºåÂè™ÊîØÊåÅËã±Êñá]Ôºâ",
    "Video Settings": "**ËßÜÈ¢ëËÆæÁΩÆ**",
    "Video Concat Mode": "ËßÜÈ¢ëÊãºÊé•Ê®°Âºè",
    "Random": "ÈöèÊú∫ÊãºÊé•ÔºàÊé®ËçêÔºâ",
    "Sequential": "È°∫Â∫èÊãºÊé•",
    "Video Transition Mode": "ËßÜÈ¢ëËΩ¨Âú∫Ê®°Âºè",
    "None": "Êó†ËΩ¨Âú∫",
    "Shuffle": "ÈöèÊú∫ËΩ¨Âú∫",
    "FadeIn": "Ê∏êÂÖ•",
    "FadeOut": "Ê∏êÂá∫",
    "SlideIn": "ÊªëÂä®ÂÖ•",
    "SlideOut": "ÊªëÂä®Âá∫",
    "Video Ratio": "ËßÜÈ¢ëÊØî‰æã",
    "Portrait": "Á´ñÂ±è 9:16ÔºàÊäñÈü≥ËßÜÈ¢ëÔºâ",
    "Landscape": "Ê®™Â±è 16:9ÔºàË•øÁìúËßÜÈ¢ëÔºâ",
    "Clip Duration": "ËßÜÈ¢ëÁâáÊÆµÊúÄÂ§ßÊó∂Èïø(Áßí)Ôºà**‰∏çÊòØËßÜÈ¢ëÊÄªÈïøÂ∫¶**ÔºåÊòØÊåáÊØè‰∏™**ÂêàÊàêÁâáÊÆµ**ÁöÑÈïøÂ∫¶Ôºâ",
    "Number of Videos Generated Simultaneously": "ÂêåÊó∂ÁîüÊàêËßÜÈ¢ëÊï∞Èáè",
    "Audio Settings": "**Èü≥È¢ëËÆæÁΩÆ**",
    "Speech Synthesis": "ÊúóËØªÂ£∞Èü≥Ôºà:red[**‰∏éÊñáÊ°àËØ≠Ë®Ä‰øùÊåÅ‰∏ÄËá¥**„ÄÇÊ≥®ÊÑèÔºöV2ÁâàÊïàÊûúÊõ¥Â•ΩÔºå‰ΩÜÊòØÈúÄË¶ÅAPI KEY]Ôºâ",
    "Speech Region": "ÊúçÂä°Âå∫Âüü (:red[ÂøÖÂ°´Ôºå[ÁÇπÂáªËé∑Âèñ](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Key": "API Key (:red[ÂøÖÂ°´ÔºåÂØÜÈí•1 Êàñ ÂØÜÈí•2 ÂùáÂèØ [ÁÇπÂáªËé∑Âèñ](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices)])",
    "Speech Volume": "ÊúóËØªÈü≥ÈáèÔºà1.0Ë°®Á§∫100%Ôºâ",
    "Speech Rate": "ÊúóËØªÈÄüÂ∫¶Ôºà1.0Ë°®Á§∫1ÂÄçÈÄüÔºâ",
    "Male": "Áî∑ÊÄß",
    "Female": "Â•≥ÊÄß",
    "Background Music": "ËÉåÊôØÈü≥‰πê",
    "No Background Music": "Êó†ËÉåÊôØÈü≥‰πê",
    "Random Background Music": "ÈöèÊú∫ËÉåÊôØÈü≥‰πê",
    "Custom Background Music": "Ëá™ÂÆö‰πâËÉåÊôØÈü≥‰πê",
    "Custom Background Music File": "ËØ∑ËæìÂÖ•Ëá™ÂÆö‰πâËÉåÊôØÈü≥‰πêÁöÑÊñá‰ª∂Ë∑ØÂæÑ",
    "Background Music Volume": "ËÉåÊôØÈü≥‰πêÈü≥ÈáèÔºà0.2Ë°®Á§∫20%ÔºåËÉåÊôØÂ£∞Èü≥‰∏çÂÆúËøáÈ´òÔºâ",
    "Subtitle Settings": "**Â≠óÂπïËÆæÁΩÆ**",
    "Enable Subtitles": "ÂêØÁî®Â≠óÂπïÔºàËã•ÂèñÊ∂àÂãæÈÄâÔºå‰∏ãÈù¢ÁöÑËÆæÁΩÆÈÉΩÂ∞Ü‰∏çÁîüÊïàÔºâ",
    "Font": "Â≠óÂπïÂ≠ó‰Ωì",
    "Position": "Â≠óÂπï‰ΩçÁΩÆ",
    "Top": "È°∂ÈÉ®",
    "Center": "‰∏≠Èó¥",
    "Bottom": "Â∫ïÈÉ®ÔºàÊé®ËçêÔºâ",
    "Custom": "Ëá™ÂÆö‰πâ‰ΩçÁΩÆÔºà70ÔºåË°®Á§∫Á¶ªÈ°∂ÈÉ®70%ÁöÑ‰ΩçÁΩÆÔºâ",
    "Font Size": "Â≠óÂπïÂ§ßÂ∞è",
    "Font Color": "Â≠óÂπïÈ¢úËâ≤",
    "Stroke Color": "ÊèèËæπÈ¢úËâ≤",
    "Stroke Width": "ÊèèËæπÁ≤óÁªÜ",
    "Generate Video": "ÁîüÊàêËßÜÈ¢ë",
    "Video Script and Subject Cannot Both Be Empty": "ËßÜÈ¢ë‰∏ªÈ¢ò Âíå ËßÜÈ¢ëÊñáÊ°àÔºå‰∏çËÉΩÂêåÊó∂‰∏∫Á©∫",
    "Generating Video": "Ê≠£Âú®ÁîüÊàêËßÜÈ¢ëÔºåËØ∑Á®çÂÄô...",
    "Start Generating Video": "ÂºÄÂßãÁîüÊàêËßÜÈ¢ë",
    "Video Generation Completed": "ËßÜÈ¢ëÁîüÊàêÂÆåÊàê",
    "Video Generation Failed": "ËßÜÈ¢ëÁîüÊàêÂ§±Ë¥•",
    "You can download the generated video from the following links": "‰Ω†ÂèØ‰ª•‰ªé‰ª•‰∏ãÈìæÊé•‰∏ãËΩΩÁîüÊàêÁöÑËßÜÈ¢ë",
    "Basic Settings": "**Âü∫Á°ÄËÆæÁΩÆ** (:blue[ÁÇπÂáªÂ±ïÂºÄ])",
    "Language": "ÁïåÈù¢ËØ≠Ë®Ä",
    "Pexels API Key": "Pexels API Key ([ÁÇπÂáªËé∑Âèñ](https://www.pexels.com/api/)) :red[Êé®Ëçê‰ΩøÁî®]",
    "Pixabay API Key": "Pixabay API Key ([ÁÇπÂáªËé∑Âèñ](https://pixabay.com/api/docs/#api_search_videos)) :red[ÂèØ‰ª•‰∏çÁî®ÈÖçÁΩÆÔºåÂ¶ÇÊûú Pexels Êó†Ê≥ï‰ΩøÁî®ÔºåÂÜçÈÄâÊã©Pixabay]",
    "LLM Provider": "Â§ßÊ®°ÂûãÊèê‰æõÂïÜ",
    "API Key": "API Key (:red[ÂøÖÂ°´ÔºåÈúÄË¶ÅÂà∞Â§ßÊ®°ÂûãÊèê‰æõÂïÜÁöÑÂêéÂè∞Áî≥ËØ∑])",
    "Base Url": "Base Url (ÂèØÈÄâ)",
    "Account ID": "Ë¥¶Êà∑ID (CloudflareÁöÑdashÈù¢Êùøurl‰∏≠Ëé∑Âèñ)",
    "Model Name": "Ê®°ÂûãÂêçÁß∞ (:blue[ÈúÄË¶ÅÂà∞Â§ßÊ®°ÂûãÊèê‰æõÂïÜÁöÑÂêéÂè∞Á°ÆËÆ§Ë¢´ÊéàÊùÉÁöÑÊ®°ÂûãÂêçÁß∞])",
    "Please Enter the LLM API Key": "ËØ∑ÂÖàÂ°´ÂÜôÂ§ßÊ®°Âûã **API Key**",
    "Please Enter the Pexels API Key": "ËØ∑ÂÖàÂ°´ÂÜô **Pexels API Key**",
    "Please Enter the Pixabay API Key": "ËØ∑ÂÖàÂ°´ÂÜô **Pixabay API Key**",
    "Get Help": "Êúâ‰ªª‰ΩïÈóÆÈ¢òÊàñÂª∫ËÆÆÔºåÂèØ‰ª•Âä†ÂÖ• **ÂæÆ‰ø°Áæ§** Ê±ÇÂä©ÊàñËÆ®ËÆ∫Ôºöhttps://harryai.cc",
    "Video Source": "ËßÜÈ¢ëÊù•Ê∫ê",
    "TikTok": "ÊäñÈü≥ (TikTok ÊîØÊåÅ‰∏≠ÔºåÊï¨ËØ∑ÊúüÂæÖ)",
    "Bilibili": "ÂìîÂì©ÂìîÂì© (Bilibili ÊîØÊåÅ‰∏≠ÔºåÊï¨ËØ∑ÊúüÂæÖ)",
    "Xiaohongshu": "Â∞èÁ∫¢‰π¶ (Xiaohongshu ÊîØÊåÅ‰∏≠ÔºåÊï¨ËØ∑ÊúüÂæÖ)",
    "Local file": "Êú¨Âú∞Êñá‰ª∂",
    "Play Voice": "ËØïÂê¨ËØ≠Èü≥ÂêàÊàê",
    "Voice Example": "ËøôÊòØ‰∏ÄÊÆµÊµãËØïËØ≠Èü≥ÂêàÊàêÁöÑÁ§∫‰æãÊñáÊú¨",
    "Synthesizing Voice": "ËØ≠Èü≥ÂêàÊàê‰∏≠ÔºåËØ∑Á®çÂÄô...",
    "TTS Provider": "ËØ≠Èü≥ÂêàÊàêÊèê‰æõÂïÜ",
    "TTS Servers": "TTSÊúçÂä°Âô®",
    "No voices available for the selected TTS server. Please select another server.": "ÂΩìÂâçÈÄâÊã©ÁöÑTTSÊúçÂä°Âô®Ê≤°ÊúâÂèØÁî®ÁöÑÂ£∞Èü≥ÔºåËØ∑ÈÄâÊã©ÂÖ∂‰ªñÊúçÂä°Âô®„ÄÇ",
    "SiliconFlow API Key": "Á°ÖÂü∫ÊµÅÂä®APIÂØÜÈí• [ÁÇπÂáªËé∑Âèñ](https://cloud.siliconflow.cn/account/ak)",
    "SiliconFlow TTS Settings": "Á°ÖÂü∫ÊµÅÂä®TTSËÆæÁΩÆ",
    "Speed: Range [0.25, 4.0], default is 1.0": "ËØ≠ÈÄüËåÉÂõ¥ [0.25, 4.0]ÔºåÈªòËÆ§ÂÄº‰∏∫1.0",
    "Volume: Uses Speech Volume setting, default 1.0 maps to gain 0": "Èü≥ÈáèÔºö‰ΩøÁî®ÊúóËØªÈü≥ÈáèËÆæÁΩÆÔºåÈªòËÆ§ÂÄº1.0ÂØπÂ∫îÂ¢ûÁõä0",
    "Hide Log": "ÈöêËóèÊó•Âøó",
    "Hide Basic Settings": "ÈöêËóèÂü∫Á°ÄËÆæÁΩÆ\n\nÈöêËóèÂêéÔºåÂü∫Á°ÄËÆæÁΩÆÈù¢ÊùøÂ∞Ü‰∏ç‰ºöÊòæÁ§∫Âú®È°µÈù¢‰∏≠„ÄÇ\n\nÂ¶ÇÈúÄË¶ÅÂÜçÊ¨°ÊòæÁ§∫ÔºåËØ∑Âú® `config.toml` ‰∏≠ËÆæÁΩÆ `hide_config = false`",
    "LLM Settings": "**Â§ßÊ®°ÂûãËÆæÁΩÆ**",
    "Video Source Settings": "**ËßÜÈ¢ëÊ∫êËÆæÁΩÆ**"
  }
}
</file>

<file path="README-en.md">
<div align="center">
<h1 align="center">MoneyPrinterTurbo üí∏</h1>

<p align="center">
  <a href="https://github.com/harry0703/MoneyPrinterTurbo/stargazers"><img src="https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Stargazers"></a>
  <a href="https://github.com/harry0703/MoneyPrinterTurbo/issues"><img src="https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Issues"></a>
  <a href="https://github.com/harry0703/MoneyPrinterTurbo/network/members"><img src="https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Forks"></a>
  <a href="https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE"><img src="https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="License"></a>
</p>

<h3>English | <a href="README.md">ÁÆÄ‰Ωì‰∏≠Êñá</a></h3>

<div align="center">
  <a href="https://trendshift.io/repositories/8731" target="_blank"><img src="https://trendshift.io/api/badge/repositories/8731" alt="harry0703%2FMoneyPrinterTurbo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>

Simply provide a <b>topic</b> or <b>keyword</b> for a video, and it will automatically generate the video copy, video
materials, video subtitles, and video background music before synthesizing a high-definition short video.

### WebUI

![](docs/webui-en.jpg)

### API Interface

![](docs/api.jpg)

</div>

## Special Thanks üôè

Due to the **deployment** and **usage** of this project, there is a certain threshold for some beginner users. We would
like to express our special thanks to

**RecCloud (AI-Powered Multimedia Service Platform)** for providing a free `AI Video Generator` service based on this
project. It allows for online use without deployment, which is very convenient.

- Chinese version: https://reccloud.cn
- English version: https://reccloud.com

![](docs/reccloud.com.jpg)

## Thanks for Sponsorship üôè

Thanks to Picwish https://picwish.com for supporting and sponsoring this project, enabling continuous updates and maintenance.

Picwish focuses on the **image processing field**, providing a rich set of **image processing tools** that extremely simplify complex operations, truly making image processing easier.

![picwish.jpg](docs/picwish.com.jpg)

## Features üéØ

- [x] Complete **MVC architecture**, **clearly structured** code, easy to maintain, supports both `API`
  and `Web interface`
- [x] Supports **AI-generated** video copy, as well as **customized copy**
- [x] Supports various **high-definition video** sizes
    - [x] Portrait 9:16, `1080x1920`
    - [x] Landscape 16:9, `1920x1080`
- [x] Supports **batch video generation**, allowing the creation of multiple videos at once, then selecting the most
  satisfactory one
- [x] Supports setting the **duration of video clips**, facilitating adjustments to material switching frequency
- [x] Supports video copy in both **Chinese** and **English**
- [x] Supports **multiple voice** synthesis, with **real-time preview** of effects
- [x] Supports **subtitle generation**, with adjustable `font`, `position`, `color`, `size`, and also
  supports `subtitle outlining`
- [x] Supports **background music**, either random or specified music files, with adjustable `background music volume`
- [x] Video material sources are **high-definition** and **royalty-free**, and you can also use your own **local materials**
- [x] Supports integration with various models such as **OpenAI**, **Moonshot**, **Azure**, **gpt4free**, **one-api**, **Qwen**, **Google Gemini**, **Ollama**, **DeepSeek**, **ERNIE**, **Pollinations** and more

### Future Plans üìÖ

- [ ] GPT-SoVITS dubbing support
- [ ] Optimize voice synthesis using large models for more natural and emotionally rich voice output
- [ ] Add video transition effects for a smoother viewing experience
- [ ] Add more video material sources, improve the matching between video materials and script
- [ ] Add video length options: short, medium, long
- [ ] Support more voice synthesis providers, such as OpenAI TTS
- [ ] Automate upload to YouTube platform

## Video Demos üì∫

### Portrait 9:16

<table>
<thead>
<tr>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji> How to Add Fun to Your Life </th>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji> What is the Meaning of Life</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6"></video></td>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476"></video></td>
</tr>
</tbody>
</table>

### Landscape 16:9

<table>
<thead>
<tr>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji> What is the Meaning of Life</th>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji> Why Exercise</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073"></video></td>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87"></video></td>
</tr>
</tbody>
</table>

## System Requirements üì¶

- Recommended minimum 4 CPU cores or more, 4G of memory or more, GPU is not required
- Windows 10 or MacOS 11.0, and their later versions

## Quick Start üöÄ

### Run in Google Colab 
Want to try MoneyPrinterTurbo without setting up a local environment? Run it directly in Google Colab!

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harry0703/MoneyPrinterTurbo/blob/main/docs/MoneyPrinterTurbo.ipynb)


### Windows

Google Drive (v1.2.6): https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing

After downloading, it is recommended to **double-click** `update.bat` first to update to the **latest code**, then double-click `start.bat` to launch

After launching, the browser will open automatically (if it opens blank, it is recommended to use **Chrome** or **Edge**)

### Other Systems

One-click startup packages have not been created yet. See the **Installation & Deployment** section below. It is recommended to use **docker** for deployment, which is more convenient.

## Installation & Deployment üì•

### Prerequisites

#### ‚ë† Clone the Project

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
```

#### ‚ë° Modify the Configuration File

- Copy the `config.example.toml` file and rename it to `config.toml`
- Follow the instructions in the `config.toml` file to configure `pexels_api_keys` and `llm_provider`, and according to
  the llm_provider's service provider, set up the corresponding API Key

### Docker Deployment üê≥

#### ‚ë† Launch the Docker Container

If you haven't installed Docker, please install it first https://www.docker.com/products/docker-desktop/
If you are using a Windows system, please refer to Microsoft's documentation:

1. https://learn.microsoft.com/en-us/windows/wsl/install
2. https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers

```shell
cd MoneyPrinterTurbo
docker-compose up
```

> NoteÔºöThe latest version of docker will automatically install docker compose in the form of a plug-in, and the start command is adjusted to `docker compose up `

#### ‚ë° Access the Web Interface

Open your browser and visit http://0.0.0.0:8501

#### ‚ë¢ Access the API Interface

Open your browser and visit http://0.0.0.0:8080/docs Or http://0.0.0.0:8080/redoc

### Manual Deployment üì¶

#### ‚ë† Create a Python Virtual Environment

It is recommended to create a Python virtual environment using [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
```

#### ‚ë° Install ImageMagick

###### Windows:

- Download https://imagemagick.org/script/download.php Choose the Windows version, make sure to select the **static library** version, such as ImageMagick-7.1.1-32-Q16-x64-**static**.exe
- Install the downloaded ImageMagick, **do not change the installation path**
- Modify the `config.toml` configuration file, set `imagemagick_path` to your actual installation path

###### MacOS:

```shell
brew install imagemagick
````

###### Ubuntu

```shell
sudo apt-get install imagemagick
```

###### CentOS

```shell
sudo yum install ImageMagick
```

#### ‚ë¢ Launch the Web Interface üåê

Note that you need to execute the following commands in the `root directory` of the MoneyPrinterTurbo project

###### Windows

```bat
webui.bat
```

###### MacOS or Linux

```shell
sh webui.sh
```

After launching, the browser will open automatically

#### ‚ë£ Launch the API Service üöÄ

```shell
python main.py
```

After launching, you can view the `API documentation` at http://127.0.0.1:8080/docs and directly test the interface
online for a quick experience.

## Voice Synthesis üó£

A list of all supported voices can be viewed here: [Voice List](./docs/voice-list.txt)

2024-04-16 v1.1.2 Added 9 new Azure voice synthesis voices that require API KEY configuration. These voices sound more realistic.

## Subtitle Generation üìú

Currently, there are 2 ways to generate subtitles:

- **edge**: Faster generation speed, better performance, no specific requirements for computer configuration, but the
  quality may be unstable
- **whisper**: Slower generation speed, poorer performance, specific requirements for computer configuration, but more
  reliable quality

You can switch between them by modifying the `subtitle_provider` in the `config.toml` configuration file

It is recommended to use `edge` mode, and switch to `whisper` mode if the quality of the subtitles generated is not
satisfactory.

> Note:
>
> 1. In whisper mode, you need to download a model file from HuggingFace, about 3GB in size, please ensure good internet connectivity
> 2. If left blank, it means no subtitles will be generated.

> Since HuggingFace is not accessible in China, you can use the following methods to download the `whisper-large-v3` model file

Download links:

- Baidu Netdisk: https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9
- Quark Netdisk: https://pan.quark.cn/s/3ee3d991d64b

After downloading the model, extract it and place the entire directory in `.\MoneyPrinterTurbo\models`,
The final file path should look like this: `.\MoneyPrinterTurbo\models\whisper-large-v3`

```
MoneyPrinterTurbo
  ‚îú‚îÄmodels
  ‚îÇ   ‚îî‚îÄwhisper-large-v3
  ‚îÇ          config.json
  ‚îÇ          model.bin
  ‚îÇ          preprocessor_config.json
  ‚îÇ          tokenizer.json
  ‚îÇ          vocabulary.json
```

## Background Music üéµ

Background music for videos is located in the project's `resource/songs` directory.
> The current project includes some default music from YouTube videos. If there are copyright issues, please delete
> them.

## Subtitle Fonts üÖ∞

Fonts for rendering video subtitles are located in the project's `resource/fonts` directory, and you can also add your
own fonts.

## Common Questions ü§î

### ‚ùìRuntimeError: No ffmpeg exe could be found

Normally, ffmpeg will be automatically downloaded and detected.
However, if your environment has issues preventing automatic downloads, you may encounter the following error:

```
RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
```

In this case, you can download ffmpeg from https://www.gyan.dev/ffmpeg/builds/, unzip it, and set `ffmpeg_path` to your
actual installation path.

```toml
[app]
# Please set according to your actual path, note that Windows path separators are \\
ffmpeg_path = "C:\\Users\\harry\\Downloads\\ffmpeg.exe"
```

### ‚ùìImageMagick is not installed on your computer

[issue 33](https://github.com/harry0703/MoneyPrinterTurbo/issues/33)

1. Follow the `example configuration` provided `download address` to
   install https://imagemagick.org/archive/binaries/ImageMagick-7.1.1-30-Q16-x64-static.exe, using the static library
2. Do not install in a path with Chinese characters to avoid unpredictable issues

[issue 54](https://github.com/harry0703/MoneyPrinterTurbo/issues/54#issuecomment-2017842022)

For Linux systems, you can manually install it, refer to https://cn.linux-console.net/?p=16978

Thanks to [@wangwenqiao666](https://github.com/wangwenqiao666) for their research and exploration

### ‚ùìImageMagick's security policy prevents operations related to temporary file @/tmp/tmpur5hyyto.txt

You can find these policies in ImageMagick's configuration file policy.xml.
This file is usually located in /etc/ImageMagick-`X`/ or a similar location in the ImageMagick installation directory.
Modify the entry containing `pattern="@"`, change `rights="none"` to `rights="read|write"` to allow read and write operations on files.

### ‚ùìOSError: [Errno 24] Too many open files

This issue is caused by the system's limit on the number of open files. You can solve it by modifying the system's file open limit.

Check the current limit:

```shell
ulimit -n
```

If it's too low, you can increase it, for example:

```shell
ulimit -n 10240
```

### ‚ùìWhisper model download failed, with the following error

LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and
outgoing trafic has been disabled.
To enablerepo look-ups and downloads online, pass 'local files only=False' as input.

or

An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub:
An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the
specified revision on the local disk. Please check your internet connection and try again.
Trying to load the model directly from the local cache, if it exists.

Solution: [Click to see how to manually download the model from netdisk](#subtitle-generation-)

## Feedback & Suggestions üì¢

- You can submit an [issue](https://github.com/harry0703/MoneyPrinterTurbo/issues) or
  a [pull request](https://github.com/harry0703/MoneyPrinterTurbo/pulls).

## License üìù

Click to view the [`LICENSE`](LICENSE) file

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&type=Date)](https://star-history.com/#harry0703/MoneyPrinterTurbo&Date)
</file>

<file path="app/config/config.py">
import os
import shutil
import socket

import toml
from loguru import logger

root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
config_file = f"{root_dir}/config.toml"


def load_config():
    # fix: IsADirectoryError: [Errno 21] Is a directory: '/MoneyPrinterTurbo/config.toml'
    if os.path.isdir(config_file):
        shutil.rmtree(config_file)

    if not os.path.isfile(config_file):
        example_file = f"{root_dir}/config.example.toml"
        if os.path.isfile(example_file):
            shutil.copyfile(example_file, config_file)
            logger.info("copy config.example.toml to config.toml")

    logger.info(f"load config from file: {config_file}")

    try:
        _config_ = toml.load(config_file)
    except Exception as e:
        logger.warning(f"load config failed: {str(e)}, try to load as utf-8-sig")
        with open(config_file, mode="r", encoding="utf-8-sig") as fp:
            _cfg_content = fp.read()
            _config_ = toml.loads(_cfg_content)
    return _config_


def save_config():
    with open(config_file, "w", encoding="utf-8") as f:
        _cfg["app"] = app
        _cfg["azure"] = azure
        _cfg["siliconflow"] = siliconflow
        _cfg["ui"] = ui
        f.write(toml.dumps(_cfg))


_cfg = load_config()
app = _cfg.get("app", {})
whisper = _cfg.get("whisper", {})
proxy = _cfg.get("proxy", {})
azure = _cfg.get("azure", {})
siliconflow = _cfg.get("siliconflow", {})
ui = _cfg.get(
    "ui",
    {
        "hide_log": False,
    },
)

hostname = socket.gethostname()

log_level = _cfg.get("log_level", "DEBUG")
listen_host = _cfg.get("listen_host", "0.0.0.0")
listen_port = _cfg.get("listen_port", 8080)
project_name = _cfg.get("project_name", "MoneyPrinterTurbo")
project_description = _cfg.get(
    "project_description",
    "<a href='https://github.com/harry0703/MoneyPrinterTurbo'>https://github.com/harry0703/MoneyPrinterTurbo</a>",
)
project_version = _cfg.get("project_version", "1.2.6")
reload_debug = False

imagemagick_path = app.get("imagemagick_path", "")
if imagemagick_path and os.path.isfile(imagemagick_path):
    os.environ["IMAGEMAGICK_BINARY"] = imagemagick_path

ffmpeg_path = app.get("ffmpeg_path", "")
if ffmpeg_path and os.path.isfile(ffmpeg_path):
    os.environ["IMAGEIO_FFMPEG_EXE"] = ffmpeg_path

logger.info(f"{project_name} v{project_version}")
</file>

<file path="app/services/voice.py">
import asyncio
import os
import re
from datetime import datetime
from typing import Union
from xml.sax.saxutils import unescape

import edge_tts
import requests
from edge_tts import SubMaker, submaker
from edge_tts.submaker import mktimestamp
from loguru import logger
from moviepy.video.tools import subtitles

from app.config import config
from app.utils import utils


def get_siliconflow_voices() -> list[str]:
    """
    Ëé∑ÂèñÁ°ÖÂü∫ÊµÅÂä®ÁöÑÂ£∞Èü≥ÂàóË°®

    Returns:
        Â£∞Èü≥ÂàóË°®ÔºåÊ†ºÂºè‰∏∫ ["siliconflow:FunAudioLLM/CosyVoice2-0.5B:alex", ...]
    """
    # Á°ÖÂü∫ÊµÅÂä®ÁöÑÂ£∞Èü≥ÂàóË°®ÂíåÂØπÂ∫îÁöÑÊÄßÂà´ÔºàÁî®‰∫éÊòæÁ§∫Ôºâ
    voices_with_gender = [
        ("FunAudioLLM/CosyVoice2-0.5B", "alex", "Male"),
        ("FunAudioLLM/CosyVoice2-0.5B", "anna", "Female"),
        ("FunAudioLLM/CosyVoice2-0.5B", "bella", "Female"),
        ("FunAudioLLM/CosyVoice2-0.5B", "benjamin", "Male"),
        ("FunAudioLLM/CosyVoice2-0.5B", "charles", "Male"),
        ("FunAudioLLM/CosyVoice2-0.5B", "claire", "Female"),
        ("FunAudioLLM/CosyVoice2-0.5B", "david", "Male"),
        ("FunAudioLLM/CosyVoice2-0.5B", "diana", "Female"),
    ]

    # Ê∑ªÂä†siliconflow:ÂâçÁºÄÔºåÂπ∂Ê†ºÂºèÂåñ‰∏∫ÊòæÁ§∫ÂêçÁß∞
    return [
        f"siliconflow:{model}:{voice}-{gender}"
        for model, voice, gender in voices_with_gender
    ]


def get_all_azure_voices(filter_locals=None) -> list[str]:
    azure_voices_str = """
Name: af-ZA-AdriNeural
Gender: Female

Name: af-ZA-WillemNeural
Gender: Male

Name: am-ET-AmehaNeural
Gender: Male

Name: am-ET-MekdesNeural
Gender: Female

Name: ar-AE-FatimaNeural
Gender: Female

Name: ar-AE-HamdanNeural
Gender: Male

Name: ar-BH-AliNeural
Gender: Male

Name: ar-BH-LailaNeural
Gender: Female

Name: ar-DZ-AminaNeural
Gender: Female

Name: ar-DZ-IsmaelNeural
Gender: Male

Name: ar-EG-SalmaNeural
Gender: Female

Name: ar-EG-ShakirNeural
Gender: Male

Name: ar-IQ-BasselNeural
Gender: Male

Name: ar-IQ-RanaNeural
Gender: Female

Name: ar-JO-SanaNeural
Gender: Female

Name: ar-JO-TaimNeural
Gender: Male

Name: ar-KW-FahedNeural
Gender: Male

Name: ar-KW-NouraNeural
Gender: Female

Name: ar-LB-LaylaNeural
Gender: Female

Name: ar-LB-RamiNeural
Gender: Male

Name: ar-LY-ImanNeural
Gender: Female

Name: ar-LY-OmarNeural
Gender: Male

Name: ar-MA-JamalNeural
Gender: Male

Name: ar-MA-MounaNeural
Gender: Female

Name: ar-OM-AbdullahNeural
Gender: Male

Name: ar-OM-AyshaNeural
Gender: Female

Name: ar-QA-AmalNeural
Gender: Female

Name: ar-QA-MoazNeural
Gender: Male

Name: ar-SA-HamedNeural
Gender: Male

Name: ar-SA-ZariyahNeural
Gender: Female

Name: ar-SY-AmanyNeural
Gender: Female

Name: ar-SY-LaithNeural
Gender: Male

Name: ar-TN-HediNeural
Gender: Male

Name: ar-TN-ReemNeural
Gender: Female

Name: ar-YE-MaryamNeural
Gender: Female

Name: ar-YE-SalehNeural
Gender: Male

Name: az-AZ-BabekNeural
Gender: Male

Name: az-AZ-BanuNeural
Gender: Female

Name: bg-BG-BorislavNeural
Gender: Male

Name: bg-BG-KalinaNeural
Gender: Female

Name: bn-BD-NabanitaNeural
Gender: Female

Name: bn-BD-PradeepNeural
Gender: Male

Name: bn-IN-BashkarNeural
Gender: Male

Name: bn-IN-TanishaaNeural
Gender: Female

Name: bs-BA-GoranNeural
Gender: Male

Name: bs-BA-VesnaNeural
Gender: Female

Name: ca-ES-EnricNeural
Gender: Male

Name: ca-ES-JoanaNeural
Gender: Female

Name: cs-CZ-AntoninNeural
Gender: Male

Name: cs-CZ-VlastaNeural
Gender: Female

Name: cy-GB-AledNeural
Gender: Male

Name: cy-GB-NiaNeural
Gender: Female

Name: da-DK-ChristelNeural
Gender: Female

Name: da-DK-JeppeNeural
Gender: Male

Name: de-AT-IngridNeural
Gender: Female

Name: de-AT-JonasNeural
Gender: Male

Name: de-CH-JanNeural
Gender: Male

Name: de-CH-LeniNeural
Gender: Female

Name: de-DE-AmalaNeural
Gender: Female

Name: de-DE-ConradNeural
Gender: Male

Name: de-DE-FlorianMultilingualNeural
Gender: Male

Name: de-DE-KatjaNeural
Gender: Female

Name: de-DE-KillianNeural
Gender: Male

Name: de-DE-SeraphinaMultilingualNeural
Gender: Female

Name: el-GR-AthinaNeural
Gender: Female

Name: el-GR-NestorasNeural
Gender: Male

Name: en-AU-NatashaNeural
Gender: Female

Name: en-AU-WilliamNeural
Gender: Male

Name: en-CA-ClaraNeural
Gender: Female

Name: en-CA-LiamNeural
Gender: Male

Name: en-GB-LibbyNeural
Gender: Female

Name: en-GB-MaisieNeural
Gender: Female

Name: en-GB-RyanNeural
Gender: Male

Name: en-GB-SoniaNeural
Gender: Female

Name: en-GB-ThomasNeural
Gender: Male

Name: en-HK-SamNeural
Gender: Male

Name: en-HK-YanNeural
Gender: Female

Name: en-IE-ConnorNeural
Gender: Male

Name: en-IE-EmilyNeural
Gender: Female

Name: en-IN-NeerjaExpressiveNeural
Gender: Female

Name: en-IN-NeerjaNeural
Gender: Female

Name: en-IN-PrabhatNeural
Gender: Male

Name: en-KE-AsiliaNeural
Gender: Female

Name: en-KE-ChilembaNeural
Gender: Male

Name: en-NG-AbeoNeural
Gender: Male

Name: en-NG-EzinneNeural
Gender: Female

Name: en-NZ-MitchellNeural
Gender: Male

Name: en-NZ-MollyNeural
Gender: Female

Name: en-PH-JamesNeural
Gender: Male

Name: en-PH-RosaNeural
Gender: Female

Name: en-SG-LunaNeural
Gender: Female

Name: en-SG-WayneNeural
Gender: Male

Name: en-TZ-ElimuNeural
Gender: Male

Name: en-TZ-ImaniNeural
Gender: Female

Name: en-US-AnaNeural
Gender: Female

Name: en-US-AndrewMultilingualNeural
Gender: Male

Name: en-US-AndrewNeural
Gender: Male

Name: en-US-AriaNeural
Gender: Female

Name: en-US-AvaMultilingualNeural
Gender: Female

Name: en-US-AvaNeural
Gender: Female

Name: en-US-BrianMultilingualNeural
Gender: Male

Name: en-US-BrianNeural
Gender: Male

Name: en-US-ChristopherNeural
Gender: Male

Name: en-US-EmmaMultilingualNeural
Gender: Female

Name: en-US-EmmaNeural
Gender: Female

Name: en-US-EricNeural
Gender: Male

Name: en-US-GuyNeural
Gender: Male

Name: en-US-JennyNeural
Gender: Female

Name: en-US-MichelleNeural
Gender: Female

Name: en-US-RogerNeural
Gender: Male

Name: en-US-SteffanNeural
Gender: Male

Name: en-ZA-LeahNeural
Gender: Female

Name: en-ZA-LukeNeural
Gender: Male

Name: es-AR-ElenaNeural
Gender: Female

Name: es-AR-TomasNeural
Gender: Male

Name: es-BO-MarceloNeural
Gender: Male

Name: es-BO-SofiaNeural
Gender: Female

Name: es-CL-CatalinaNeural
Gender: Female

Name: es-CL-LorenzoNeural
Gender: Male

Name: es-CO-GonzaloNeural
Gender: Male

Name: es-CO-SalomeNeural
Gender: Female

Name: es-CR-JuanNeural
Gender: Male

Name: es-CR-MariaNeural
Gender: Female

Name: es-CU-BelkysNeural
Gender: Female

Name: es-CU-ManuelNeural
Gender: Male

Name: es-DO-EmilioNeural
Gender: Male

Name: es-DO-RamonaNeural
Gender: Female

Name: es-EC-AndreaNeural
Gender: Female

Name: es-EC-LuisNeural
Gender: Male

Name: es-ES-AlvaroNeural
Gender: Male

Name: es-ES-ElviraNeural
Gender: Female

Name: es-ES-XimenaNeural
Gender: Female

Name: es-GQ-JavierNeural
Gender: Male

Name: es-GQ-TeresaNeural
Gender: Female

Name: es-GT-AndresNeural
Gender: Male

Name: es-GT-MartaNeural
Gender: Female

Name: es-HN-CarlosNeural
Gender: Male

Name: es-HN-KarlaNeural
Gender: Female

Name: es-MX-DaliaNeural
Gender: Female

Name: es-MX-JorgeNeural
Gender: Male

Name: es-NI-FedericoNeural
Gender: Male

Name: es-NI-YolandaNeural
Gender: Female

Name: es-PA-MargaritaNeural
Gender: Female

Name: es-PA-RobertoNeural
Gender: Male

Name: es-PE-AlexNeural
Gender: Male

Name: es-PE-CamilaNeural
Gender: Female

Name: es-PR-KarinaNeural
Gender: Female

Name: es-PR-VictorNeural
Gender: Male

Name: es-PY-MarioNeural
Gender: Male

Name: es-PY-TaniaNeural
Gender: Female

Name: es-SV-LorenaNeural
Gender: Female

Name: es-SV-RodrigoNeural
Gender: Male

Name: es-US-AlonsoNeural
Gender: Male

Name: es-US-PalomaNeural
Gender: Female

Name: es-UY-MateoNeural
Gender: Male

Name: es-UY-ValentinaNeural
Gender: Female

Name: es-VE-PaolaNeural
Gender: Female

Name: es-VE-SebastianNeural
Gender: Male

Name: et-EE-AnuNeural
Gender: Female

Name: et-EE-KertNeural
Gender: Male

Name: fa-IR-DilaraNeural
Gender: Female

Name: fa-IR-FaridNeural
Gender: Male

Name: fi-FI-HarriNeural
Gender: Male

Name: fi-FI-NooraNeural
Gender: Female

Name: fil-PH-AngeloNeural
Gender: Male

Name: fil-PH-BlessicaNeural
Gender: Female

Name: fr-BE-CharlineNeural
Gender: Female

Name: fr-BE-GerardNeural
Gender: Male

Name: fr-CA-AntoineNeural
Gender: Male

Name: fr-CA-JeanNeural
Gender: Male

Name: fr-CA-SylvieNeural
Gender: Female

Name: fr-CA-ThierryNeural
Gender: Male

Name: fr-CH-ArianeNeural
Gender: Female

Name: fr-CH-FabriceNeural
Gender: Male

Name: fr-FR-DeniseNeural
Gender: Female

Name: fr-FR-EloiseNeural
Gender: Female

Name: fr-FR-HenriNeural
Gender: Male

Name: fr-FR-RemyMultilingualNeural
Gender: Male

Name: fr-FR-VivienneMultilingualNeural
Gender: Female

Name: ga-IE-ColmNeural
Gender: Male

Name: ga-IE-OrlaNeural
Gender: Female

Name: gl-ES-RoiNeural
Gender: Male

Name: gl-ES-SabelaNeural
Gender: Female

Name: gu-IN-DhwaniNeural
Gender: Female

Name: gu-IN-NiranjanNeural
Gender: Male

Name: he-IL-AvriNeural
Gender: Male

Name: he-IL-HilaNeural
Gender: Female

Name: hi-IN-MadhurNeural
Gender: Male

Name: hi-IN-SwaraNeural
Gender: Female

Name: hr-HR-GabrijelaNeural
Gender: Female

Name: hr-HR-SreckoNeural
Gender: Male

Name: hu-HU-NoemiNeural
Gender: Female

Name: hu-HU-TamasNeural
Gender: Male

Name: id-ID-ArdiNeural
Gender: Male

Name: id-ID-GadisNeural
Gender: Female

Name: is-IS-GudrunNeural
Gender: Female

Name: is-IS-GunnarNeural
Gender: Male

Name: it-IT-DiegoNeural
Gender: Male

Name: it-IT-ElsaNeural
Gender: Female

Name: it-IT-GiuseppeMultilingualNeural
Gender: Male

Name: it-IT-IsabellaNeural
Gender: Female

Name: iu-Cans-CA-SiqiniqNeural
Gender: Female

Name: iu-Cans-CA-TaqqiqNeural
Gender: Male

Name: iu-Latn-CA-SiqiniqNeural
Gender: Female

Name: iu-Latn-CA-TaqqiqNeural
Gender: Male

Name: ja-JP-KeitaNeural
Gender: Male

Name: ja-JP-NanamiNeural
Gender: Female

Name: jv-ID-DimasNeural
Gender: Male

Name: jv-ID-SitiNeural
Gender: Female

Name: ka-GE-EkaNeural
Gender: Female

Name: ka-GE-GiorgiNeural
Gender: Male

Name: kk-KZ-AigulNeural
Gender: Female

Name: kk-KZ-DauletNeural
Gender: Male

Name: km-KH-PisethNeural
Gender: Male

Name: km-KH-SreymomNeural
Gender: Female

Name: kn-IN-GaganNeural
Gender: Male

Name: kn-IN-SapnaNeural
Gender: Female

Name: ko-KR-HyunsuMultilingualNeural
Gender: Male

Name: ko-KR-InJoonNeural
Gender: Male

Name: ko-KR-SunHiNeural
Gender: Female

Name: lo-LA-ChanthavongNeural
Gender: Male

Name: lo-LA-KeomanyNeural
Gender: Female

Name: lt-LT-LeonasNeural
Gender: Male

Name: lt-LT-OnaNeural
Gender: Female

Name: lv-LV-EveritaNeural
Gender: Female

Name: lv-LV-NilsNeural
Gender: Male

Name: mk-MK-AleksandarNeural
Gender: Male

Name: mk-MK-MarijaNeural
Gender: Female

Name: ml-IN-MidhunNeural
Gender: Male

Name: ml-IN-SobhanaNeural
Gender: Female

Name: mn-MN-BataaNeural
Gender: Male

Name: mn-MN-YesuiNeural
Gender: Female

Name: mr-IN-AarohiNeural
Gender: Female

Name: mr-IN-ManoharNeural
Gender: Male

Name: ms-MY-OsmanNeural
Gender: Male

Name: ms-MY-YasminNeural
Gender: Female

Name: mt-MT-GraceNeural
Gender: Female

Name: mt-MT-JosephNeural
Gender: Male

Name: my-MM-NilarNeural
Gender: Female

Name: my-MM-ThihaNeural
Gender: Male

Name: nb-NO-FinnNeural
Gender: Male

Name: nb-NO-PernilleNeural
Gender: Female

Name: ne-NP-HemkalaNeural
Gender: Female

Name: ne-NP-SagarNeural
Gender: Male

Name: nl-BE-ArnaudNeural
Gender: Male

Name: nl-BE-DenaNeural
Gender: Female

Name: nl-NL-ColetteNeural
Gender: Female

Name: nl-NL-FennaNeural
Gender: Female

Name: nl-NL-MaartenNeural
Gender: Male

Name: pl-PL-MarekNeural
Gender: Male

Name: pl-PL-ZofiaNeural
Gender: Female

Name: ps-AF-GulNawazNeural
Gender: Male

Name: ps-AF-LatifaNeural
Gender: Female

Name: pt-BR-AntonioNeural
Gender: Male

Name: pt-BR-FranciscaNeural
Gender: Female

Name: pt-BR-ThalitaMultilingualNeural
Gender: Female

Name: pt-PT-DuarteNeural
Gender: Male

Name: pt-PT-RaquelNeural
Gender: Female

Name: ro-RO-AlinaNeural
Gender: Female

Name: ro-RO-EmilNeural
Gender: Male

Name: ru-RU-DmitryNeural
Gender: Male

Name: ru-RU-SvetlanaNeural
Gender: Female

Name: si-LK-SameeraNeural
Gender: Male

Name: si-LK-ThiliniNeural
Gender: Female

Name: sk-SK-LukasNeural
Gender: Male

Name: sk-SK-ViktoriaNeural
Gender: Female

Name: sl-SI-PetraNeural
Gender: Female

Name: sl-SI-RokNeural
Gender: Male

Name: so-SO-MuuseNeural
Gender: Male

Name: so-SO-UbaxNeural
Gender: Female

Name: sq-AL-AnilaNeural
Gender: Female

Name: sq-AL-IlirNeural
Gender: Male

Name: sr-RS-NicholasNeural
Gender: Male

Name: sr-RS-SophieNeural
Gender: Female

Name: su-ID-JajangNeural
Gender: Male

Name: su-ID-TutiNeural
Gender: Female

Name: sv-SE-MattiasNeural
Gender: Male

Name: sv-SE-SofieNeural
Gender: Female

Name: sw-KE-RafikiNeural
Gender: Male

Name: sw-KE-ZuriNeural
Gender: Female

Name: sw-TZ-DaudiNeural
Gender: Male

Name: sw-TZ-RehemaNeural
Gender: Female

Name: ta-IN-PallaviNeural
Gender: Female

Name: ta-IN-ValluvarNeural
Gender: Male

Name: ta-LK-KumarNeural
Gender: Male

Name: ta-LK-SaranyaNeural
Gender: Female

Name: ta-MY-KaniNeural
Gender: Female

Name: ta-MY-SuryaNeural
Gender: Male

Name: ta-SG-AnbuNeural
Gender: Male

Name: ta-SG-VenbaNeural
Gender: Female

Name: te-IN-MohanNeural
Gender: Male

Name: te-IN-ShrutiNeural
Gender: Female

Name: th-TH-NiwatNeural
Gender: Male

Name: th-TH-PremwadeeNeural
Gender: Female

Name: tr-TR-AhmetNeural
Gender: Male

Name: tr-TR-EmelNeural
Gender: Female

Name: uk-UA-OstapNeural
Gender: Male

Name: uk-UA-PolinaNeural
Gender: Female

Name: ur-IN-GulNeural
Gender: Female

Name: ur-IN-SalmanNeural
Gender: Male

Name: ur-PK-AsadNeural
Gender: Male

Name: ur-PK-UzmaNeural
Gender: Female

Name: uz-UZ-MadinaNeural
Gender: Female

Name: uz-UZ-SardorNeural
Gender: Male

Name: vi-VN-HoaiMyNeural
Gender: Female

Name: vi-VN-NamMinhNeural
Gender: Male

Name: zh-CN-XiaoxiaoNeural
Gender: Female

Name: zh-CN-XiaoyiNeural
Gender: Female

Name: zh-CN-YunjianNeural
Gender: Male

Name: zh-CN-YunxiNeural
Gender: Male

Name: zh-CN-YunxiaNeural
Gender: Male

Name: zh-CN-YunyangNeural
Gender: Male

Name: zh-CN-liaoning-XiaobeiNeural
Gender: Female

Name: zh-CN-shaanxi-XiaoniNeural
Gender: Female

Name: zh-HK-HiuGaaiNeural
Gender: Female

Name: zh-HK-HiuMaanNeural
Gender: Female

Name: zh-HK-WanLungNeural
Gender: Male

Name: zh-TW-HsiaoChenNeural
Gender: Female

Name: zh-TW-HsiaoYuNeural
Gender: Female

Name: zh-TW-YunJheNeural
Gender: Male

Name: zu-ZA-ThandoNeural
Gender: Female

Name: zu-ZA-ThembaNeural
Gender: Male


Name: en-US-AvaMultilingualNeural-V2
Gender: Female

Name: en-US-AndrewMultilingualNeural-V2
Gender: Male

Name: en-US-EmmaMultilingualNeural-V2
Gender: Female

Name: en-US-BrianMultilingualNeural-V2
Gender: Male

Name: de-DE-FlorianMultilingualNeural-V2
Gender: Male

Name: de-DE-SeraphinaMultilingualNeural-V2
Gender: Female

Name: fr-FR-RemyMultilingualNeural-V2
Gender: Male

Name: fr-FR-VivienneMultilingualNeural-V2
Gender: Female

Name: zh-CN-XiaoxiaoMultilingualNeural-V2
Gender: Female
    """.strip()
    voices = []
    # ÂÆö‰πâÊ≠£ÂàôË°®ËææÂºèÊ®°ÂºèÔºåÁî®‰∫éÂåπÈÖç Name Âíå Gender Ë°å
    pattern = re.compile(r"Name:\s*(.+)\s*Gender:\s*(.+)\s*", re.MULTILINE)
    # ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊü•ÊâæÊâÄÊúâÂåπÈÖçÈ°π
    matches = pattern.findall(azure_voices_str)

    for name, gender in matches:
        # Â∫îÁî®ËøáÊª§Êù°‰ª∂
        if filter_locals and any(
            name.lower().startswith(fl.lower()) for fl in filter_locals
        ):
            voices.append(f"{name}-{gender}")
        elif not filter_locals:
            voices.append(f"{name}-{gender}")

    voices.sort()
    return voices


def parse_voice_name(name: str):
    # zh-CN-XiaoyiNeural-Female
    # zh-CN-YunxiNeural-Male
    # zh-CN-XiaoxiaoMultilingualNeural-V2-Female
    name = name.replace("-Female", "").replace("-Male", "").strip()
    return name


def is_azure_v2_voice(voice_name: str):
    voice_name = parse_voice_name(voice_name)
    if voice_name.endswith("-V2"):
        return voice_name.replace("-V2", "").strip()
    return ""


def is_siliconflow_voice(voice_name: str):
    """Ê£ÄÊü•ÊòØÂê¶ÊòØÁ°ÖÂü∫ÊµÅÂä®ÁöÑÂ£∞Èü≥"""
    return voice_name.startswith("siliconflow:")


def tts(
    text: str,
    voice_name: str,
    voice_rate: float,
    voice_file: str,
    voice_volume: float = 1.0,
) -> Union[SubMaker, None]:
    if is_azure_v2_voice(voice_name):
        return azure_tts_v2(text, voice_name, voice_file)
    elif is_siliconflow_voice(voice_name):
        # ‰ªévoice_name‰∏≠ÊèêÂèñÊ®°ÂûãÂíåÂ£∞Èü≥
        # Ê†ºÂºè: siliconflow:model:voice-Gender
        parts = voice_name.split(":")
        if len(parts) >= 3:
            model = parts[1]
            # ÁßªÈô§ÊÄßÂà´ÂêéÁºÄÔºå‰æãÂ¶Ç "alex-Male" -> "alex"
            voice_with_gender = parts[2]
            voice = voice_with_gender.split("-")[0]
            # ÊûÑÂª∫ÂÆåÊï¥ÁöÑvoiceÂèÇÊï∞ÔºåÊ†ºÂºè‰∏∫ "model:voice"
            full_voice = f"{model}:{voice}"
            return siliconflow_tts(
                text, model, full_voice, voice_rate, voice_file, voice_volume
            )
        else:
            logger.error(f"Invalid siliconflow voice name format: {voice_name}")
            return None
    return azure_tts_v1(text, voice_name, voice_rate, voice_file)


def convert_rate_to_percent(rate: float) -> str:
    if rate == 1.0:
        return "+0%"
    percent = round((rate - 1.0) * 100)
    if percent > 0:
        return f"+{percent}%"
    else:
        return f"{percent}%"


def azure_tts_v1(
    text: str, voice_name: str, voice_rate: float, voice_file: str
) -> Union[SubMaker, None]:
    voice_name = parse_voice_name(voice_name)
    text = text.strip()
    rate_str = convert_rate_to_percent(voice_rate)
    for i in range(3):
        try:
            logger.info(f"start, voice name: {voice_name}, try: {i + 1}")

            async def _do() -> SubMaker:
                communicate = edge_tts.Communicate(text, voice_name, rate=rate_str)
                sub_maker = edge_tts.SubMaker()
                with open(voice_file, "wb") as file:
                    async for chunk in communicate.stream():
                        if chunk["type"] == "audio":
                            file.write(chunk["data"])
                        elif chunk["type"] == "WordBoundary":
                            sub_maker.create_sub(
                                (chunk["offset"], chunk["duration"]), chunk["text"]
                            )
                return sub_maker

            sub_maker = asyncio.run(_do())
            if not sub_maker or not sub_maker.subs:
                logger.warning("failed, sub_maker is None or sub_maker.subs is None")
                continue

            logger.info(f"completed, output file: {voice_file}")
            return sub_maker
        except Exception as e:
            logger.error(f"failed, error: {str(e)}")
    return None


def siliconflow_tts(
    text: str,
    model: str,
    voice: str,
    voice_rate: float,
    voice_file: str,
    voice_volume: float = 1.0,
) -> Union[SubMaker, None]:
    """
    ‰ΩøÁî®Á°ÖÂü∫ÊµÅÂä®ÁöÑAPIÁîüÊàêËØ≠Èü≥

    Args:
        text: Ë¶ÅËΩ¨Êç¢‰∏∫ËØ≠Èü≥ÁöÑÊñáÊú¨
        model: Ê®°ÂûãÂêçÁß∞ÔºåÂ¶Ç "FunAudioLLM/CosyVoice2-0.5B"
        voice: Â£∞Èü≥ÂêçÁß∞ÔºåÂ¶Ç "FunAudioLLM/CosyVoice2-0.5B:alex"
        voice_rate: ËØ≠Èü≥ÈÄüÂ∫¶ÔºåËåÉÂõ¥[0.25, 4.0]
        voice_file: ËæìÂá∫ÁöÑÈü≥È¢ëÊñá‰ª∂Ë∑ØÂæÑ
        voice_volume: ËØ≠Èü≥Èü≥ÈáèÔºåËåÉÂõ¥[0.6, 5.0]ÔºåÈúÄË¶ÅËΩ¨Êç¢‰∏∫Á°ÖÂü∫ÊµÅÂä®ÁöÑÂ¢ûÁõäËåÉÂõ¥[-10, 10]

    Returns:
        SubMakerÂØπË±°ÊàñNone
    """
    text = text.strip()
    api_key = config.siliconflow.get("api_key", "")

    if not api_key:
        logger.error("SiliconFlow API key is not set")
        return None

    # Â∞Üvoice_volumeËΩ¨Êç¢‰∏∫Á°ÖÂü∫ÊµÅÂä®ÁöÑÂ¢ûÁõäËåÉÂõ¥
    # ÈªòËÆ§voice_volume‰∏∫1.0ÔºåÂØπÂ∫îgain‰∏∫0
    gain = voice_volume - 1.0
    # Á°Æ‰øùgainÂú®[-10, 10]ËåÉÂõ¥ÂÜÖ
    gain = max(-10, min(10, gain))

    url = "https://api.siliconflow.cn/v1/audio/speech"

    payload = {
        "model": model,
        "input": text,
        "voice": voice,
        "response_format": "mp3",
        "sample_rate": 32000,
        "stream": False,
        "speed": voice_rate,
        "gain": gain,
    }

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    for i in range(3):  # Â∞ùËØï3Ê¨°
        try:
            logger.info(
                f"start siliconflow tts, model: {model}, voice: {voice}, try: {i + 1}"
            )

            response = requests.post(url, json=payload, headers=headers)

            if response.status_code == 200:
                # ‰øùÂ≠òÈü≥È¢ëÊñá‰ª∂
                with open(voice_file, "wb") as f:
                    f.write(response.content)

                # ÂàõÂª∫‰∏Ä‰∏™Á©∫ÁöÑSubMakerÂØπË±°
                sub_maker = SubMaker()

                # Ëé∑ÂèñÈü≥È¢ëÊñá‰ª∂ÁöÑÂÆûÈôÖÈïøÂ∫¶
                try:
                    # Â∞ùËØï‰ΩøÁî®moviepyËé∑ÂèñÈü≥È¢ëÈïøÂ∫¶
                    from moviepy import AudioFileClip

                    audio_clip = AudioFileClip(voice_file)
                    audio_duration = audio_clip.duration
                    audio_clip.close()

                    # Â∞ÜÈü≥È¢ëÈïøÂ∫¶ËΩ¨Êç¢‰∏∫100Á∫≥ÁßíÂçï‰ΩçÔºà‰∏éedge_ttsÂÖºÂÆπÔºâ
                    audio_duration_100ns = int(audio_duration * 10000000)

                    # ‰ΩøÁî®ÊñáÊú¨ÂàÜÂâ≤Êù•ÂàõÂª∫Êõ¥ÂáÜÁ°ÆÁöÑÂ≠óÂπï
                    # Â∞ÜÊñáÊú¨ÊåâÊ†áÁÇπÁ¨¶Âè∑ÂàÜÂâ≤ÊàêÂè•Â≠ê
                    sentences = utils.split_string_by_punctuations(text)

                    if sentences:
                        # ËÆ°ÁÆóÊØè‰∏™Âè•Â≠êÁöÑÂ§ßËá¥Êó∂ÈïøÔºàÊåâÂ≠óÁ¨¶Êï∞ÊØî‰æãÂàÜÈÖçÔºâ
                        total_chars = sum(len(s) for s in sentences)
                        char_duration = (
                            audio_duration_100ns / total_chars if total_chars > 0 else 0
                        )

                        current_offset = 0
                        for sentence in sentences:
                            if not sentence.strip():
                                continue

                            # ËÆ°ÁÆóÂΩìÂâçÂè•Â≠êÁöÑÊó∂Èïø
                            sentence_chars = len(sentence)
                            sentence_duration = int(sentence_chars * char_duration)

                            # Ê∑ªÂä†Âà∞SubMaker
                            sub_maker.subs.append(sentence)
                            sub_maker.offset.append(
                                (current_offset, current_offset + sentence_duration)
                            )

                            # Êõ¥Êñ∞ÂÅèÁßªÈáè
                            current_offset += sentence_duration
                    else:
                        # Â¶ÇÊûúÊó†Ê≥ïÂàÜÂâ≤ÔºåÂàô‰ΩøÁî®Êï¥‰∏™ÊñáÊú¨‰Ωú‰∏∫‰∏Ä‰∏™Â≠óÂπï
                        sub_maker.subs = [text]
                        sub_maker.offset = [(0, audio_duration_100ns)]

                except Exception as e:
                    logger.warning(f"Failed to create accurate subtitles: {str(e)}")
                    # ÂõûÈÄÄÂà∞ÁÆÄÂçïÁöÑÂ≠óÂπï
                    sub_maker.subs = [text]
                    # ‰ΩøÁî®Èü≥È¢ëÊñá‰ª∂ÁöÑÂÆûÈôÖÈïøÂ∫¶ÔºåÂ¶ÇÊûúÊó†Ê≥ïËé∑ÂèñÔºåÂàôÂÅáËÆæ‰∏∫10Áßí
                    sub_maker.offset = [
                        (
                            0,
                            (
                                audio_duration_100ns
                                if "audio_duration_100ns" in locals()
                                else 10000000
                            ),
                        )
                    ]

                logger.success(f"siliconflow tts succeeded: {voice_file}")
                print("s", sub_maker.subs, sub_maker.offset)
                return sub_maker
            else:
                logger.error(
                    f"siliconflow tts failed with status code {response.status_code}: {response.text}"
                )
        except Exception as e:
            logger.error(f"siliconflow tts failed: {str(e)}")

    return None


def azure_tts_v2(text: str, voice_name: str, voice_file: str) -> Union[SubMaker, None]:
    voice_name = is_azure_v2_voice(voice_name)
    if not voice_name:
        logger.error(f"invalid voice name: {voice_name}")
        raise ValueError(f"invalid voice name: {voice_name}")
    text = text.strip()

    def _format_duration_to_offset(duration) -> int:
        if isinstance(duration, str):
            time_obj = datetime.strptime(duration, "%H:%M:%S.%f")
            milliseconds = (
                (time_obj.hour * 3600000)
                + (time_obj.minute * 60000)
                + (time_obj.second * 1000)
                + (time_obj.microsecond // 1000)
            )
            return milliseconds * 10000

        if isinstance(duration, int):
            return duration

        return 0

    for i in range(3):
        try:
            logger.info(f"start, voice name: {voice_name}, try: {i + 1}")

            import azure.cognitiveservices.speech as speechsdk

            sub_maker = SubMaker()

            def speech_synthesizer_word_boundary_cb(evt: speechsdk.SessionEventArgs):
                # print('WordBoundary event:')
                # print('\tBoundaryType: {}'.format(evt.boundary_type))
                # print('\tAudioOffset: {}ms'.format((evt.audio_offset + 5000)))
                # print('\tDuration: {}'.format(evt.duration))
                # print('\tText: {}'.format(evt.text))
                # print('\tTextOffset: {}'.format(evt.text_offset))
                # print('\tWordLength: {}'.format(evt.word_length))

                duration = _format_duration_to_offset(str(evt.duration))
                offset = _format_duration_to_offset(evt.audio_offset)
                sub_maker.subs.append(evt.text)
                sub_maker.offset.append((offset, offset + duration))

            # Creates an instance of a speech config with specified subscription key and service region.
            speech_key = config.azure.get("speech_key", "")
            service_region = config.azure.get("speech_region", "")
            if not speech_key or not service_region:
                logger.error("Azure speech key or region is not set")
                return None

            audio_config = speechsdk.audio.AudioOutputConfig(
                filename=voice_file, use_default_speaker=True
            )
            speech_config = speechsdk.SpeechConfig(
                subscription=speech_key, region=service_region
            )
            speech_config.speech_synthesis_voice_name = voice_name
            # speech_config.set_property(property_id=speechsdk.PropertyId.SpeechServiceResponse_RequestSentenceBoundary,
            #                            value='true')
            speech_config.set_property(
                property_id=speechsdk.PropertyId.SpeechServiceResponse_RequestWordBoundary,
                value="true",
            )

            speech_config.set_speech_synthesis_output_format(
                speechsdk.SpeechSynthesisOutputFormat.Audio48Khz192KBitRateMonoMp3
            )
            speech_synthesizer = speechsdk.SpeechSynthesizer(
                audio_config=audio_config, speech_config=speech_config
            )
            speech_synthesizer.synthesis_word_boundary.connect(
                speech_synthesizer_word_boundary_cb
            )

            result = speech_synthesizer.speak_text_async(text).get()
            if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                logger.success(f"azure v2 speech synthesis succeeded: {voice_file}")
                return sub_maker
            elif result.reason == speechsdk.ResultReason.Canceled:
                cancellation_details = result.cancellation_details
                logger.error(
                    f"azure v2 speech synthesis canceled: {cancellation_details.reason}"
                )
                if cancellation_details.reason == speechsdk.CancellationReason.Error:
                    logger.error(
                        f"azure v2 speech synthesis error: {cancellation_details.error_details}"
                    )
            logger.info(f"completed, output file: {voice_file}")
        except Exception as e:
            logger.error(f"failed, error: {str(e)}")
    return None


def _format_text(text: str) -> str:
    # text = text.replace("\n", " ")
    text = text.replace("[", " ")
    text = text.replace("]", " ")
    text = text.replace("(", " ")
    text = text.replace(")", " ")
    text = text.replace("{", " ")
    text = text.replace("}", " ")
    text = text.strip()
    return text


def create_subtitle(sub_maker: submaker.SubMaker, text: str, subtitle_file: str):
    """
    ‰ºòÂåñÂ≠óÂπïÊñá‰ª∂
    1. Â∞ÜÂ≠óÂπïÊñá‰ª∂ÊåâÁÖßÊ†áÁÇπÁ¨¶Âè∑ÂàÜÂâ≤ÊàêÂ§öË°å
    2. ÈÄêË°åÂåπÈÖçÂ≠óÂπïÊñá‰ª∂‰∏≠ÁöÑÊñáÊú¨
    3. ÁîüÊàêÊñ∞ÁöÑÂ≠óÂπïÊñá‰ª∂
    """

    text = _format_text(text)

    def formatter(idx: int, start_time: float, end_time: float, sub_text: str) -> str:
        """
        1
        00:00:00,000 --> 00:00:02,360
        Ë∑ëÊ≠•ÊòØ‰∏ÄÈ°πÁÆÄÂçïÊòìË°åÁöÑËøêÂä®
        """
        start_t = mktimestamp(start_time).replace(".", ",")
        end_t = mktimestamp(end_time).replace(".", ",")
        return f"{idx}\n{start_t} --> {end_t}\n{sub_text}\n"

    start_time = -1.0
    sub_items = []
    sub_index = 0

    script_lines = utils.split_string_by_punctuations(text)

    def match_line(_sub_line: str, _sub_index: int):
        if len(script_lines) <= _sub_index:
            return ""

        _line = script_lines[_sub_index]
        if _sub_line == _line:
            return script_lines[_sub_index].strip()

        _sub_line_ = re.sub(r"[^\w\s]", "", _sub_line)
        _line_ = re.sub(r"[^\w\s]", "", _line)
        if _sub_line_ == _line_:
            return _line_.strip()

        _sub_line_ = re.sub(r"\W+", "", _sub_line)
        _line_ = re.sub(r"\W+", "", _line)
        if _sub_line_ == _line_:
            return _line.strip()

        return ""

    sub_line = ""

    try:
        for _, (offset, sub) in enumerate(zip(sub_maker.offset, sub_maker.subs)):
            _start_time, end_time = offset
            if start_time < 0:
                start_time = _start_time

            sub = unescape(sub)
            sub_line += sub
            sub_text = match_line(sub_line, sub_index)
            if sub_text:
                sub_index += 1
                line = formatter(
                    idx=sub_index,
                    start_time=start_time,
                    end_time=end_time,
                    sub_text=sub_text,
                )
                sub_items.append(line)
                start_time = -1.0
                sub_line = ""

        if len(sub_items) == len(script_lines):
            with open(subtitle_file, "w", encoding="utf-8") as file:
                file.write("\n".join(sub_items) + "\n")
            try:
                sbs = subtitles.file_to_subtitles(subtitle_file, encoding="utf-8")
                duration = max([tb for ((ta, tb), txt) in sbs])
                logger.info(
                    f"completed, subtitle file created: {subtitle_file}, duration: {duration}"
                )
            except Exception as e:
                logger.error(f"failed, error: {str(e)}")
                os.remove(subtitle_file)
        else:
            logger.warning(
                f"failed, sub_items len: {len(sub_items)}, script_lines len: {len(script_lines)}"
            )

    except Exception as e:
        logger.error(f"failed, error: {str(e)}")


def get_audio_duration(sub_maker: submaker.SubMaker):
    """
    Ëé∑ÂèñÈü≥È¢ëÊó∂Èïø
    """
    if not sub_maker.offset:
        return 0.0
    return sub_maker.offset[-1][1] / 10000000


if __name__ == "__main__":
    voice_name = "zh-CN-XiaoxiaoMultilingualNeural-V2-Female"
    voice_name = parse_voice_name(voice_name)
    voice_name = is_azure_v2_voice(voice_name)
    print(voice_name)

    voices = get_all_azure_voices()
    print(len(voices))

    async def _do():
        temp_dir = utils.storage_dir("temp")

        voice_names = [
            "zh-CN-XiaoxiaoMultilingualNeural",
            # Â•≥ÊÄß
            "zh-CN-XiaoxiaoNeural",
            "zh-CN-XiaoyiNeural",
            # Áî∑ÊÄß
            "zh-CN-YunyangNeural",
            "zh-CN-YunxiNeural",
        ]
        text = """
        ÈùôÂ§úÊÄùÊòØÂîê‰ª£ËØó‰∫∫ÊùéÁôΩÂàõ‰ΩúÁöÑ‰∏ÄÈ¶ñ‰∫îË®ÄÂè§ËØó„ÄÇËøôÈ¶ñËØóÊèèÁªò‰∫ÜËØó‰∫∫Âú®ÂØÇÈùôÁöÑÂ§úÊôöÔºåÁúãÂà∞Á™óÂâçÁöÑÊòéÊúàÔºå‰∏çÁ¶ÅÊÉ≥Ëµ∑ËøúÊñπÁöÑÂÆ∂‰π°Âíå‰∫≤‰∫∫ÔºåË°®Ëææ‰∫Ü‰ªñÂØπÂÆ∂‰π°Âíå‰∫≤‰∫∫ÁöÑÊ∑±Ê∑±ÊÄùÂøµ‰πãÊÉÖ„ÄÇÂÖ®ËØóÂÜÖÂÆπÊòØÔºö‚ÄúÂ∫äÂâçÊòéÊúàÂÖâÔºåÁñëÊòØÂú∞‰∏äÈúú„ÄÇ‰∏æÂ§¥ÊúõÊòéÊúàÔºå‰ΩéÂ§¥ÊÄùÊïÖ‰π°„ÄÇ‚ÄùÂú®ËøôÁü≠Áü≠ÁöÑÂõõÂè•ËØó‰∏≠ÔºåËØó‰∫∫ÈÄöËøá‚ÄúÊòéÊúà‚ÄùÂíå‚ÄúÊÄùÊïÖ‰π°‚ÄùÁöÑÊÑèË±°ÔºåÂ∑ßÂ¶ôÂú∞Ë°®Ëææ‰∫ÜÁ¶ª‰π°ËÉå‰∫ï‰∫∫ÁöÑÂ≠§Áã¨‰∏éÂìÄÊÑÅ„ÄÇÈ¶ñÂè•‚ÄúÂ∫äÂâçÊòéÊúàÂÖâ‚ÄùËÆæÊôØÁ´ãÊÑèÔºåÈÄöËøáÊòé‰∫ÆÁöÑÊúàÂÖâÂºïÂá∫ËØó‰∫∫ÁöÑÈÅêÊÉ≥Ôºõ‚ÄúÁñëÊòØÂú∞‰∏äÈúú‚ÄùÂ¢ûÊ∑ª‰∫ÜÂ§úÊôöÁöÑÂØíÂÜ∑ÊÑüÔºåÂä†Ê∑±‰∫ÜËØó‰∫∫ÁöÑÂ≠§ÂØÇ‰πãÊÉÖÔºõ‚Äú‰∏æÂ§¥ÊúõÊòéÊúà‚ÄùÂíå‚Äú‰ΩéÂ§¥ÊÄùÊïÖ‰π°‚ÄùÂàôÊòØÊÉÖÊÑüÁöÑÂçáÂçéÔºåÂ±ïÁé∞‰∫ÜËØó‰∫∫ÂÜÖÂøÉÊ∑±Â§ÑÁöÑ‰π°ÊÑÅÂíåÂØπÂÆ∂ÁöÑÊ∏¥Êúõ„ÄÇËøôÈ¶ñËØóÁÆÄÊ¥ÅÊòéÂø´ÔºåÊÉÖÊÑüÁúüÊåöÔºåÊòØ‰∏≠ÂõΩÂè§ÂÖ∏ËØóÊ≠å‰∏≠ÈùûÂ∏∏ËëóÂêçÁöÑ‰∏ÄÈ¶ñÔºå‰πüÊ∑±ÂèóÂêé‰∫∫ÂñúÁà±ÂíåÊé®Â¥á„ÄÇ
            """

        text = """
        What is the meaning of life? This question has puzzled philosophers, scientists, and thinkers of all kinds for centuries. Throughout history, various cultures and individuals have come up with their interpretations and beliefs around the purpose of life. Some say it's to seek happiness and self-fulfillment, while others believe it's about contributing to the welfare of others and making a positive impact in the world. Despite the myriad of perspectives, one thing remains clear: the meaning of life is a deeply personal concept that varies from one person to another. It's an existential inquiry that encourages us to reflect on our values, desires, and the essence of our existence.
        """

        text = """
               È¢ÑËÆ°Êú™Êù•3Â§©Ê∑±Âú≥ÂÜ∑Á©∫Ê∞îÊ¥ªÂä®È¢ëÁπÅÔºåÊú™Êù•‰∏§Â§©ÊåÅÁª≠Èò¥Â§©ÊúâÂ∞èÈõ®ÔºåÂá∫Èó®Â∏¶Â•ΩÈõ®ÂÖ∑Ôºõ
               10-11Êó•ÊåÅÁª≠Èò¥Â§©ÊúâÂ∞èÈõ®ÔºåÊó•Ê∏©Â∑ÆÂ∞èÔºåÊ∞îÊ∏©Âú®13-17‚ÑÉ‰πãÈó¥Ôºå‰ΩìÊÑüÈò¥ÂáâÔºõ
               12Êó•Â§©Ê∞îÁü≠ÊöÇÂ•ΩËΩ¨ÔºåÊó©ÊôöÊ∏ÖÂáâÔºõ
                   """

        text = "[Opening scene: A sunny day in a suburban neighborhood. A young boy named Alex, around 8 years old, is playing in his front yard with his loyal dog, Buddy.]\n\n[Camera zooms in on Alex as he throws a ball for Buddy to fetch. Buddy excitedly runs after it and brings it back to Alex.]\n\nAlex: Good boy, Buddy! You're the best dog ever!\n\n[Buddy barks happily and wags his tail.]\n\n[As Alex and Buddy continue playing, a series of potential dangers loom nearby, such as a stray dog approaching, a ball rolling towards the street, and a suspicious-looking stranger walking by.]\n\nAlex: Uh oh, Buddy, look out!\n\n[Buddy senses the danger and immediately springs into action. He barks loudly at the stray dog, scaring it away. Then, he rushes to retrieve the ball before it reaches the street and gently nudges it back towards Alex. Finally, he stands protectively between Alex and the stranger, growling softly to warn them away.]\n\nAlex: Wow, Buddy, you're like my superhero!\n\n[Just as Alex and Buddy are about to head inside, they hear a loud crash from a nearby construction site. They rush over to investigate and find a pile of rubble blocking the path of a kitten trapped underneath.]\n\nAlex: Oh no, Buddy, we have to help!\n\n[Buddy barks in agreement and together they work to carefully move the rubble aside, allowing the kitten to escape unharmed. The kitten gratefully nuzzles against Buddy, who responds with a friendly lick.]\n\nAlex: We did it, Buddy! We saved the day again!\n\n[As Alex and Buddy walk home together, the sun begins to set, casting a warm glow over the neighborhood.]\n\nAlex: Thanks for always being there to watch over me, Buddy. You're not just my dog, you're my best friend.\n\n[Buddy barks happily and nuzzles against Alex as they disappear into the sunset, ready to face whatever adventures tomorrow may bring.]\n\n[End scene.]"

        text = "Â§ßÂÆ∂Â•ΩÔºåÊàëÊòØ‰πîÂì•Ôºå‰∏Ä‰∏™ÊÉ≥Â∏Æ‰Ω†Êää‰ø°Áî®Âç°ÂÖ®ÈÉ®ËøòÊ∏ÖÁöÑÂÆ∂‰ºôÔºÅ\n‰ªäÂ§©Êàë‰ª¨Ë¶ÅËÅäÁöÑÊòØ‰ø°Áî®Âç°ÁöÑÂèñÁé∞ÂäüËÉΩ„ÄÇ\n‰Ω†ÊòØ‰∏çÊòØ‰πüÊõæÁªèÂõ†‰∏∫‰∏ÄÊó∂ÁöÑËµÑÈáëÁ¥ßÂº†ÔºåËÄåÊãøÁùÄ‰ø°Áî®Âç°Âà∞ATMÊú∫ÂèñÁé∞ÔºüÂ¶ÇÊûúÊòØÔºåÈÇ£‰Ω†ÂæóÂ•ΩÂ•ΩÁúãÁúãËøô‰∏™ËßÜÈ¢ë‰∫Ü„ÄÇ\nÁé∞Âú®ÈÉΩ2024Âπ¥‰∫ÜÔºåÊàë‰ª•‰∏∫Áé∞Âú®‰∏ç‰ºöÂÜçÊúâ‰∫∫Áî®‰ø°Áî®Âç°ÂèñÁé∞ÂäüËÉΩ‰∫Ü„ÄÇÂâçÂá†Â§©‰∏Ä‰∏™Á≤â‰∏ùÂèëÊù•‰∏ÄÂº†ÂõæÁâáÔºåÂèñÁé∞1‰∏á„ÄÇ\n‰ø°Áî®Âç°ÂèñÁé∞Êúâ‰∏â‰∏™ÂºäÁ´Ø„ÄÇ\n‰∏ÄÔºå‰ø°Áî®Âç°ÂèñÁé∞ÂäüËÉΩ‰ª£‰ª∑ÂèØ‰∏çÂ∞è„ÄÇ‰ºöÂÖàÊî∂Âèñ‰∏Ä‰∏™ÂèñÁé∞ÊâãÁª≠Ë¥πÔºåÊØîÂ¶ÇËøô‰∏™Á≤â‰∏ùÔºåÂèñÁé∞1‰∏áÔºåÊåâ2.5%Êî∂ÂèñÊâãÁª≠Ë¥πÔºåÊî∂Âèñ‰∫Ü250ÂÖÉ„ÄÇ\n‰∫åÔºå‰ø°Áî®Âç°Ê≠£Â∏∏Ê∂àË¥πÊúâÊúÄÈïø56Â§©ÁöÑÂÖçÊÅØÊúüÔºå‰ΩÜÂèñÁé∞‰∏ç‰∫´ÂèóÂÖçÊÅØÊúü„ÄÇ‰ªéÂèñÁé∞ÈÇ£‰∏ÄÂ§©ÂºÄÂßãÔºåÊØèÂ§©ÊåâÁÖß‰∏á5Êî∂ÂèñÂà©ÊÅØÔºåËøô‰∏™Á≤â‰∏ùÁî®‰∫Ü11Â§©ÔºåÊî∂Âèñ‰∫Ü55ÂÖÉÂà©ÊÅØ„ÄÇ\n‰∏âÔºåÈ¢ëÁπÅÁöÑÂèñÁé∞Ë°å‰∏∫ÔºåÈì∂Ë°å‰ºöËÆ§‰∏∫‰Ω†ËµÑÈáëÁ¥ßÂº†Ôºå‰ºöË¢´Ê†áËÆ∞‰∏∫È´òÈ£éÈô©Áî®Êà∑ÔºåÂΩ±Âìç‰Ω†ÁöÑÁªºÂêàËØÑÂàÜÂíåÈ¢ùÂ∫¶„ÄÇ\nÈÇ£‰πàÔºåÂ¶ÇÊûú‰Ω†ËµÑÈáëÁ¥ßÂº†‰∫ÜÔºåËØ•ÊÄé‰πàÂäûÂë¢Ôºü\n‰πîÂì•Áªô‰Ω†ÊîØ‰∏ÄÊãõÔºåÁî®Á†¥ÊÄùÊú∫Êë©Êì¶‰ø°Áî®Âç°ÔºåÂè™ÈúÄË¶ÅÂ∞ëÈáèÁöÑÊâãÁª≠Ë¥πÔºåËÄå‰∏îËøòÂèØ‰ª•‰∫´ÂèóÊúÄÈïø56Â§©ÁöÑÂÖçÊÅØÊúü„ÄÇ\nÊúÄÂêéÔºåÂ¶ÇÊûú‰Ω†ÂØπÁé©Âç°ÊÑüÂÖ¥Ë∂£ÔºåÂèØ‰ª•Êâæ‰πîÂì•È¢ÜÂèñ‰∏ÄÊú¨„ÄäÂç°Á•ûÁßòÁ±ç„ÄãÔºåÁî®Âç°ËøáÁ®ã‰∏≠ÈÅáÂà∞‰ªª‰ΩïÁñëÊÉëÔºå‰πüÊ¨¢ËøéÊâæ‰πîÂì•‰∫§ÊµÅ„ÄÇ\nÂà´Âøò‰∫ÜÔºåÂÖ≥Ê≥®‰πîÂì•ÔºåÂõûÂ§çÁî®Âç°ÊäÄÂ∑ßÔºåÂÖçË¥πÈ¢ÜÂèñ„Ää2024Áî®Âç°ÊäÄÂ∑ß„ÄãÔºåËÆ©Êàë‰ª¨‰∏ÄËµ∑Êàê‰∏∫Áî®Âç°È´òÊâãÔºÅ"

        text = """
        2023ÂÖ®Âπ¥‰∏öÁª©ÈÄüËßà
ÂÖ¨Âè∏ÂÖ®Âπ¥Á¥ØËÆ°ÂÆûÁé∞Ëê•‰∏öÊî∂ÂÖ•1476.94‰∫øÂÖÉÔºåÂêåÊØîÂ¢ûÈïø19.01%ÔºåÂΩíÊØçÂáÄÂà©Ê∂¶747.34‰∫øÂÖÉÔºåÂêåÊØîÂ¢ûÈïø19.16%„ÄÇEPSËææÂà∞59.49ÂÖÉ„ÄÇÁ¨¨ÂõõÂ≠£Â∫¶ÂçïÂ≠£ÔºåËê•‰∏öÊî∂ÂÖ•444.25‰∫øÂÖÉÔºåÂêåÊØîÂ¢ûÈïø20.26%ÔºåÁéØÊØîÂ¢ûÈïø31.86%ÔºõÂΩíÊØçÂáÄÂà©Ê∂¶218.58‰∫øÂÖÉÔºåÂêåÊØîÂ¢ûÈïø19.33%ÔºåÁéØÊØîÂ¢ûÈïø29.37%„ÄÇËøô‰∏ÄÈò∂ÊÆµ
ÁöÑ‰∏öÁª©Ë°®Áé∞‰∏ç‰ªÖÁ™ÅÊòæ‰∫ÜÂÖ¨Âè∏ÁöÑÂ¢ûÈïøÂä®ÂäõÂíåÁõàÂà©ËÉΩÂäõÔºå‰πüÂèçÊò†Âá∫ÂÖ¨Âè∏Âú®Á´û‰∫âÊøÄÁÉàÁöÑÂ∏ÇÂú∫ÁéØÂ¢É‰∏≠‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÂèëÂ±ïÂäøÂ§¥„ÄÇ
2023Âπ¥Q4‰∏öÁª©ÈÄüËßà
Á¨¨ÂõõÂ≠£Â∫¶ÔºåËê•‰∏öÊî∂ÂÖ•Ë¥°ÁåÆ‰∏ªË¶ÅÂ¢ûÈïøÁÇπÔºõÈîÄÂîÆË¥πÁî®È´òÂ¢ûËá¥ÁõàÂà©ËÉΩÂäõÊâøÂéãÔºõÁ®éÈáëÂêåÊØî‰∏äÂçá27%ÔºåÊâ∞Âä®ÂáÄÂà©ÁéáË°®Áé∞„ÄÇ
‰∏öÁª©Ëß£ËØª
Âà©Ê∂¶ÊñπÈù¢Ôºå2023ÂÖ®Âπ¥Ë¥µÂ∑ûËåÖÂè∞Ôºå>ÂΩíÊØçÂáÄÂà©Ê∂¶Â¢ûÈÄü‰∏∫19%ÔºåÂÖ∂‰∏≠Ëê•‰∏öÊî∂ÂÖ•Ê≠£Ë¥°ÁåÆ18%ÔºåËê•‰∏öÊàêÊú¨Ê≠£Ë¥°ÁåÆÁôæÂàÜ‰πã‰∏ÄÔºåÁÆ°ÁêÜË¥πÁî®Ê≠£Ë¥°ÁåÆÁôæÂàÜ‰πã‰∏ÄÁÇπÂõõ„ÄÇ(Ê≥®ÔºöÂΩíÊØçÂáÄÂà©Ê∂¶Â¢ûÈÄüÂÄº=Ëê•‰∏öÊî∂ÂÖ•Â¢ûÈÄü+ÂêÑÁßëÁõÆË¥°ÁåÆÔºåÂ±ïÁ§∫Ë¥°ÁåÆ/ÊãñÁ¥ØÁöÑÂâçÂõõÂêçÁßëÁõÆÔºå‰∏îË¶ÅÊ±ÇË¥°ÁåÆÂÄº/ÂáÄÂà©Ê∂¶Â¢ûÈÄü>15%)
"""
        text = "ÈùôÂ§úÊÄùÊòØÂîê‰ª£ËØó‰∫∫ÊùéÁôΩÂàõ‰ΩúÁöÑ‰∏ÄÈ¶ñ‰∫îË®ÄÂè§ËØó„ÄÇËøôÈ¶ñËØóÊèèÁªò‰∫ÜËØó‰∫∫Âú®ÂØÇÈùôÁöÑÂ§úÊôöÔºåÁúãÂà∞Á™óÂâçÁöÑÊòéÊúàÔºå‰∏çÁ¶ÅÊÉ≥Ëµ∑ËøúÊñπÁöÑÂÆ∂‰π°Âíå‰∫≤‰∫∫"

        text = _format_text(text)
        lines = utils.split_string_by_punctuations(text)
        print(lines)

        for voice_name in voice_names:
            voice_file = f"{temp_dir}/tts-{voice_name}.mp3"
            subtitle_file = f"{temp_dir}/tts.mp3.srt"
            sub_maker = azure_tts_v2(
                text=text, voice_name=voice_name, voice_file=voice_file
            )
            create_subtitle(sub_maker=sub_maker, text=text, subtitle_file=subtitle_file)
            audio_duration = get_audio_duration(sub_maker)
            print(f"voice: {voice_name}, audio duration: {audio_duration}s")

    loop = asyncio.get_event_loop_policy().get_event_loop()
    try:
        loop.run_until_complete(_do())
    finally:
        loop.close()
</file>

<file path="requirements.txt">
moviepy==2.1.2
streamlit==1.45.0
edge_tts==6.1.19
fastapi==0.115.6
uvicorn==0.32.1
openai==1.56.1
faster-whisper==1.1.0
loguru==0.7.3
google.generativeai==0.8.3
dashscope==1.20.14
g4f==0.5.2.2
azure-cognitiveservices-speech==1.41.1
redis==5.2.0
python-multipart==0.0.19
pyyaml
requests>=2.31.0
</file>

<file path="README.md">
<div align="center">
<h1 align="center">MoneyPrinterTurbo üí∏</h1>

<p align="center">
  <a href="https://github.com/harry0703/MoneyPrinterTurbo/stargazers"><img src="https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Stargazers"></a>
  <a href="https://github.com/harry0703/MoneyPrinterTurbo/issues"><img src="https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Issues"></a>
  <a href="https://github.com/harry0703/MoneyPrinterTurbo/network/members"><img src="https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Forks"></a>
  <a href="https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE"><img src="https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="License"></a>
</p>
<br>
<h3>ÁÆÄ‰Ωì‰∏≠Êñá | <a href="README-en.md">English</a></h3>
<div align="center">
  <a href="https://trendshift.io/repositories/8731" target="_blank"><img src="https://trendshift.io/api/badge/repositories/8731" alt="harry0703%2FMoneyPrinterTurbo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>
<br>
Âè™ÈúÄÊèê‰æõ‰∏Ä‰∏™ËßÜÈ¢ë <b>‰∏ªÈ¢ò</b> Êàñ <b>ÂÖ≥ÈîÆËØç</b> ÔºåÂ∞±ÂèØ‰ª•ÂÖ®Ëá™Âä®ÁîüÊàêËßÜÈ¢ëÊñáÊ°à„ÄÅËßÜÈ¢ëÁ¥†Êùê„ÄÅËßÜÈ¢ëÂ≠óÂπï„ÄÅËßÜÈ¢ëËÉåÊôØÈü≥‰πêÔºåÁÑ∂ÂêéÂêàÊàê‰∏Ä‰∏™È´òÊ∏ÖÁöÑÁü≠ËßÜÈ¢ë„ÄÇ
<br>

<h4>WebÁïåÈù¢</h4>

![](docs/webui.jpg)

<h4>APIÁïåÈù¢</h4>

![](docs/api.jpg)

</div>

## ÁâπÂà´ÊÑüË∞¢ üôè

Áî±‰∫éËØ•È°πÁõÆÁöÑ **ÈÉ®ÁΩ≤** Âíå **‰ΩøÁî®**ÔºåÂØπ‰∫é‰∏Ä‰∫õÂ∞èÁôΩÁî®Êà∑Êù•ËØ¥ÔºåËøòÊòØ **Êúâ‰∏ÄÂÆöÁöÑÈó®Êßõ**ÔºåÂú®Ê≠§ÁâπÂà´ÊÑüË∞¢
**ÂΩïÂíñÔºàAIÊô∫ËÉΩ Â§öÂ™í‰ΩìÊúçÂä°Âπ≥Âè∞Ôºâ** ÁΩëÁ´ôÂü∫‰∫éËØ•È°πÁõÆÔºåÊèê‰æõÁöÑÂÖçË¥π`AIËßÜÈ¢ëÁîüÊàêÂô®`ÊúçÂä°ÔºåÂèØ‰ª•‰∏çÁî®ÈÉ®ÁΩ≤ÔºåÁõ¥Êé•Âú®Á∫ø‰ΩøÁî®ÔºåÈùûÂ∏∏Êñπ‰æø„ÄÇ

- ‰∏≠ÊñáÁâàÔºöhttps://reccloud.cn
- Ëã±ÊñáÁâàÔºöhttps://reccloud.com

![](docs/reccloud.cn.jpg)

## ÊÑüË∞¢ËµûÂä© üôè

ÊÑüË∞¢‰ΩêÁ≥ñ https://picwish.cn ÂØπËØ•È°πÁõÆÁöÑÊîØÊåÅÂíåËµûÂä©Ôºå‰ΩøÂæóËØ•È°πÁõÆËÉΩÂ§üÊåÅÁª≠ÁöÑÊõ¥Êñ∞ÂíåÁª¥Êä§„ÄÇ

‰ΩêÁ≥ñ‰∏ìÊ≥®‰∫é**ÂõæÂÉèÂ§ÑÁêÜÈ¢ÜÂüü**ÔºåÊèê‰æõ‰∏∞ÂØåÁöÑ**ÂõæÂÉèÂ§ÑÁêÜÂ∑•ÂÖ∑**ÔºåÂ∞ÜÂ§çÊùÇÊìç‰ΩúÊûÅËá¥ÁÆÄÂåñÔºåÁúüÊ≠£ÂÆûÁé∞ËÆ©ÂõæÂÉèÂ§ÑÁêÜÊõ¥ÁÆÄÂçï„ÄÇ

![picwish.jpg](docs/picwish.jpg)

## ÂäüËÉΩÁâπÊÄß üéØ

- [x] ÂÆåÊï¥ÁöÑ **MVCÊû∂ÊûÑ**Ôºå‰ª£Á†Å **ÁªìÊûÑÊ∏ÖÊô∞**ÔºåÊòì‰∫éÁª¥Êä§ÔºåÊîØÊåÅ `API` Âíå `WebÁïåÈù¢`
- [x] ÊîØÊåÅËßÜÈ¢ëÊñáÊ°à **AIËá™Âä®ÁîüÊàê**Ôºå‰πüÂèØ‰ª•**Ëá™ÂÆö‰πâÊñáÊ°à**
- [x] ÊîØÊåÅÂ§öÁßç **È´òÊ∏ÖËßÜÈ¢ë** Â∞∫ÂØ∏
    - [x] Á´ñÂ±è 9:16Ôºå`1080x1920`
    - [x] Ê®™Â±è 16:9Ôºå`1920x1080`
- [x] ÊîØÊåÅ **ÊâπÈáèËßÜÈ¢ëÁîüÊàê**ÔºåÂèØ‰ª•‰∏ÄÊ¨°ÁîüÊàêÂ§ö‰∏™ËßÜÈ¢ëÔºåÁÑ∂ÂêéÈÄâÊã©‰∏Ä‰∏™ÊúÄÊª°ÊÑèÁöÑ
- [x] ÊîØÊåÅ **ËßÜÈ¢ëÁâáÊÆµÊó∂Èïø** ËÆæÁΩÆÔºåÊñπ‰æøË∞ÉËäÇÁ¥†ÊùêÂàáÊç¢È¢ëÁéá
- [x] ÊîØÊåÅ **‰∏≠Êñá** Âíå **Ëã±Êñá** ËßÜÈ¢ëÊñáÊ°à
- [x] ÊîØÊåÅ **Â§öÁßçËØ≠Èü≥** ÂêàÊàêÔºåÂèØ **ÂÆûÊó∂ËØïÂê¨** ÊïàÊûú
- [x] ÊîØÊåÅ **Â≠óÂπïÁîüÊàê**ÔºåÂèØ‰ª•Ë∞ÉÊï¥ `Â≠ó‰Ωì`„ÄÅ`‰ΩçÁΩÆ`„ÄÅ`È¢úËâ≤`„ÄÅ`Â§ßÂ∞è`ÔºåÂêåÊó∂ÊîØÊåÅ`Â≠óÂπïÊèèËæπ`ËÆæÁΩÆ
- [x] ÊîØÊåÅ **ËÉåÊôØÈü≥‰πê**ÔºåÈöèÊú∫ÊàñËÄÖÊåáÂÆöÈü≥‰πêÊñá‰ª∂ÔºåÂèØËÆæÁΩÆ`ËÉåÊôØÈü≥‰πêÈü≥Èáè`
- [x] ËßÜÈ¢ëÁ¥†ÊùêÊù•Ê∫ê **È´òÊ∏Ö**ÔºåËÄå‰∏î **Êó†ÁâàÊùÉ**Ôºå‰πüÂèØ‰ª•‰ΩøÁî®Ëá™Â∑±ÁöÑ **Êú¨Âú∞Á¥†Êùê**
- [x] ÊîØÊåÅ **OpenAI**„ÄÅ**Moonshot**„ÄÅ**Azure**„ÄÅ**gpt4free**„ÄÅ**one-api**„ÄÅ**ÈÄö‰πâÂçÉÈóÆ**„ÄÅ**Google Gemini**„ÄÅ**Ollama**„ÄÅ**DeepSeek**„ÄÅ **ÊñáÂøÉ‰∏ÄË®Ä**, **Pollinations** Á≠âÂ§öÁßçÊ®°ÂûãÊé•ÂÖ•
    - ‰∏≠ÂõΩÁî®Êà∑Âª∫ËÆÆ‰ΩøÁî® **DeepSeek** Êàñ **Moonshot** ‰Ωú‰∏∫Â§ßÊ®°ÂûãÊèê‰æõÂïÜÔºàÂõΩÂÜÖÂèØÁõ¥Êé•ËÆøÈóÆÔºå‰∏çÈúÄË¶ÅVPN„ÄÇÊ≥®ÂÜåÂ∞±ÈÄÅÈ¢ùÂ∫¶ÔºåÂü∫Êú¨Â§üÁî®Ôºâ


### ÂêéÊúüËÆ°Âàí üìÖ

- [ ] GPT-SoVITS ÈÖçÈü≥ÊîØÊåÅ
- [ ] ‰ºòÂåñËØ≠Èü≥ÂêàÊàêÔºåÂà©Áî®Â§ßÊ®°ÂûãÔºå‰ΩøÂÖ∂ÂêàÊàêÁöÑÂ£∞Èü≥ÔºåÊõ¥Âä†Ëá™ÁÑ∂ÔºåÊÉÖÁª™Êõ¥Âä†‰∏∞ÂØå
- [ ] Â¢ûÂä†ËßÜÈ¢ëËΩ¨Âú∫ÊïàÊûúÔºå‰ΩøÂÖ∂ÁúãËµ∑Êù•Êõ¥Âä†ÁöÑÊµÅÁïÖ
- [ ] Â¢ûÂä†Êõ¥Â§öËßÜÈ¢ëÁ¥†ÊùêÊù•Ê∫êÔºå‰ºòÂåñËßÜÈ¢ëÁ¥†ÊùêÂíåÊñáÊ°àÁöÑÂåπÈÖçÂ∫¶
- [ ] Â¢ûÂä†ËßÜÈ¢ëÈïøÂ∫¶ÈÄâÈ°πÔºöÁü≠„ÄÅ‰∏≠„ÄÅÈïø
- [ ] ÊîØÊåÅÊõ¥Â§öÁöÑËØ≠Èü≥ÂêàÊàêÊúçÂä°ÂïÜÔºåÊØîÂ¶Ç OpenAI TTS
- [ ] Ëá™Âä®‰∏ä‰º†Âà∞YouTubeÂπ≥Âè∞

## ËßÜÈ¢ëÊºîÁ§∫ üì∫

### Á´ñÂ±è 9:16

<table>
<thead>
<tr>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji> „ÄäÂ¶Ç‰ΩïÂ¢ûÂä†ÁîüÊ¥ªÁöÑ‰πêË∂£„Äã</th>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji> „ÄäÈáëÈí±ÁöÑ‰ΩúÁî®„Äã<br>Êõ¥ÁúüÂÆûÁöÑÂêàÊàêÂ£∞Èü≥</th>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji> „ÄäÁîüÂëΩÁöÑÊÑè‰πâÊòØ‰ªÄ‰πà„Äã</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6"></video></td>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8"></video></td>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476"></video></td>
</tr>
</tbody>
</table>

### Ê®™Â±è 16:9

<table>
<thead>
<tr>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji>„ÄäÁîüÂëΩÁöÑÊÑè‰πâÊòØ‰ªÄ‰πà„Äã</th>
<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">‚ñ∂Ô∏è</g-emoji>„Ää‰∏∫‰ªÄ‰πàË¶ÅËøêÂä®„Äã</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073"></video></td>
<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87"></video></td>
</tr>
</tbody>
</table>

## ÈÖçÁΩÆË¶ÅÊ±Ç üì¶

- Âª∫ËÆÆÊúÄ‰Ωé CPU **4Ê†∏** Êàñ‰ª•‰∏äÔºåÂÜÖÂ≠ò **4G** Êàñ‰ª•‰∏äÔºåÊòæÂç°ÈùûÂøÖÈ°ª
- Windows 10 Êàñ MacOS 11.0 ‰ª•‰∏äÁ≥ªÁªü


## Âø´ÈÄüÂºÄÂßã üöÄ

### Âú® Google Colab ‰∏≠ËøêË°å
ÂÖçÂéªÊú¨Âú∞ÁéØÂ¢ÉÈÖçÁΩÆÔºåÁÇπÂáªÁõ¥Êé•Âú® Google Colab ‰∏≠Âø´ÈÄü‰ΩìÈ™å MoneyPrinterTurbo

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harry0703/MoneyPrinterTurbo/blob/main/docs/MoneyPrinterTurbo.ipynb)


### Windows‰∏ÄÈîÆÂêØÂä®ÂåÖ

‰∏ãËΩΩ‰∏ÄÈîÆÂêØÂä®ÂåÖÔºåËß£ÂéãÁõ¥Êé•‰ΩøÁî®ÔºàË∑ØÂæÑ‰∏çË¶ÅÊúâ **‰∏≠Êñá**„ÄÅ**ÁâπÊÆäÂ≠óÁ¨¶**„ÄÅ**Á©∫Ê†º**Ôºâ

- ÁôæÂ∫¶ÁΩëÁõòÔºàv1.2.6Ôºâ: https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx ÊèêÂèñÁ†Å: sbqx
- Google Drive (v1.2.6): https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing

‰∏ãËΩΩÂêéÔºåÂª∫ËÆÆÂÖà**ÂèåÂáªÊâßË°å** `update.bat` Êõ¥Êñ∞Âà∞**ÊúÄÊñ∞‰ª£Á†Å**ÔºåÁÑ∂ÂêéÂèåÂáª `start.bat` ÂêØÂä®

ÂêØÂä®ÂêéÔºå‰ºöËá™Âä®ÊâìÂºÄÊµèËßàÂô®ÔºàÂ¶ÇÊûúÊâìÂºÄÊòØÁ©∫ÁôΩÔºåÂª∫ËÆÆÊç¢Êàê **Chrome** ÊàñËÄÖ **Edge** ÊâìÂºÄÔºâ

## ÂÆâË£ÖÈÉ®ÁΩ≤ üì•

### ÂâçÊèêÊù°‰ª∂

- Â∞ΩÈáè‰∏çË¶Å‰ΩøÁî® **‰∏≠ÊñáË∑ØÂæÑ**ÔºåÈÅøÂÖçÂá∫Áé∞‰∏Ä‰∫õÊó†Ê≥ïÈ¢ÑÊñôÁöÑÈóÆÈ¢ò
- ËØ∑Á°Æ‰øù‰Ω†ÁöÑ **ÁΩëÁªú** ÊòØÊ≠£Â∏∏ÁöÑÔºåVPNÈúÄË¶ÅÊâìÂºÄ`ÂÖ®Â±ÄÊµÅÈáè`Ê®°Âºè

#### ‚ë† ÂÖãÈöÜ‰ª£Á†Å

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
```

#### ‚ë° ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÔºàÂèØÈÄâÔºåÂª∫ËÆÆÂêØÂä®Âêé‰πüÂèØ‰ª•Âú® WebUI ÈáåÈù¢ÈÖçÁΩÆÔºâ

- Â∞Ü `config.example.toml` Êñá‰ª∂Â§çÂà∂‰∏Ä‰ªΩÔºåÂëΩÂêç‰∏∫ `config.toml`
- ÊåâÁÖß `config.toml` Êñá‰ª∂‰∏≠ÁöÑËØ¥ÊòéÔºåÈÖçÁΩÆÂ•Ω `pexels_api_keys` Âíå `llm_provider`ÔºåÂπ∂Ê†πÊçÆ llm_provider ÂØπÂ∫îÁöÑÊúçÂä°ÂïÜÔºåÈÖçÁΩÆÁõ∏ÂÖ≥ÁöÑ
  API Key

### DockerÈÉ®ÁΩ≤ üê≥

#### ‚ë† ÂêØÂä®Docker

Â¶ÇÊûúÊú™ÂÆâË£Ö DockerÔºåËØ∑ÂÖàÂÆâË£Ö https://www.docker.com/products/docker-desktop/

Â¶ÇÊûúÊòØWindowsÁ≥ªÁªüÔºåËØ∑ÂèÇËÄÉÂæÆËΩØÁöÑÊñáÊ°£Ôºö

1. https://learn.microsoft.com/zh-cn/windows/wsl/install
2. https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers

```shell
cd MoneyPrinterTurbo
docker-compose up
```

> Ê≥®ÊÑèÔºöÊúÄÊñ∞ÁâàÁöÑdockerÂÆâË£ÖÊó∂‰ºöËá™Âä®‰ª•Êèí‰ª∂ÁöÑÂΩ¢ÂºèÂÆâË£Ödocker composeÔºåÂêØÂä®ÂëΩ‰ª§Ë∞ÉÊï¥‰∏∫docker compose up

#### ‚ë° ËÆøÈóÆWebÁïåÈù¢

ÊâìÂºÄÊµèËßàÂô®ÔºåËÆøÈóÆ http://0.0.0.0:8501

#### ‚ë¢ ËÆøÈóÆAPIÊñáÊ°£

ÊâìÂºÄÊµèËßàÂô®ÔºåËÆøÈóÆ http://0.0.0.0:8080/docs ÊàñËÄÖ http://0.0.0.0:8080/redoc

### ÊâãÂä®ÈÉ®ÁΩ≤ üì¶

> ËßÜÈ¢ëÊïôÁ®ã

- ÂÆåÊï¥ÁöÑ‰ΩøÁî®ÊºîÁ§∫Ôºöhttps://v.douyin.com/iFhnwsKY/
- Â¶Ç‰ΩïÂú®Windows‰∏äÈÉ®ÁΩ≤Ôºöhttps://v.douyin.com/iFyjoW3M

#### ‚ë† ÂàõÂª∫ËôöÊãüÁéØÂ¢É

Âª∫ËÆÆ‰ΩøÁî® [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) ÂàõÂª∫ python ËôöÊãüÁéØÂ¢É

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
```

#### ‚ë° ÂÆâË£ÖÂ•Ω ImageMagick

- Windows:
    - ‰∏ãËΩΩ https://imagemagick.org/script/download.php ÈÄâÊã©WindowsÁâàÊú¨ÔºåÂàáËÆ∞‰∏ÄÂÆöË¶ÅÈÄâÊã© **ÈùôÊÄÅÂ∫ì** ÁâàÊú¨ÔºåÊØîÂ¶Ç
      ImageMagick-7.1.1-32-Q16-x64-**static**.exe
    - ÂÆâË£Ö‰∏ãËΩΩÂ•ΩÁöÑ ImageMagickÔºå**Ê≥®ÊÑè‰∏çË¶Å‰øÆÊîπÂÆâË£ÖË∑ØÂæÑ**
    - ‰øÆÊîπ `ÈÖçÁΩÆÊñá‰ª∂ config.toml` ‰∏≠ÁöÑ `imagemagick_path` ‰∏∫‰Ω†ÁöÑ **ÂÆûÈôÖÂÆâË£ÖË∑ØÂæÑ**

- MacOS:
  ```shell
  brew install imagemagick
  ````
- Ubuntu
  ```shell
  sudo apt-get install imagemagick
  ```
- CentOS
  ```shell
  sudo yum install ImageMagick
  ```

#### ‚ë¢ ÂêØÂä®WebÁïåÈù¢ üåê

Ê≥®ÊÑèÈúÄË¶ÅÂà∞ MoneyPrinterTurbo È°πÁõÆ `Ê†πÁõÆÂΩï` ‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§

###### Windows

```bat
webui.bat
```

###### MacOS or Linux

```shell
sh webui.sh
```

ÂêØÂä®ÂêéÔºå‰ºöËá™Âä®ÊâìÂºÄÊµèËßàÂô®ÔºàÂ¶ÇÊûúÊâìÂºÄÊòØÁ©∫ÁôΩÔºåÂª∫ËÆÆÊç¢Êàê **Chrome** ÊàñËÄÖ **Edge** ÊâìÂºÄÔºâ

#### ‚ë£ ÂêØÂä®APIÊúçÂä° üöÄ

```shell
python main.py
```

ÂêØÂä®ÂêéÔºåÂèØ‰ª•Êü•Áúã `APIÊñáÊ°£` http://127.0.0.1:8080/docs ÊàñËÄÖ http://127.0.0.1:8080/redoc Áõ¥Êé•Âú®Á∫øË∞ÉËØïÊé•Âè£ÔºåÂø´ÈÄü‰ΩìÈ™å„ÄÇ

## ËØ≠Èü≥ÂêàÊàê üó£

ÊâÄÊúâÊîØÊåÅÁöÑÂ£∞Èü≥ÂàóË°®ÔºåÂèØ‰ª•Êü•ÁúãÔºö[Â£∞Èü≥ÂàóË°®](./docs/voice-list.txt)

2024-04-16 v1.1.2 Êñ∞Â¢û‰∫Ü9ÁßçAzureÁöÑËØ≠Èü≥ÂêàÊàêÂ£∞Èü≥ÔºåÈúÄË¶ÅÈÖçÁΩÆAPI KEYÔºåËØ•Â£∞Èü≥ÂêàÊàêÁöÑÊõ¥Âä†ÁúüÂÆû„ÄÇ

## Â≠óÂπïÁîüÊàê üìú

ÂΩìÂâçÊîØÊåÅ2ÁßçÂ≠óÂπïÁîüÊàêÊñπÂºèÔºö

- **edge**: ÁîüÊàê`ÈÄüÂ∫¶Âø´`ÔºåÊÄßËÉΩÊõ¥Â•ΩÔºåÂØπÁîµËÑëÈÖçÁΩÆÊ≤°ÊúâË¶ÅÊ±ÇÔºå‰ΩÜÊòØË¥®ÈáèÂèØËÉΩ‰∏çÁ®≥ÂÆö
- **whisper**: ÁîüÊàê`ÈÄüÂ∫¶ÊÖ¢`ÔºåÊÄßËÉΩËæÉÂ∑ÆÔºåÂØπÁîµËÑëÈÖçÁΩÆÊúâ‰∏ÄÂÆöË¶ÅÊ±ÇÔºå‰ΩÜÊòØ`Ë¥®ÈáèÊõ¥ÂèØÈù†`„ÄÇ

ÂèØ‰ª•‰øÆÊîπ `config.toml` ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑ `subtitle_provider` ËøõË°åÂàáÊç¢

Âª∫ËÆÆ‰ΩøÁî® `edge` Ê®°ÂºèÔºåÂ¶ÇÊûúÁîüÊàêÁöÑÂ≠óÂπïË¥®Èáè‰∏çÂ•ΩÔºåÂÜçÂàáÊç¢Âà∞ `whisper` Ê®°Âºè

> Ê≥®ÊÑèÔºö

1. whisper Ê®°Âºè‰∏ãÈúÄË¶ÅÂà∞ HuggingFace ‰∏ãËΩΩ‰∏Ä‰∏™Ê®°ÂûãÊñá‰ª∂ÔºåÂ§ßÁ∫¶ 3GB Â∑¶Âè≥ÔºåËØ∑Á°Æ‰øùÁΩëÁªúÈÄöÁïÖ
2. Â¶ÇÊûúÁïôÁ©∫ÔºåË°®Á§∫‰∏çÁîüÊàêÂ≠óÂπï„ÄÇ

> Áî±‰∫éÂõΩÂÜÖÊó†Ê≥ïËÆøÈóÆ HuggingFaceÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÊñπÊ≥ï‰∏ãËΩΩ `whisper-large-v3` ÁöÑÊ®°ÂûãÊñá‰ª∂

‰∏ãËΩΩÂú∞ÂùÄÔºö

- ÁôæÂ∫¶ÁΩëÁõò: https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9
- Â§∏ÂÖãÁΩëÁõòÔºöhttps://pan.quark.cn/s/3ee3d991d64b

Ê®°Âûã‰∏ãËΩΩÂêéËß£ÂéãÔºåÊï¥‰∏™ÁõÆÂΩïÊîæÂà∞ `.\MoneyPrinterTurbo\models` ÈáåÈù¢Ôºå
ÊúÄÁªàÁöÑÊñá‰ª∂Ë∑ØÂæÑÂ∫îËØ•ÊòØËøôÊ†∑: `.\MoneyPrinterTurbo\models\whisper-large-v3`

```
MoneyPrinterTurbo  
  ‚îú‚îÄmodels
  ‚îÇ   ‚îî‚îÄwhisper-large-v3
  ‚îÇ          config.json
  ‚îÇ          model.bin
  ‚îÇ          preprocessor_config.json
  ‚îÇ          tokenizer.json
  ‚îÇ          vocabulary.json
```

## ËÉåÊôØÈü≥‰πê üéµ

Áî®‰∫éËßÜÈ¢ëÁöÑËÉåÊôØÈü≥‰πêÔºå‰Ωç‰∫éÈ°πÁõÆÁöÑ `resource/songs` ÁõÆÂΩï‰∏ã„ÄÇ
> ÂΩìÂâçÈ°πÁõÆÈáåÈù¢Êîæ‰∫Ü‰∏Ä‰∫õÈªòËÆ§ÁöÑÈü≥‰πêÔºåÊù•Ëá™‰∫é YouTube ËßÜÈ¢ëÔºåÂ¶ÇÊúâ‰æµÊùÉÔºåËØ∑Âà†Èô§„ÄÇ

## Â≠óÂπïÂ≠ó‰Ωì üÖ∞

Áî®‰∫éËßÜÈ¢ëÂ≠óÂπïÁöÑÊ∏≤ÊüìÔºå‰Ωç‰∫éÈ°πÁõÆÁöÑ `resource/fonts` ÁõÆÂΩï‰∏ãÔºå‰Ω†‰πüÂèØ‰ª•ÊîæËøõÂéªËá™Â∑±ÁöÑÂ≠ó‰Ωì„ÄÇ

## Â∏∏ËßÅÈóÆÈ¢ò ü§î

### ‚ùìRuntimeError: No ffmpeg exe could be found

ÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåffmpeg ‰ºöË¢´Ëá™Âä®‰∏ãËΩΩÔºåÂπ∂‰∏î‰ºöË¢´Ëá™Âä®Ê£ÄÊµãÂà∞„ÄÇ
‰ΩÜÊòØÂ¶ÇÊûú‰Ω†ÁöÑÁéØÂ¢ÉÊúâÈóÆÈ¢òÔºåÊó†Ê≥ïËá™Âä®‰∏ãËΩΩÔºåÂèØËÉΩ‰ºöÈÅáÂà∞Â¶Ç‰∏ãÈîôËØØÔºö

```
RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
```

Ê≠§Êó∂‰Ω†ÂèØ‰ª•‰ªé https://www.gyan.dev/ffmpeg/builds/ ‰∏ãËΩΩffmpegÔºåËß£ÂéãÂêéÔºåËÆæÁΩÆ `ffmpeg_path` ‰∏∫‰Ω†ÁöÑÂÆûÈôÖÂÆâË£ÖË∑ØÂæÑÂç≥ÂèØ„ÄÇ

```toml
[app]
# ËØ∑Ê†πÊçÆ‰Ω†ÁöÑÂÆûÈôÖË∑ØÂæÑËÆæÁΩÆÔºåÊ≥®ÊÑè Windows Ë∑ØÂæÑÂàÜÈöîÁ¨¶‰∏∫ \\
ffmpeg_path = "C:\\Users\\harry\\Downloads\\ffmpeg.exe"
```

### ‚ùìImageMagickÁöÑÂÆâÂÖ®Á≠ñÁï•ÈòªÊ≠¢‰∫Ü‰∏é‰∏¥Êó∂Êñá‰ª∂@/tmp/tmpur5hyyto.txtÁõ∏ÂÖ≥ÁöÑÊìç‰Ωú

ÂèØ‰ª•Âú®ImageMagickÁöÑÈÖçÁΩÆÊñá‰ª∂policy.xml‰∏≠ÊâæÂà∞Ëøô‰∫õÁ≠ñÁï•„ÄÇ
Ëøô‰∏™Êñá‰ª∂ÈÄöÂ∏∏‰Ωç‰∫é /etc/ImageMagick-`X`/ Êàñ ImageMagick ÂÆâË£ÖÁõÆÂΩïÁöÑÁ±ª‰ºº‰ΩçÁΩÆ„ÄÇ
‰øÆÊîπÂåÖÂê´`pattern="@"`ÁöÑÊù°ÁõÆÔºåÂ∞Ü`rights="none"`Êõ¥Êîπ‰∏∫`rights="read|write"`‰ª•ÂÖÅËÆ∏ÂØπÊñá‰ª∂ÁöÑËØªÂÜôÊìç‰Ωú„ÄÇ

### ‚ùìOSError: [Errno 24] Too many open files

Ëøô‰∏™ÈóÆÈ¢òÊòØÁî±‰∫éÁ≥ªÁªüÊâìÂºÄÊñá‰ª∂Êï∞ÈôêÂà∂ÂØºËá¥ÁöÑÔºåÂèØ‰ª•ÈÄöËøá‰øÆÊîπÁ≥ªÁªüÁöÑÊñá‰ª∂ÊâìÂºÄÊï∞ÈôêÂà∂Êù•Ëß£ÂÜ≥„ÄÇ

Êü•ÁúãÂΩìÂâçÈôêÂà∂

```shell
ulimit -n
```

Â¶ÇÊûúËøá‰ΩéÔºåÂèØ‰ª•Ë∞ÉÈ´ò‰∏Ä‰∫õÔºåÊØîÂ¶Ç

```shell
ulimit -n 10240
```

### ‚ùìWhisper Ê®°Âûã‰∏ãËΩΩÂ§±Ë¥•ÔºåÂá∫Áé∞Â¶Ç‰∏ãÈîôËØØ

LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and
outgoing trafic has been disabled.
To enablerepo look-ups and downloads online, pass 'local files only=False' as input.

ÊàñËÄÖ

An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub:
An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the
specified revision on the local disk. Please check your internet connection and try again.
Trying to load the model directly from the local cache, if it exists.

Ëß£ÂÜ≥ÊñπÊ≥ïÔºö[ÁÇπÂáªÊü•ÁúãÂ¶Ç‰Ωï‰ªéÁΩëÁõòÊâãÂä®‰∏ãËΩΩÊ®°Âûã](#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-)

## ÂèçÈ¶àÂª∫ËÆÆ üì¢

- ÂèØ‰ª•Êèê‰∫§ [issue](https://github.com/harry0703/MoneyPrinterTurbo/issues)
  ÊàñËÄÖ [pull request](https://github.com/harry0703/MoneyPrinterTurbo/pulls)„ÄÇ

## ËÆ∏ÂèØËØÅ üìù

ÁÇπÂáªÊü•Áúã [`LICENSE`](LICENSE) Êñá‰ª∂

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&type=Date)](https://star-history.com/#harry0703/MoneyPrinterTurbo&Date)
</file>

<file path="app/services/video.py">
import glob
import itertools
import os
import random
import gc
import shutil
from typing import List
from loguru import logger
from moviepy import (
    AudioFileClip,
    ColorClip,
    CompositeAudioClip,
    CompositeVideoClip,
    ImageClip,
    TextClip,
    VideoFileClip,
    afx,
    concatenate_videoclips,
)
from moviepy.video.tools.subtitles import SubtitlesClip
from PIL import ImageFont

from app.models import const
from app.models.schema import (
    MaterialInfo,
    VideoAspect,
    VideoConcatMode,
    VideoParams,
    VideoTransitionMode,
)
from app.services.utils import video_effects
from app.utils import utils


class SubClippedVideoClip:
    def __init__(
        self,
        file_path,
        start_time=None,
        end_time=None,
        width=None,
        height=None,
        duration=None,
    ):
        self.file_path = file_path
        self.start_time = start_time
        self.end_time = end_time
        self.width = width
        self.height = height
        if duration is None:
            self.duration = end_time - start_time
        else:
            self.duration = duration

    def __str__(self):
        return f"SubClippedVideoClip(file_path={self.file_path}, start_time={self.start_time}, end_time={self.end_time}, duration={self.duration}, width={self.width}, height={self.height})"


audio_codec = "aac"
video_codec = "libx264"
fps = 30


def close_clip(clip):
    if clip is None:
        return

    try:
        # close main resources
        if hasattr(clip, "reader") and clip.reader is not None:
            clip.reader.close()

        # close audio resources
        if hasattr(clip, "audio") and clip.audio is not None:
            if hasattr(clip.audio, "reader") and clip.audio.reader is not None:
                clip.audio.reader.close()
            del clip.audio

        # close mask resources
        if hasattr(clip, "mask") and clip.mask is not None:
            if hasattr(clip.mask, "reader") and clip.mask.reader is not None:
                clip.mask.reader.close()
            del clip.mask

        # handle child clips in composite clips
        if hasattr(clip, "clips") and clip.clips:
            for child_clip in clip.clips:
                if child_clip is not clip:  # avoid possible circular references
                    close_clip(child_clip)

        # clear clip list
        if hasattr(clip, "clips"):
            clip.clips = []

    except Exception as e:
        logger.error(f"failed to close clip: {str(e)}")

    del clip
    gc.collect()


def delete_files(files: List[str] | str):
    if isinstance(files, str):
        files = [files]

    for file in files:
        try:
            os.remove(file)
        except:
            pass


def get_bgm_file(bgm_type: str = "random", bgm_file: str = ""):
    if not bgm_type:
        return ""

    if bgm_file and os.path.exists(bgm_file):
        return bgm_file

    if bgm_type == "random":
        suffix = "*.mp3"
        song_dir = utils.song_dir()
        files = glob.glob(os.path.join(song_dir, suffix))
        return random.choice(files)

    return ""


def combine_videos(
    combined_video_path: str,
    video_paths: List[str],
    audio_file: str,
    video_aspect: VideoAspect = VideoAspect.portrait,
    video_concat_mode: VideoConcatMode = VideoConcatMode.random,
    video_transition_mode: VideoTransitionMode = None,
    max_clip_duration: int = 5,
    threads: int = 2,
) -> str:
    audio_clip = AudioFileClip(audio_file)
    audio_duration = audio_clip.duration
    logger.info(f"audio duration: {audio_duration} seconds")
    # Required duration of each clip
    req_dur = audio_duration / len(video_paths)
    req_dur = max_clip_duration
    logger.info(f"maximum clip duration: {req_dur} seconds")
    output_dir = os.path.dirname(combined_video_path)

    aspect = VideoAspect(video_aspect)
    video_width, video_height = aspect.to_resolution()

    processed_clips = []
    subclipped_items = []
    video_duration = 0
    for video_path in video_paths:
        clip = VideoFileClip(video_path)
        clip_duration = clip.duration
        clip_w, clip_h = clip.size
        close_clip(clip)

        start_time = 0

        while start_time < clip_duration:
            end_time = min(start_time + max_clip_duration, clip_duration)
            if clip_duration - start_time >= max_clip_duration:
                subclipped_items.append(
                    SubClippedVideoClip(
                        file_path=video_path,
                        start_time=start_time,
                        end_time=end_time,
                        width=clip_w,
                        height=clip_h,
                    )
                )
            start_time = end_time
            if video_concat_mode.value == VideoConcatMode.sequential.value:
                break

    # random subclipped_items order
    if video_concat_mode.value == VideoConcatMode.random.value:
        random.shuffle(subclipped_items)

    logger.debug(f"total subclipped items: {len(subclipped_items)}")

    # Add downloaded clips over and over until the duration of the audio (max_duration) has been reached
    for i, subclipped_item in enumerate(subclipped_items):
        if video_duration > audio_duration:
            break

        logger.debug(
            f"processing clip {i+1}: {subclipped_item.width}x{subclipped_item.height}, current duration: {video_duration:.2f}s, remaining: {audio_duration - video_duration:.2f}s"
        )

        try:
            clip = VideoFileClip(subclipped_item.file_path).subclipped(
                subclipped_item.start_time, subclipped_item.end_time
            )
            clip_duration = clip.duration
            # Not all videos are same size, so we need to resize them
            clip_w, clip_h = clip.size
            if clip_w != video_width or clip_h != video_height:
                clip_ratio = clip.w / clip.h
                video_ratio = video_width / video_height
                logger.debug(
                    f"resizing clip, source: {clip_w}x{clip_h}, ratio: {clip_ratio:.2f}, target: {video_width}x{video_height}, ratio: {video_ratio:.2f}"
                )

                if clip_ratio == video_ratio:
                    clip = clip.resized(new_size=(video_width, video_height))
                else:
                    if clip_ratio > video_ratio:
                        scale_factor = video_width / clip_w
                    else:
                        scale_factor = video_height / clip_h

                    new_width = int(clip_w * scale_factor)
                    new_height = int(clip_h * scale_factor)

                    background = ColorClip(
                        size=(video_width, video_height), color=(0, 0, 0)
                    ).with_duration(clip_duration)
                    clip_resized = clip.resized(
                        new_size=(new_width, new_height)
                    ).with_position("center")
                    clip = CompositeVideoClip([background, clip_resized])

            shuffle_side = random.choice(["left", "right", "top", "bottom"])
            if video_transition_mode.value == VideoTransitionMode.none.value:
                clip = clip
            elif video_transition_mode.value == VideoTransitionMode.fade_in.value:
                clip = video_effects.fadein_transition(clip, 1)
            elif video_transition_mode.value == VideoTransitionMode.fade_out.value:
                clip = video_effects.fadeout_transition(clip, 1)
            elif video_transition_mode.value == VideoTransitionMode.slide_in.value:
                clip = video_effects.slidein_transition(clip, 1, shuffle_side)
            elif video_transition_mode.value == VideoTransitionMode.slide_out.value:
                clip = video_effects.slideout_transition(clip, 1, shuffle_side)
            elif video_transition_mode.value == VideoTransitionMode.shuffle.value:
                transition_funcs = [
                    lambda c: video_effects.fadein_transition(c, 1),
                    lambda c: video_effects.fadeout_transition(c, 1),
                    lambda c: video_effects.slidein_transition(c, 1, shuffle_side),
                    lambda c: video_effects.slideout_transition(c, 1, shuffle_side),
                ]
                shuffle_transition = random.choice(transition_funcs)
                clip = shuffle_transition(clip)

            if clip.duration > max_clip_duration:
                clip = clip.subclipped(0, max_clip_duration)

            # wirte clip to temp file
            clip_file = f"{output_dir}/temp-clip-{i+1}.mp4"
            clip.write_videofile(clip_file, logger=None, fps=fps, codec=video_codec)

            close_clip(clip)

            processed_clips.append(
                SubClippedVideoClip(
                    file_path=clip_file,
                    duration=clip.duration,
                    width=clip_w,
                    height=clip_h,
                )
            )
            video_duration += clip.duration

        except Exception as e:
            logger.error(f"failed to process clip: {str(e)}")

    # loop processed clips until the video duration matches or exceeds the audio duration.
    if video_duration < audio_duration:
        logger.warning(
            f"video duration ({video_duration:.2f}s) is shorter than audio duration ({audio_duration:.2f}s), looping clips to match audio length."
        )
        base_clips = processed_clips.copy()
        for clip in itertools.cycle(base_clips):
            if video_duration >= audio_duration:
                break
            processed_clips.append(clip)
            video_duration += clip.duration
        logger.info(
            f"video duration: {video_duration:.2f}s, audio duration: {audio_duration:.2f}s, looped {len(processed_clips)-len(base_clips)} clips"
        )

    # merge video clips progressively, avoid loading all videos at once to avoid memory overflow
    logger.info("starting clip merging process")
    if not processed_clips:
        logger.warning("no clips available for merging")
        return combined_video_path

    # if there is only one clip, use it directly
    if len(processed_clips) == 1:
        logger.info("using single clip directly")
        shutil.copy(processed_clips[0].file_path, combined_video_path)
        delete_files(processed_clips)
        logger.info("video combining completed")
        return combined_video_path

    # create initial video file as base
    base_clip_path = processed_clips[0].file_path
    temp_merged_video = f"{output_dir}/temp-merged-video.mp4"
    temp_merged_next = f"{output_dir}/temp-merged-next.mp4"

    # copy first clip as initial merged video
    shutil.copy(base_clip_path, temp_merged_video)

    # merge remaining video clips one by one
    for i, clip in enumerate(processed_clips[1:], 1):
        logger.info(
            f"merging clip {i}/{len(processed_clips)-1}, duration: {clip.duration:.2f}s"
        )

        try:
            # load current base video and next clip to merge
            base_clip = VideoFileClip(temp_merged_video)
            next_clip = VideoFileClip(clip.file_path)

            # merge these two clips
            merged_clip = concatenate_videoclips([base_clip, next_clip])

            # save merged result to temp file
            merged_clip.write_videofile(
                filename=temp_merged_next,
                threads=threads,
                logger=None,
                temp_audiofile_path=output_dir,
                audio_codec=audio_codec,
                fps=fps,
            )
            close_clip(base_clip)
            close_clip(next_clip)
            close_clip(merged_clip)

            # replace base file with new merged file
            delete_files(temp_merged_video)
            os.rename(temp_merged_next, temp_merged_video)

        except Exception as e:
            logger.error(f"failed to merge clip: {str(e)}")
            continue

    # after merging, rename final result to target file name
    os.rename(temp_merged_video, combined_video_path)

    # clean temp files
    clip_files = [clip.file_path for clip in processed_clips]
    delete_files(clip_files)

    logger.info("video combining completed")
    return combined_video_path


def wrap_text(text, max_width, font="Arial", fontsize=60):
    # Create ImageFont
    font = ImageFont.truetype(font, fontsize)

    def get_text_size(inner_text):
        inner_text = inner_text.strip()
        left, top, right, bottom = font.getbbox(inner_text)
        return right - left, bottom - top

    width, height = get_text_size(text)
    if width <= max_width:
        return text, height

    processed = True

    _wrapped_lines_ = []
    words = text.split(" ")
    _txt_ = ""
    for word in words:
        _before = _txt_
        _txt_ += f"{word} "
        _width, _height = get_text_size(_txt_)
        if _width <= max_width:
            continue
        else:
            if _txt_.strip() == word.strip():
                processed = False
                break
            _wrapped_lines_.append(_before)
            _txt_ = f"{word} "
    _wrapped_lines_.append(_txt_)
    if processed:
        _wrapped_lines_ = [line.strip() for line in _wrapped_lines_]
        result = "\n".join(_wrapped_lines_).strip()
        height = len(_wrapped_lines_) * height
        return result, height

    _wrapped_lines_ = []
    chars = list(text)
    _txt_ = ""
    for word in chars:
        _txt_ += word
        _width, _height = get_text_size(_txt_)
        if _width <= max_width:
            continue
        else:
            _wrapped_lines_.append(_txt_)
            _txt_ = ""
    _wrapped_lines_.append(_txt_)
    result = "\n".join(_wrapped_lines_).strip()
    height = len(_wrapped_lines_) * height
    return result, height


def generate_video(
    video_path: str,
    audio_path: str,
    subtitle_path: str,
    output_file: str,
    params: VideoParams,
):
    aspect = VideoAspect(params.video_aspect)
    video_width, video_height = aspect.to_resolution()

    logger.info(f"generating video: {video_width} x {video_height}")
    logger.info(f"  ‚ë† video: {video_path}")
    logger.info(f"  ‚ë° audio: {audio_path}")
    logger.info(f"  ‚ë¢ subtitle: {subtitle_path}")
    logger.info(f"  ‚ë£ output: {output_file}")

    # https://github.com/harry0703/MoneyPrinterTurbo/issues/217
    # PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'final-1.mp4.tempTEMP_MPY_wvf_snd.mp3'
    # write into the same directory as the output file
    output_dir = os.path.dirname(output_file)

    font_path = ""
    if params.subtitle_enabled:
        if not params.font_name:
            params.font_name = "STHeitiMedium.ttc"
        font_path = os.path.join(utils.font_dir(), params.font_name)
        if os.name == "nt":
            font_path = font_path.replace("\\", "/")

        logger.info(f"  ‚ë§ font: {font_path}")

    def create_text_clip(subtitle_item):
        params.font_size = int(params.font_size)
        params.stroke_width = int(params.stroke_width)
        phrase = subtitle_item[1]
        max_width = video_width * 0.9
        wrapped_txt, txt_height = wrap_text(
            phrase, max_width=max_width, font=font_path, fontsize=params.font_size
        )
        interline = int(params.font_size * 0.25)
        size = (
            int(max_width),
            int(
                txt_height
                + params.font_size * 0.25
                + (interline * (wrapped_txt.count("\n") + 1))
            ),
        )

        _clip = TextClip(
            text=wrapped_txt,
            font=font_path,
            font_size=params.font_size,
            color=params.text_fore_color,
            bg_color=params.text_background_color,
            stroke_color=params.stroke_color,
            stroke_width=params.stroke_width,
            # interline=interline,
            # size=size,
        )
        duration = subtitle_item[0][1] - subtitle_item[0][0]
        _clip = _clip.with_start(subtitle_item[0][0])
        _clip = _clip.with_end(subtitle_item[0][1])
        _clip = _clip.with_duration(duration)
        if params.subtitle_position == "bottom":
            _clip = _clip.with_position(("center", video_height * 0.95 - _clip.h))
        elif params.subtitle_position == "top":
            _clip = _clip.with_position(("center", video_height * 0.05))
        elif params.subtitle_position == "custom":
            # Ensure the subtitle is fully within the screen bounds
            margin = 10  # Additional margin, in pixels
            max_y = video_height - _clip.h - margin
            min_y = margin
            custom_y = (video_height - _clip.h) * (params.custom_position / 100)
            custom_y = max(
                min_y, min(custom_y, max_y)
            )  # Constrain the y value within the valid range
            _clip = _clip.with_position(("center", custom_y))
        else:  # center
            _clip = _clip.with_position(("center", "center"))
        return _clip

    video_clip = VideoFileClip(video_path).without_audio()
    audio_clip = AudioFileClip(audio_path).with_effects(
        [afx.MultiplyVolume(params.voice_volume)]
    )

    def make_textclip(text):
        return TextClip(
            text=text,
            font=font_path,
            font_size=params.font_size,
        )

    if subtitle_path and os.path.exists(subtitle_path):
        sub = SubtitlesClip(
            subtitles=subtitle_path, encoding="utf-8", make_textclip=make_textclip
        )
        text_clips = []
        for item in sub.subtitles:
            clip = create_text_clip(subtitle_item=item)
            text_clips.append(clip)
        video_clip = CompositeVideoClip([video_clip, *text_clips])

    bgm_file = get_bgm_file(bgm_type=params.bgm_type, bgm_file=params.bgm_file)
    if bgm_file:
        try:
            bgm_clip = AudioFileClip(bgm_file).with_effects(
                [
                    afx.MultiplyVolume(params.bgm_volume),
                    afx.AudioFadeOut(3),
                    afx.AudioLoop(duration=video_clip.duration),
                ]
            )
            audio_clip = CompositeAudioClip([audio_clip, bgm_clip])
        except Exception as e:
            logger.error(f"failed to add bgm: {str(e)}")

    video_clip = video_clip.with_audio(audio_clip)
    video_clip.write_videofile(
        output_file,
        audio_codec=audio_codec,
        temp_audiofile_path=output_dir,
        threads=params.n_threads or 2,
        logger=None,
        fps=fps,
    )
    video_clip.close()
    del video_clip


def preprocess_video(materials: List[MaterialInfo], clip_duration=4):
    for material in materials:
        if not material.url:
            continue

        ext = utils.parse_extension(material.url)
        try:
            clip = VideoFileClip(material.url)
        except Exception:
            clip = ImageClip(material.url)

        width = clip.size[0]
        height = clip.size[1]
        if width < 480 or height < 480:
            logger.warning(
                f"low resolution material: {width}x{height}, minimum 480x480 required"
            )
            continue

        if ext in const.FILE_TYPE_IMAGES:
            logger.info(f"processing image: {material.url}")
            # Create an image clip and set its duration to 3 seconds
            clip = (
                ImageClip(material.url)
                .with_duration(clip_duration)
                .with_position("center")
            )
            # Apply a zoom effect using the resize method.
            # A lambda function is used to make the zoom effect dynamic over time.
            # The zoom effect starts from the original size and gradually scales up to 120%.
            # t represents the current time, and clip.duration is the total duration of the clip (3 seconds).
            # Note: 1 represents 100% size, so 1.2 represents 120% size.
            zoom_clip = clip.resized(
                lambda t: 1 + (clip_duration * 0.03) * (t / clip.duration)
            )

            # Optionally, create a composite video clip containing the zoomed clip.
            # This is useful when you want to add other elements to the video.
            final_clip = CompositeVideoClip([zoom_clip])

            # Output the video to a file.
            video_file = f"{material.url}.mp4"
            final_clip.write_videofile(video_file, fps=30, logger=None)
            close_clip(clip)
            material.url = video_file
            logger.success(f"image processed: {video_file}")
    return materials
</file>

<file path="webui/Main.py">
import os
import platform
import sys
from uuid import uuid4

import streamlit as st
from loguru import logger

# Add the root directory of the project to the system path to allow importing modules from the project
root_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
if root_dir not in sys.path:
    sys.path.append(root_dir)
    print("******** sys.path ********")
    print(sys.path)
    print("")

from app.config import config
from app.models.schema import (
    MaterialInfo,
    VideoAspect,
    VideoConcatMode,
    VideoParams,
    VideoTransitionMode,
)
from app.services import llm, voice
from app.services import task as tm
from app.utils import utils

st.set_page_config(
    page_title="MoneyPrinterTurbo",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "Report a bug": "https://github.com/harry0703/MoneyPrinterTurbo/issues",
        "About": "# MoneyPrinterTurbo\nSimply provide a topic or keyword for a video, and it will "
        "automatically generate the video copy, video materials, video subtitles, "
        "and video background music before synthesizing a high-definition short "
        "video.\n\nhttps://github.com/harry0703/MoneyPrinterTurbo",
    },
)


streamlit_style = """
<style>
h1 {
    padding-top: 0 !important;
}
</style>
"""
st.markdown(streamlit_style, unsafe_allow_html=True)

# ÂÆö‰πâËµÑÊ∫êÁõÆÂΩï
font_dir = os.path.join(root_dir, "resource", "fonts")
song_dir = os.path.join(root_dir, "resource", "songs")
i18n_dir = os.path.join(root_dir, "webui", "i18n")
config_file = os.path.join(root_dir, "webui", ".streamlit", "webui.toml")
system_locale = utils.get_system_locale()


if "video_subject" not in st.session_state:
    st.session_state["video_subject"] = ""
if "video_script" not in st.session_state:
    st.session_state["video_script"] = ""
if "video_terms" not in st.session_state:
    st.session_state["video_terms"] = ""
if "ui_language" not in st.session_state:
    st.session_state["ui_language"] = config.ui.get("language", system_locale)

# Âä†ËΩΩËØ≠Ë®ÄÊñá‰ª∂
locales = utils.load_locales(i18n_dir)

# ÂàõÂª∫‰∏Ä‰∏™È°∂ÈÉ®Ê†èÔºåÂåÖÂê´Ê†áÈ¢òÂíåËØ≠Ë®ÄÈÄâÊã©
title_col, lang_col = st.columns([3, 1])

with title_col:
    st.title(f"MoneyPrinterTurbo v{config.project_version}")

with lang_col:
    display_languages = []
    selected_index = 0
    for i, code in enumerate(locales.keys()):
        display_languages.append(f"{code} - {locales[code].get('Language')}")
        if code == st.session_state.get("ui_language", ""):
            selected_index = i

    selected_language = st.selectbox(
        "Language / ËØ≠Ë®Ä",
        options=display_languages,
        index=selected_index,
        key="top_language_selector",
        label_visibility="collapsed",
    )
    if selected_language:
        code = selected_language.split(" - ")[0].strip()
        st.session_state["ui_language"] = code
        config.ui["language"] = code

support_locales = [
    "zh-CN",
    "zh-HK",
    "zh-TW",
    "de-DE",
    "en-US",
    "fr-FR",
    "vi-VN",
    "th-TH",
]


def get_all_fonts():
    fonts = []
    for root, dirs, files in os.walk(font_dir):
        for file in files:
            if file.endswith(".ttf") or file.endswith(".ttc"):
                fonts.append(file)
    fonts.sort()
    return fonts


def get_all_songs():
    songs = []
    for root, dirs, files in os.walk(song_dir):
        for file in files:
            if file.endswith(".mp3"):
                songs.append(file)
    return songs


def open_task_folder(task_id):
    try:
        sys = platform.system()
        path = os.path.join(root_dir, "storage", "tasks", task_id)
        if os.path.exists(path):
            if sys == "Windows":
                os.system(f"start {path}")
            if sys == "Darwin":
                os.system(f"open {path}")
    except Exception as e:
        logger.error(e)


def scroll_to_bottom():
    js = """
    <script>
        console.log("scroll_to_bottom");
        function scroll(dummy_var_to_force_repeat_execution){
            var sections = parent.document.querySelectorAll('section.main');
            console.log(sections);
            for(let index = 0; index<sections.length; index++) {
                sections[index].scrollTop = sections[index].scrollHeight;
            }
        }
        scroll(1);
    </script>
    """
    st.components.v1.html(js, height=0, width=0)


def init_log():
    logger.remove()
    _lvl = "DEBUG"

    def format_record(record):
        # Ëé∑ÂèñÊó•ÂøóËÆ∞ÂΩï‰∏≠ÁöÑÊñá‰ª∂ÂÖ®Ë∑ØÂæÑ
        file_path = record["file"].path
        # Â∞ÜÁªùÂØπË∑ØÂæÑËΩ¨Êç¢‰∏∫Áõ∏ÂØπ‰∫éÈ°πÁõÆÊ†πÁõÆÂΩïÁöÑË∑ØÂæÑ
        relative_path = os.path.relpath(file_path, root_dir)
        # Êõ¥Êñ∞ËÆ∞ÂΩï‰∏≠ÁöÑÊñá‰ª∂Ë∑ØÂæÑ
        record["file"].path = f"./{relative_path}"
        # ËøîÂõû‰øÆÊîπÂêéÁöÑÊ†ºÂºèÂ≠óÁ¨¶‰∏≤
        # ÊÇ®ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅË∞ÉÊï¥ËøôÈáåÁöÑÊ†ºÂºè
        record["message"] = record["message"].replace(root_dir, ".")

        _format = (
            "<green>{time:%Y-%m-%d %H:%M:%S}</> | "
            + "<level>{level}</> | "
            + '"{file.path}:{line}":<blue> {function}</> '
            + "- <level>{message}</>"
            + "\n"
        )
        return _format

    logger.add(
        sys.stdout,
        level=_lvl,
        format=format_record,
        colorize=True,
    )


init_log()

locales = utils.load_locales(i18n_dir)


def tr(key):
    loc = locales.get(st.session_state["ui_language"], {})
    return loc.get("Translation", {}).get(key, key)


# ÂàõÂª∫Âü∫Á°ÄËÆæÁΩÆÊäòÂè†Ê°Ü
if not config.app.get("hide_config", False):
    with st.expander(tr("Basic Settings"), expanded=False):
        config_panels = st.columns(3)
        left_config_panel = config_panels[0]
        middle_config_panel = config_panels[1]
        right_config_panel = config_panels[2]

        # Â∑¶‰æßÈù¢Êùø - Êó•ÂøóËÆæÁΩÆ
        with left_config_panel:
            # ÊòØÂê¶ÈöêËóèÈÖçÁΩÆÈù¢Êùø
            hide_config = st.checkbox(
                tr("Hide Basic Settings"), value=config.app.get("hide_config", False)
            )
            config.app["hide_config"] = hide_config

            # ÊòØÂê¶Á¶ÅÁî®Êó•ÂøóÊòæÁ§∫
            hide_log = st.checkbox(
                tr("Hide Log"), value=config.ui.get("hide_log", False)
            )
            config.ui["hide_log"] = hide_log

        # ‰∏≠Èó¥Èù¢Êùø - LLM ËÆæÁΩÆ

        with middle_config_panel:
            st.write(tr("LLM Settings"))
            llm_providers = [
                "OpenAI",
                "Moonshot",
                "Azure",
                "Qwen",
                "DeepSeek",
                "Gemini",
                "Ollama",
                "G4f",
                "OneAPI",
                "Cloudflare",
                "ERNIE",
                "Pollinations",
            ]
            saved_llm_provider = config.app.get("llm_provider", "OpenAI").lower()
            saved_llm_provider_index = 0
            for i, provider in enumerate(llm_providers):
                if provider.lower() == saved_llm_provider:
                    saved_llm_provider_index = i
                    break

            llm_provider = st.selectbox(
                tr("LLM Provider"),
                options=llm_providers,
                index=saved_llm_provider_index,
            )
            llm_helper = st.container()
            llm_provider = llm_provider.lower()
            config.app["llm_provider"] = llm_provider

            llm_api_key = config.app.get(f"{llm_provider}_api_key", "")
            llm_secret_key = config.app.get(
                f"{llm_provider}_secret_key", ""
            )  # only for baidu ernie
            llm_base_url = config.app.get(f"{llm_provider}_base_url", "")
            llm_model_name = config.app.get(f"{llm_provider}_model_name", "")
            llm_account_id = config.app.get(f"{llm_provider}_account_id", "")

            tips = ""
            if llm_provider == "ollama":
                if not llm_model_name:
                    llm_model_name = "qwen:7b"
                if not llm_base_url:
                    llm_base_url = "http://localhost:11434/v1"

                with llm_helper:
                    tips = """
                            ##### OllamaÈÖçÁΩÆËØ¥Êòé
                            - **API Key**: Èöè‰æøÂ°´ÂÜôÔºåÊØîÂ¶Ç 123
                            - **Base Url**: ‰∏ÄËà¨‰∏∫ http://localhost:11434/v1
                                - Â¶ÇÊûú `MoneyPrinterTurbo` Âíå `Ollama` **‰∏çÂú®Âêå‰∏ÄÂè∞Êú∫Âô®‰∏ä**ÔºåÈúÄË¶ÅÂ°´ÂÜô `Ollama` Êú∫Âô®ÁöÑIPÂú∞ÂùÄ
                                - Â¶ÇÊûú `MoneyPrinterTurbo` ÊòØ `Docker` ÈÉ®ÁΩ≤ÔºåÂª∫ËÆÆÂ°´ÂÜô `http://host.docker.internal:11434/v1`
                            - **Model Name**: ‰ΩøÁî® `ollama list` Êü•ÁúãÔºåÊØîÂ¶Ç `qwen:7b`
                            """

            if llm_provider == "openai":
                if not llm_model_name:
                    llm_model_name = "gpt-3.5-turbo"
                with llm_helper:
                    tips = """
                            ##### OpenAI ÈÖçÁΩÆËØ¥Êòé
                            > ÈúÄË¶ÅVPNÂºÄÂêØÂÖ®Â±ÄÊµÅÈáèÊ®°Âºè
                            - **API Key**: [ÁÇπÂáªÂà∞ÂÆòÁΩëÁî≥ËØ∑](https://platform.openai.com/api-keys)
                            - **Base Url**: ÂèØ‰ª•ÁïôÁ©∫
                            - **Model Name**: Â°´ÂÜô**ÊúâÊùÉÈôê**ÁöÑÊ®°ÂûãÔºå[ÁÇπÂáªÊü•ÁúãÊ®°ÂûãÂàóË°®](https://platform.openai.com/settings/organization/limits)
                            """

            if llm_provider == "moonshot":
                if not llm_model_name:
                    llm_model_name = "moonshot-v1-8k"
                with llm_helper:
                    tips = """
                            ##### Moonshot ÈÖçÁΩÆËØ¥Êòé
                            - **API Key**: [ÁÇπÂáªÂà∞ÂÆòÁΩëÁî≥ËØ∑](https://platform.moonshot.cn/console/api-keys)
                            - **Base Url**: Âõ∫ÂÆö‰∏∫ https://api.moonshot.cn/v1
                            - **Model Name**: ÊØîÂ¶Ç moonshot-v1-8kÔºå[ÁÇπÂáªÊü•ÁúãÊ®°ÂûãÂàóË°®](https://platform.moonshot.cn/docs/intro#%E6%A8%A1%E5%9E%8B%E5%88%97%E8%A1%A8)
                            """
            if llm_provider == "oneapi":
                if not llm_model_name:
                    llm_model_name = (
                        "claude-3-5-sonnet-20240620"  # ÈªòËÆ§Ê®°ÂûãÔºåÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅË∞ÉÊï¥
                    )
                with llm_helper:
                    tips = """
                        ##### OneAPI ÈÖçÁΩÆËØ¥Êòé
                        - **API Key**: Â°´ÂÜôÊÇ®ÁöÑ OneAPI ÂØÜÈí•
                        - **Base Url**: Â°´ÂÜô OneAPI ÁöÑÂü∫Á°Ä URL
                        - **Model Name**: Â°´ÂÜôÊÇ®Ë¶Å‰ΩøÁî®ÁöÑÊ®°ÂûãÂêçÁß∞Ôºå‰æãÂ¶Ç claude-3-5-sonnet-20240620
                        """

            if llm_provider == "qwen":
                if not llm_model_name:
                    llm_model_name = "qwen-max"
                with llm_helper:
                    tips = """
                            ##### ÈÄö‰πâÂçÉÈóÆQwen ÈÖçÁΩÆËØ¥Êòé
                            - **API Key**: [ÁÇπÂáªÂà∞ÂÆòÁΩëÁî≥ËØ∑](https://dashscope.console.aliyun.com/apiKey)
                            - **Base Url**: ÁïôÁ©∫
                            - **Model Name**: ÊØîÂ¶Ç qwen-maxÔºå[ÁÇπÂáªÊü•ÁúãÊ®°ÂûãÂàóË°®](https://help.aliyun.com/zh/dashscope/developer-reference/model-introduction#3ef6d0bcf91wy)
                            """

            if llm_provider == "g4f":
                if not llm_model_name:
                    llm_model_name = "gpt-3.5-turbo"
                with llm_helper:
                    tips = """
                            ##### gpt4free ÈÖçÁΩÆËØ¥Êòé
                            > [GitHubÂºÄÊ∫êÈ°πÁõÆ](https://github.com/xtekky/gpt4free)ÔºåÂèØ‰ª•ÂÖçË¥π‰ΩøÁî®GPTÊ®°ÂûãÔºå‰ΩÜÊòØ**Á®≥ÂÆöÊÄßËæÉÂ∑Æ**
                            - **API Key**: Èöè‰æøÂ°´ÂÜôÔºåÊØîÂ¶Ç 123
                            - **Base Url**: ÁïôÁ©∫
                            - **Model Name**: ÊØîÂ¶Ç gpt-3.5-turboÔºå[ÁÇπÂáªÊü•ÁúãÊ®°ÂûãÂàóË°®](https://github.com/xtekky/gpt4free/blob/main/g4f/models.py#L308)
                            """
            if llm_provider == "azure":
                with llm_helper:
                    tips = """
                            ##### Azure ÈÖçÁΩÆËØ¥Êòé
                            > [ÁÇπÂáªÊü•ÁúãÂ¶Ç‰ΩïÈÉ®ÁΩ≤Ê®°Âûã](https://learn.microsoft.com/zh-cn/azure/ai-services/openai/how-to/create-resource)
                            - **API Key**: [ÁÇπÂáªÂà∞AzureÂêéÂè∞ÂàõÂª∫](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/OpenAI)
                            - **Base Url**: ÁïôÁ©∫
                            - **Model Name**: Â°´ÂÜô‰Ω†ÂÆûÈôÖÁöÑÈÉ®ÁΩ≤Âêç
                            """

            if llm_provider == "gemini":
                if not llm_model_name:
                    llm_model_name = "gemini-1.0-pro"

                with llm_helper:
                    tips = """
                            ##### Gemini ÈÖçÁΩÆËØ¥Êòé
                            > ÈúÄË¶ÅVPNÂºÄÂêØÂÖ®Â±ÄÊµÅÈáèÊ®°Âºè
                            - **API Key**: [ÁÇπÂáªÂà∞ÂÆòÁΩëÁî≥ËØ∑](https://ai.google.dev/)
                            - **Base Url**: ÁïôÁ©∫
                            - **Model Name**: ÊØîÂ¶Ç gemini-1.0-pro
                            """

            if llm_provider == "deepseek":
                if not llm_model_name:
                    llm_model_name = "deepseek-chat"
                if not llm_base_url:
                    llm_base_url = "https://api.deepseek.com"
                with llm_helper:
                    tips = """
                            ##### DeepSeek ÈÖçÁΩÆËØ¥Êòé
                            - **API Key**: [ÁÇπÂáªÂà∞ÂÆòÁΩëÁî≥ËØ∑](https://platform.deepseek.com/api_keys)
                            - **Base Url**: Âõ∫ÂÆö‰∏∫ https://api.deepseek.com
                            - **Model Name**: Âõ∫ÂÆö‰∏∫ deepseek-chat
                            """

            if llm_provider == "ernie":
                with llm_helper:
                    tips = """
                            ##### ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®Ä ÈÖçÁΩÆËØ¥Êòé
                            - **API Key**: [ÁÇπÂáªÂà∞ÂÆòÁΩëÁî≥ËØ∑](https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application)
                            - **Secret Key**: [ÁÇπÂáªÂà∞ÂÆòÁΩëÁî≥ËØ∑](https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application)
                            - **Base Url**: Â°´ÂÜô **ËØ∑Ê±ÇÂú∞ÂùÄ** [ÁÇπÂáªÊü•ÁúãÊñáÊ°£](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/jlil56u11#%E8%AF%B7%E6%B1%82%E8%AF%B4%E6%98%8E)
                            """

            if llm_provider == "pollinations":
                if not llm_model_name:
                    llm_model_name = "default"
                with llm_helper:
                    tips = """
                            ##### Pollinations AI Configuration
                            - **API Key**: Optional - Leave empty for public access
                            - **Base Url**: Default is https://text.pollinations.ai/openai
                            - **Model Name**: Use 'openai-fast' or specify a model name
                            """

            if tips and config.ui["language"] == "zh":
                st.warning(
                    "‰∏≠ÂõΩÁî®Êà∑Âª∫ËÆÆ‰ΩøÁî® **DeepSeek** Êàñ **Moonshot** ‰Ωú‰∏∫Â§ßÊ®°ÂûãÊèê‰æõÂïÜ\n- ÂõΩÂÜÖÂèØÁõ¥Êé•ËÆøÈóÆÔºå‰∏çÈúÄË¶ÅVPN \n- Ê≥®ÂÜåÂ∞±ÈÄÅÈ¢ùÂ∫¶ÔºåÂü∫Êú¨Â§üÁî®"
                )
                st.info(tips)

            st_llm_api_key = st.text_input(
                tr("API Key"), value=llm_api_key, type="password"
            )
            st_llm_base_url = st.text_input(tr("Base Url"), value=llm_base_url)
            st_llm_model_name = ""
            if llm_provider != "ernie":
                st_llm_model_name = st.text_input(
                    tr("Model Name"),
                    value=llm_model_name,
                    key=f"{llm_provider}_model_name_input",
                )
                if st_llm_model_name:
                    config.app[f"{llm_provider}_model_name"] = st_llm_model_name
            else:
                st_llm_model_name = None

            if st_llm_api_key:
                config.app[f"{llm_provider}_api_key"] = st_llm_api_key
            if st_llm_base_url:
                config.app[f"{llm_provider}_base_url"] = st_llm_base_url
            if st_llm_model_name:
                config.app[f"{llm_provider}_model_name"] = st_llm_model_name
            if llm_provider == "ernie":
                st_llm_secret_key = st.text_input(
                    tr("Secret Key"), value=llm_secret_key, type="password"
                )
                config.app[f"{llm_provider}_secret_key"] = st_llm_secret_key

            if llm_provider == "cloudflare":
                st_llm_account_id = st.text_input(
                    tr("Account ID"), value=llm_account_id
                )
                if st_llm_account_id:
                    config.app[f"{llm_provider}_account_id"] = st_llm_account_id

        # Âè≥‰æßÈù¢Êùø - API ÂØÜÈí•ËÆæÁΩÆ
        with right_config_panel:

            def get_keys_from_config(cfg_key):
                api_keys = config.app.get(cfg_key, [])
                if isinstance(api_keys, str):
                    api_keys = [api_keys]
                api_key = ", ".join(api_keys)
                return api_key

            def save_keys_to_config(cfg_key, value):
                value = value.replace(" ", "")
                if value:
                    config.app[cfg_key] = value.split(",")

            st.write(tr("Video Source Settings"))

            pexels_api_key = get_keys_from_config("pexels_api_keys")
            pexels_api_key = st.text_input(
                tr("Pexels API Key"), value=pexels_api_key, type="password"
            )
            save_keys_to_config("pexels_api_keys", pexels_api_key)

            pixabay_api_key = get_keys_from_config("pixabay_api_keys")
            pixabay_api_key = st.text_input(
                tr("Pixabay API Key"), value=pixabay_api_key, type="password"
            )
            save_keys_to_config("pixabay_api_keys", pixabay_api_key)

llm_provider = config.app.get("llm_provider", "").lower()
panel = st.columns(3)
left_panel = panel[0]
middle_panel = panel[1]
right_panel = panel[2]

params = VideoParams(video_subject="")
uploaded_files = []

with left_panel:
    with st.container(border=True):
        st.write(tr("Video Script Settings"))
        params.video_subject = st.text_input(
            tr("Video Subject"),
            value=st.session_state["video_subject"],
            key="video_subject_input",
        ).strip()

        video_languages = [
            (tr("Auto Detect"), ""),
        ]
        for code in support_locales:
            video_languages.append((code, code))

        selected_index = st.selectbox(
            tr("Script Language"),
            index=0,
            options=range(
                len(video_languages)
            ),  # Use the index as the internal option value
            format_func=lambda x: video_languages[x][
                0
            ],  # The label is displayed to the user
        )
        params.video_language = video_languages[selected_index][1]

        if st.button(
            tr("Generate Video Script and Keywords"), key="auto_generate_script"
        ):
            with st.spinner(tr("Generating Video Script and Keywords")):
                script = llm.generate_script(
                    video_subject=params.video_subject, language=params.video_language
                )
                terms = llm.generate_terms(params.video_subject, script)
                if "Error: " in script:
                    st.error(tr(script))
                elif "Error: " in terms:
                    st.error(tr(terms))
                else:
                    st.session_state["video_script"] = script
                    st.session_state["video_terms"] = ", ".join(terms)
        params.video_script = st.text_area(
            tr("Video Script"), value=st.session_state["video_script"], height=280
        )
        if st.button(tr("Generate Video Keywords"), key="auto_generate_terms"):
            if not params.video_script:
                st.error(tr("Please Enter the Video Subject"))
                st.stop()

            with st.spinner(tr("Generating Video Keywords")):
                terms = llm.generate_terms(params.video_subject, params.video_script)
                if "Error: " in terms:
                    st.error(tr(terms))
                else:
                    st.session_state["video_terms"] = ", ".join(terms)

        params.video_terms = st.text_area(
            tr("Video Keywords"), value=st.session_state["video_terms"]
        )

with middle_panel:
    with st.container(border=True):
        st.write(tr("Video Settings"))
        video_concat_modes = [
            (tr("Sequential"), "sequential"),
            (tr("Random"), "random"),
        ]
        video_sources = [
            (tr("Pexels"), "pexels"),
            (tr("Pixabay"), "pixabay"),
            (tr("Local file"), "local"),
            (tr("TikTok"), "douyin"),
            (tr("Bilibili"), "bilibili"),
            (tr("Xiaohongshu"), "xiaohongshu"),
        ]

        saved_video_source_name = config.app.get("video_source", "pexels")
        saved_video_source_index = [v[1] for v in video_sources].index(
            saved_video_source_name
        )

        selected_index = st.selectbox(
            tr("Video Source"),
            options=range(len(video_sources)),
            format_func=lambda x: video_sources[x][0],
            index=saved_video_source_index,
        )
        params.video_source = video_sources[selected_index][1]
        config.app["video_source"] = params.video_source

        if params.video_source == "local":
            uploaded_files = st.file_uploader(
                "Upload Local Files",
                type=["mp4", "mov", "avi", "flv", "mkv", "jpg", "jpeg", "png"],
                accept_multiple_files=True,
            )

        selected_index = st.selectbox(
            tr("Video Concat Mode"),
            index=1,
            options=range(
                len(video_concat_modes)
            ),  # Use the index as the internal option value
            format_func=lambda x: video_concat_modes[x][
                0
            ],  # The label is displayed to the user
        )
        params.video_concat_mode = VideoConcatMode(
            video_concat_modes[selected_index][1]
        )

        # ËßÜÈ¢ëËΩ¨Âú∫Ê®°Âºè
        video_transition_modes = [
            (tr("None"), VideoTransitionMode.none.value),
            (tr("Shuffle"), VideoTransitionMode.shuffle.value),
            (tr("FadeIn"), VideoTransitionMode.fade_in.value),
            (tr("FadeOut"), VideoTransitionMode.fade_out.value),
            (tr("SlideIn"), VideoTransitionMode.slide_in.value),
            (tr("SlideOut"), VideoTransitionMode.slide_out.value),
        ]
        selected_index = st.selectbox(
            tr("Video Transition Mode"),
            options=range(len(video_transition_modes)),
            format_func=lambda x: video_transition_modes[x][0],
            index=0,
        )
        params.video_transition_mode = VideoTransitionMode(
            video_transition_modes[selected_index][1]
        )

        video_aspect_ratios = [
            (tr("Portrait"), VideoAspect.portrait.value),
            (tr("Landscape"), VideoAspect.landscape.value),
        ]
        selected_index = st.selectbox(
            tr("Video Ratio"),
            options=range(
                len(video_aspect_ratios)
            ),  # Use the index as the internal option value
            format_func=lambda x: video_aspect_ratios[x][
                0
            ],  # The label is displayed to the user
        )
        params.video_aspect = VideoAspect(video_aspect_ratios[selected_index][1])

        params.video_clip_duration = st.selectbox(
            tr("Clip Duration"), options=[2, 3, 4, 5, 6, 7, 8, 9, 10], index=1
        )
        params.video_count = st.selectbox(
            tr("Number of Videos Generated Simultaneously"),
            options=[1, 2, 3, 4, 5],
            index=0,
        )
    with st.container(border=True):
        st.write(tr("Audio Settings"))

        # Ê∑ªÂä†TTSÊúçÂä°Âô®ÈÄâÊã©‰∏ãÊãâÊ°Ü
        tts_servers = [
            ("azure-tts-v1", "Azure TTS V1"),
            ("azure-tts-v2", "Azure TTS V2"),
            ("siliconflow", "SiliconFlow TTS"),
        ]

        # Ëé∑Âèñ‰øùÂ≠òÁöÑTTSÊúçÂä°Âô®ÔºåÈªòËÆ§‰∏∫v1
        saved_tts_server = config.ui.get("tts_server", "azure-tts-v1")
        saved_tts_server_index = 0
        for i, (server_value, _) in enumerate(tts_servers):
            if server_value == saved_tts_server:
                saved_tts_server_index = i
                break

        selected_tts_server_index = st.selectbox(
            tr("TTS Servers"),
            options=range(len(tts_servers)),
            format_func=lambda x: tts_servers[x][1],
            index=saved_tts_server_index,
        )

        selected_tts_server = tts_servers[selected_tts_server_index][0]
        config.ui["tts_server"] = selected_tts_server

        # Ê†πÊçÆÈÄâÊã©ÁöÑTTSÊúçÂä°Âô®Ëé∑ÂèñÂ£∞Èü≥ÂàóË°®
        filtered_voices = []

        if selected_tts_server == "siliconflow":
            # Ëé∑ÂèñÁ°ÖÂü∫ÊµÅÂä®ÁöÑÂ£∞Èü≥ÂàóË°®
            filtered_voices = voice.get_siliconflow_voices()
        else:
            # Ëé∑ÂèñAzureÁöÑÂ£∞Èü≥ÂàóË°®
            all_voices = voice.get_all_azure_voices(filter_locals=None)

            # Ê†πÊçÆÈÄâÊã©ÁöÑTTSÊúçÂä°Âô®Á≠õÈÄâÂ£∞Èü≥
            for v in all_voices:
                if selected_tts_server == "azure-tts-v2":
                    # V2ÁâàÊú¨ÁöÑÂ£∞Èü≥ÂêçÁß∞‰∏≠ÂåÖÂê´"v2"
                    if "V2" in v:
                        filtered_voices.append(v)
                else:
                    # V1ÁâàÊú¨ÁöÑÂ£∞Èü≥ÂêçÁß∞‰∏≠‰∏çÂåÖÂê´"v2"
                    if "V2" not in v:
                        filtered_voices.append(v)

        friendly_names = {
            v: v.replace("Female", tr("Female"))
            .replace("Male", tr("Male"))
            .replace("Neural", "")
            for v in filtered_voices
        }

        saved_voice_name = config.ui.get("voice_name", "")
        saved_voice_name_index = 0

        # Ê£ÄÊü•‰øùÂ≠òÁöÑÂ£∞Èü≥ÊòØÂê¶Âú®ÂΩìÂâçÁ≠õÈÄâÁöÑÂ£∞Èü≥ÂàóË°®‰∏≠
        if saved_voice_name in friendly_names:
            saved_voice_name_index = list(friendly_names.keys()).index(saved_voice_name)
        else:
            # Â¶ÇÊûú‰∏çÂú®ÔºåÂàôÊ†πÊçÆÂΩìÂâçUIËØ≠Ë®ÄÈÄâÊã©‰∏Ä‰∏™ÈªòËÆ§Â£∞Èü≥
            for i, v in enumerate(filtered_voices):
                if v.lower().startswith(st.session_state["ui_language"].lower()):
                    saved_voice_name_index = i
                    break

        # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞ÂåπÈÖçÁöÑÂ£∞Èü≥Ôºå‰ΩøÁî®Á¨¨‰∏Ä‰∏™Â£∞Èü≥
        if saved_voice_name_index >= len(friendly_names) and friendly_names:
            saved_voice_name_index = 0

        # Á°Æ‰øùÊúâÂ£∞Èü≥ÂèØÈÄâ
        if friendly_names:
            selected_friendly_name = st.selectbox(
                tr("Speech Synthesis"),
                options=list(friendly_names.values()),
                index=(
                    min(saved_voice_name_index, len(friendly_names) - 1)
                    if friendly_names
                    else 0
                ),
            )

            voice_name = list(friendly_names.keys())[
                list(friendly_names.values()).index(selected_friendly_name)
            ]
            params.voice_name = voice_name
            config.ui["voice_name"] = voice_name
        else:
            # Â¶ÇÊûúÊ≤°ÊúâÂ£∞Èü≥ÂèØÈÄâÔºåÊòæÁ§∫ÊèêÁ§∫‰ø°ÊÅØ
            st.warning(
                tr(
                    "No voices available for the selected TTS server. Please select another server."
                )
            )
            params.voice_name = ""
            config.ui["voice_name"] = ""

        # Âè™ÊúâÂú®ÊúâÂ£∞Èü≥ÂèØÈÄâÊó∂ÊâçÊòæÁ§∫ËØïÂê¨ÊåâÈíÆ
        if friendly_names and st.button(tr("Play Voice")):
            play_content = params.video_subject
            if not play_content:
                play_content = params.video_script
            if not play_content:
                play_content = tr("Voice Example")
            with st.spinner(tr("Synthesizing Voice")):
                temp_dir = utils.storage_dir("temp", create=True)
                audio_file = os.path.join(temp_dir, f"tmp-voice-{str(uuid4())}.mp3")
                sub_maker = voice.tts(
                    text=play_content,
                    voice_name=voice_name,
                    voice_rate=params.voice_rate,
                    voice_file=audio_file,
                    voice_volume=params.voice_volume,
                )
                # if the voice file generation failed, try again with a default content.
                if not sub_maker:
                    play_content = "This is a example voice. if you hear this, the voice synthesis failed with the original content."
                    sub_maker = voice.tts(
                        text=play_content,
                        voice_name=voice_name,
                        voice_rate=params.voice_rate,
                        voice_file=audio_file,
                        voice_volume=params.voice_volume,
                    )

                if sub_maker and os.path.exists(audio_file):
                    st.audio(audio_file, format="audio/mp3")
                    if os.path.exists(audio_file):
                        os.remove(audio_file)

        # ÂΩìÈÄâÊã©V2ÁâàÊú¨ÊàñËÄÖÂ£∞Èü≥ÊòØV2Â£∞Èü≥Êó∂ÔºåÊòæÁ§∫ÊúçÂä°Âå∫ÂüüÂíåAPI keyËæìÂÖ•Ê°Ü
        if selected_tts_server == "azure-tts-v2" or (
            voice_name and voice.is_azure_v2_voice(voice_name)
        ):
            saved_azure_speech_region = config.azure.get("speech_region", "")
            saved_azure_speech_key = config.azure.get("speech_key", "")
            azure_speech_region = st.text_input(
                tr("Speech Region"),
                value=saved_azure_speech_region,
                key="azure_speech_region_input",
            )
            azure_speech_key = st.text_input(
                tr("Speech Key"),
                value=saved_azure_speech_key,
                type="password",
                key="azure_speech_key_input",
            )
            config.azure["speech_region"] = azure_speech_region
            config.azure["speech_key"] = azure_speech_key

        # ÂΩìÈÄâÊã©Á°ÖÂü∫ÊµÅÂä®Êó∂ÔºåÊòæÁ§∫API keyËæìÂÖ•Ê°ÜÂíåËØ¥Êòé‰ø°ÊÅØ
        if selected_tts_server == "siliconflow" or (
            voice_name and voice.is_siliconflow_voice(voice_name)
        ):
            saved_siliconflow_api_key = config.siliconflow.get("api_key", "")

            siliconflow_api_key = st.text_input(
                tr("SiliconFlow API Key"),
                value=saved_siliconflow_api_key,
                type="password",
                key="siliconflow_api_key_input",
            )

            # ÊòæÁ§∫Á°ÖÂü∫ÊµÅÂä®ÁöÑËØ¥Êòé‰ø°ÊÅØ
            st.info(
                tr("SiliconFlow TTS Settings")
                + ":\n"
                + "- "
                + tr("Speed: Range [0.25, 4.0], default is 1.0")
                + "\n"
                + "- "
                + tr("Volume: Uses Speech Volume setting, default 1.0 maps to gain 0")
            )

            config.siliconflow["api_key"] = siliconflow_api_key

        params.voice_volume = st.selectbox(
            tr("Speech Volume"),
            options=[0.6, 0.8, 1.0, 1.2, 1.5, 2.0, 3.0, 4.0, 5.0],
            index=2,
        )

        params.voice_rate = st.selectbox(
            tr("Speech Rate"),
            options=[0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.5, 1.8, 2.0],
            index=2,
        )

        bgm_options = [
            (tr("No Background Music"), ""),
            (tr("Random Background Music"), "random"),
            (tr("Custom Background Music"), "custom"),
        ]
        selected_index = st.selectbox(
            tr("Background Music"),
            index=1,
            options=range(
                len(bgm_options)
            ),  # Use the index as the internal option value
            format_func=lambda x: bgm_options[x][
                0
            ],  # The label is displayed to the user
        )
        # Get the selected background music type
        params.bgm_type = bgm_options[selected_index][1]

        # Show or hide components based on the selection
        if params.bgm_type == "custom":
            custom_bgm_file = st.text_input(
                tr("Custom Background Music File"), key="custom_bgm_file_input"
            )
            if custom_bgm_file and os.path.exists(custom_bgm_file):
                params.bgm_file = custom_bgm_file
                # st.write(f":red[Â∑≤ÈÄâÊã©Ëá™ÂÆö‰πâËÉåÊôØÈü≥‰πê]Ôºö**{custom_bgm_file}**")
        params.bgm_volume = st.selectbox(
            tr("Background Music Volume"),
            options=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
            index=2,
        )

with right_panel:
    with st.container(border=True):
        st.write(tr("Subtitle Settings"))
        params.subtitle_enabled = st.checkbox(tr("Enable Subtitles"), value=True)
        font_names = get_all_fonts()
        saved_font_name = config.ui.get("font_name", "MicrosoftYaHeiBold.ttc")
        saved_font_name_index = 0
        if saved_font_name in font_names:
            saved_font_name_index = font_names.index(saved_font_name)
        params.font_name = st.selectbox(
            tr("Font"), font_names, index=saved_font_name_index
        )
        config.ui["font_name"] = params.font_name

        subtitle_positions = [
            (tr("Top"), "top"),
            (tr("Center"), "center"),
            (tr("Bottom"), "bottom"),
            (tr("Custom"), "custom"),
        ]
        selected_index = st.selectbox(
            tr("Position"),
            index=2,
            options=range(len(subtitle_positions)),
            format_func=lambda x: subtitle_positions[x][0],
        )
        params.subtitle_position = subtitle_positions[selected_index][1]

        if params.subtitle_position == "custom":
            custom_position = st.text_input(
                tr("Custom Position (% from top)"),
                value="70.0",
                key="custom_position_input",
            )
            try:
                params.custom_position = float(custom_position)
                if params.custom_position < 0 or params.custom_position > 100:
                    st.error(tr("Please enter a value between 0 and 100"))
            except ValueError:
                st.error(tr("Please enter a valid number"))

        font_cols = st.columns([0.3, 0.7])
        with font_cols[0]:
            saved_text_fore_color = config.ui.get("text_fore_color", "#FFFFFF")
            params.text_fore_color = st.color_picker(
                tr("Font Color"), saved_text_fore_color
            )
            config.ui["text_fore_color"] = params.text_fore_color

        with font_cols[1]:
            saved_font_size = config.ui.get("font_size", 60)
            params.font_size = st.slider(tr("Font Size"), 30, 100, saved_font_size)
            config.ui["font_size"] = params.font_size

        stroke_cols = st.columns([0.3, 0.7])
        with stroke_cols[0]:
            params.stroke_color = st.color_picker(tr("Stroke Color"), "#000000")
        with stroke_cols[1]:
            params.stroke_width = st.slider(tr("Stroke Width"), 0.0, 10.0, 1.5)

start_button = st.button(tr("Generate Video"), use_container_width=True, type="primary")
if start_button:
    config.save_config()
    task_id = str(uuid4())
    if not params.video_subject and not params.video_script:
        st.error(tr("Video Script and Subject Cannot Both Be Empty"))
        scroll_to_bottom()
        st.stop()

    if params.video_source not in ["pexels", "pixabay", "local"]:
        st.error(tr("Please Select a Valid Video Source"))
        scroll_to_bottom()
        st.stop()

    if params.video_source == "pexels" and not config.app.get("pexels_api_keys", ""):
        st.error(tr("Please Enter the Pexels API Key"))
        scroll_to_bottom()
        st.stop()

    if params.video_source == "pixabay" and not config.app.get("pixabay_api_keys", ""):
        st.error(tr("Please Enter the Pixabay API Key"))
        scroll_to_bottom()
        st.stop()

    if uploaded_files:
        local_videos_dir = utils.storage_dir("local_videos", create=True)
        for file in uploaded_files:
            file_path = os.path.join(local_videos_dir, f"{file.file_id}_{file.name}")
            with open(file_path, "wb") as f:
                f.write(file.getbuffer())
                m = MaterialInfo()
                m.provider = "local"
                m.url = file_path
                if not params.video_materials:
                    params.video_materials = []
                params.video_materials.append(m)

    log_container = st.empty()
    log_records = []

    def log_received(msg):
        if config.ui["hide_log"]:
            return
        with log_container:
            log_records.append(msg)
            st.code("\n".join(log_records))

    logger.add(log_received)

    st.toast(tr("Generating Video"))
    logger.info(tr("Start Generating Video"))
    logger.info(utils.to_json(params))
    scroll_to_bottom()

    result = tm.start(task_id=task_id, params=params)
    if not result or "videos" not in result:
        st.error(tr("Video Generation Failed"))
        logger.error(tr("Video Generation Failed"))
        scroll_to_bottom()
        st.stop()

    video_files = result.get("videos", [])
    st.success(tr("Video Generation Completed"))
    try:
        if video_files:
            player_cols = st.columns(len(video_files) * 2 + 1)
            for i, url in enumerate(video_files):
                player_cols[i * 2 + 1].video(url)
    except Exception:
        pass

    open_task_folder(task_id)
    logger.info(tr("Video Generation Completed"))
    scroll_to_bottom()

config.save_config()
</file>

</files>
